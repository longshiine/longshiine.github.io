<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta name="robots" content="index,follow"/><meta property="og:type" content="website"/><meta property="og:site_name" content="Jang. Inspiration"/><meta property="twitter:domain" content="jang-inspiration.com"/><meta name="description" content="정책 이터레이션(Policy Itertaion)과 가치 이터레이션(Value Iteration)에 대해 살펴보자. 또한 다이나믹 프로그래밍의 한계와 모델없이 학습하는 강화학습 등에 대해 톺아보자."/><meta property="og:description" content="정책 이터레이션(Policy Itertaion)과 가치 이터레이션(Value Iteration)에 대해 살펴보자. 또한 다이나믹 프로그래밍의 한계와 모델없이 학습하는 강화학습 등에 대해 톺아보자."/><meta name="twitter:description" content="정책 이터레이션(Policy Itertaion)과 가치 이터레이션(Value Iteration)에 대해 살펴보자. 또한 다이나믹 프로그래밍의 한계와 모델없이 학습하는 강화학습 등에 대해 톺아보자."/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:image" content="https://jang-inspiration.com/api/social-image?id=3931006b-7fcb-4441-b508-0a5ba536daaf"/><meta property="og:image" content="https://jang-inspiration.com/api/social-image?id=3931006b-7fcb-4441-b508-0a5ba536daaf"/><link rel="canonical" href="https://jang-inspiration.com/policy-value-iteration"/><meta property="og:url" content="https://jang-inspiration.com/policy-value-iteration"/><meta property="twitter:url" content="https://jang-inspiration.com/policy-value-iteration"/><link rel="alternate" type="application/rss+xml" href="https://jang-inspiration.com/feed" title="Jang. Inspiration"/><meta property="og:title" content="&lt;파이썬과 케라스로 배우는 강화학습&gt; - (5) 정책 이터레이션, 가치 이터레이션"/><meta name="twitter:title" content="&lt;파이썬과 케라스로 배우는 강화학습&gt; - (5) 정책 이터레이션, 가치 이터레이션"/><title>&lt;파이썬과 케라스로 배우는 강화학습&gt; - (5) 정책 이터레이션, 가치 이터레이션</title><meta name="naver-site-verification" content="3942485b5f7254d146b71f1249d907d89048a4d6"/><link rel="preload" as="image" href="https://images.unsplash.com/photo-1563209259-b2fa97148ce1?ixlib=rb-4.0.3&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb"/><meta name="next-head-count" content="22"/><link rel="shortcut icon" href="/favicon.ico"/><link rel="icon" type="image/png" sizes="32x32" href="favicon.png"/><link rel="manifest" href="/manifest.json"/><link rel="preload" href="/_next/static/css/d2e6a1cb5181bdcf.css" as="style"/><link rel="stylesheet" href="/_next/static/css/d2e6a1cb5181bdcf.css" data-n-g=""/><link rel="preload" href="/_next/static/css/4e32f0fa5eadbe4b.css" as="style"/><link rel="stylesheet" href="/_next/static/css/4e32f0fa5eadbe4b.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script defer="" src="/_next/static/chunks/3607272e.d338bf53926ee7c2.js"></script><script defer="" src="/_next/static/chunks/853.526a4df21aef109c.js"></script><script defer="" src="/_next/static/chunks/175675d1.a2f4b19cd9daa73f.js"></script><script defer="" src="/_next/static/chunks/274.59c48af6aaac8ebd.js"></script><script defer="" src="/_next/static/chunks/358.0027340467b29549.js"></script><script src="/_next/static/chunks/webpack-bcbf8f4abc46b243.js" defer=""></script><script src="/_next/static/chunks/framework-4556c45dd113b893.js" defer=""></script><script src="/_next/static/chunks/main-f08b69bdcc7bbb61.js" defer=""></script><script src="/_next/static/chunks/pages/_app-27630c741ae01a08.js" defer=""></script><script src="/_next/static/chunks/780-d9c40783d635496e.js" defer=""></script><script src="/_next/static/chunks/634-d6e2e697c346745d.js" defer=""></script><script src="/_next/static/chunks/pages/%5BpageId%5D-35899cbd792aff57.js" defer=""></script><script src="/_next/static/I-ojLq_lc7g-_5LzXbOVe/_buildManifest.js" defer=""></script><script src="/_next/static/I-ojLq_lc7g-_5LzXbOVe/_ssgManifest.js" defer=""></script></head><body><script>
/** Inlined version of noflash.js from use-dark-mode */
;(function () {
  var storageKey = 'darkMode'
  var classNameDark = 'dark-mode'
  var classNameLight = 'light-mode'
  function setClassOnDocumentBody(darkMode) {
    document.body.classList.add(darkMode ? classNameDark : classNameLight)
    document.body.classList.remove(darkMode ? classNameLight : classNameDark)
  }
  var preferDarkQuery = '(prefers-color-scheme: dark)'
  var mql = window.matchMedia(preferDarkQuery)
  var supportsColorSchemeQuery = mql.media === preferDarkQuery
  var localStorageTheme = null
  try {
    localStorageTheme = localStorage.getItem(storageKey)
  } catch (err) {}
  var localStorageExists = localStorageTheme !== null
  if (localStorageExists) {
    localStorageTheme = JSON.parse(localStorageTheme)
  }
  // Determine the source of truth
  if (localStorageExists) {
    // source of truth from localStorage
    setClassOnDocumentBody(localStorageTheme)
  } else if (supportsColorSchemeQuery) {
    // source of truth from system
    setClassOnDocumentBody(mql.matches)
    localStorage.setItem(storageKey, mql.matches)
  } else {
    // source of truth from document.body
    var isDarkMode = document.body.classList.contains(classNameDark)
    localStorage.setItem(storageKey, JSON.stringify(isDarkMode))
  }
})();
</script><div id="__next"><div class="notion notion-app light-mode notion-block-3931006b7fcb4441b5080a5ba536daaf"><div class="notion-viewport"></div><div class="notion-frame"><header class="notion-header"><div class="notion-nav-header"><div class="breadcrumbs"><a class="breadcrumb" href="/"><div class="notion-page-icon-inline notion-page-icon-image"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%272000%27%20height=%272000%27/%3e"/></span><img alt="Jang. Inspiration" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" class="icon notion-page-icon" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/webp;base64,UklGRv4AAABXRUJQVlA4WAoAAAAQAAAADwAADwAAQUxQSFUAAAARL6CmkRQ4eF5eaio0EBGB5+2AUWxbbR5t/yWkKiDsUwVQnn8xoViI6P8EAOK9AiCF/BQQSXKHsKrOmo21PddC4xsKsbafNSc1jfQLgBSSGsD0HBoAAFZQOCCCAAAAsAIAnQEqEAAQAAVAfCWkAA+DSIAfdDvk/UudpKAAAPYcuQdG3VZft4V447Ryb0FWml/BLapzhboSZ6SLFp2WFSojeqJNq2VAANIrwpK47u1M/geoV8ECX9+TEWBDeQC4c2g3HcSldGi6FylcOf6PWeqJaegO6jdPFlsvX04FC2gAAA==&quot;)"/><noscript><img alt="Jang. Inspiration" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fb29e9b03-c79c-4e52-a45e-7228163ba524%2Fcompass-circular-tool_(3).png?table=block&amp;id=6246082f-4014-4d06-98ab-59e9840b298a&amp;cache=v2" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="icon notion-page-icon" loading="lazy"/></noscript></span></div><span class="title">Jang. Inspiration</span></a></div><div class="notion-nav-header-rhs breadcrumbs"><a href="/about" class="breadcrumb button">About</a><div class="breadcrumb button styles_hidden__7gYve"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="32" d="M256 48v48m0 320v48m147.08-355.08l-33.94 33.94M142.86 369.14l-33.94 33.94M464 256h-48m-320 0H48m355.08 147.08l-33.94-33.94M142.86 142.86l-33.94-33.94"></path><circle cx="256" cy="256" r="80" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="32"></circle></svg></div><div role="button" class="breadcrumb button notion-search-button"><svg class="notion-icon searchIcon" viewBox="0 0 17 17"><path d="M6.78027 13.6729C8.24805 13.6729 9.60156 13.1982 10.709 12.4072L14.875 16.5732C15.0684 16.7666 15.3232 16.8633 15.5957 16.8633C16.167 16.8633 16.5713 16.4238 16.5713 15.8613C16.5713 15.5977 16.4834 15.3516 16.29 15.1582L12.1504 11.0098C13.0205 9.86719 13.5391 8.45215 13.5391 6.91406C13.5391 3.19629 10.498 0.155273 6.78027 0.155273C3.0625 0.155273 0.0214844 3.19629 0.0214844 6.91406C0.0214844 10.6318 3.0625 13.6729 6.78027 13.6729ZM6.78027 12.2139C3.87988 12.2139 1.48047 9.81445 1.48047 6.91406C1.48047 4.01367 3.87988 1.61426 6.78027 1.61426C9.68066 1.61426 12.0801 4.01367 12.0801 6.91406C12.0801 9.81445 9.68066 12.2139 6.78027 12.2139Z"></path></svg></div></div></div></header><div class="notion-page-scroller"><div class="notion-page-cover-wrapper"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%274509%27%20height=%273433%27/%3e"/></span><img alt="&lt;파이썬과 케라스로 배우는 강화학습&gt; - (5) 정책 이터레이션, 가치 이터레이션" src="https://images.unsplash.com/photo-1563209259-b2fa97148ce1?ixlib=rb-4.0.3&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb" decoding="async" data-nimg="intrinsic" class="notion-page-cover" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-position:center 50%;background-size:cover;background-position:center 50%;filter:blur(20px);background-image:url(&quot;data:image/webp;base64,UklGRkQAAABXRUJQVlA4IDgAAAAQAgCdASoQAAwABUB8JZQCdAEN4Z4/0jgAAP7nI6g5q7g7NuXgHKw945x+RhNJbTW2vaRJ2E2wAA==&quot;)"/><noscript><img alt="&lt;파이썬과 케라스로 배우는 강화학습&gt; - (5) 정책 이터레이션, 가치 이터레이션" src="https://images.unsplash.com/photo-1563209259-b2fa97148ce1?ixlib=rb-4.0.3&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-position:center 50%" class="notion-page-cover"/></noscript></span></div><main class="notion-page notion-page-has-cover notion-page-has-icon notion-page-has-text-icon notion-full-page"><div class="notion-page-icon-hero notion-page-icon-span"><span class="notion-page-icon" role="img" aria-label="🎮">🎮</span></div><h1 class="notion-title">&lt;파이썬과 케라스로 배우는 강화학습&gt; - (5) <b>정책 이터레이션, 가치 이터레이션</b></h1><div class="notion-collection-page-properties"><div class="notion-collection-row"><div class="notion-collection-row-body"><div class="notion-collection-row-property"><div class="notion-collection-column-title"><svg viewBox="0 0 14 14" class="notion-collection-column-title-icon"><path d="M7 13A6 6 0 107 1a6 6 0 000 12zM3.751 5.323A.2.2 0 013.909 5h6.182a.2.2 0 01.158.323L7.158 9.297a.2.2 0 01-.316 0L3.751 5.323z"></path></svg><div class="notion-collection-column-title-body">Category</div></div><div class="notion-collection-row-value"><span class="notion-property notion-property-select"><div class="notion-property-select-item notion-item-orange">강화학습</div></span></div></div><div class="notion-collection-row-property"><div class="notion-collection-column-title"><svg viewBox="0 0 14 14" class="notion-collection-column-title-icon"><path d="M4 3a1 1 0 011-1h7a1 1 0 110 2H5a1 1 0 01-1-1zm0 4a1 1 0 011-1h7a1 1 0 110 2H5a1 1 0 01-1-1zm0 4a1 1 0 011-1h7a1 1 0 110 2H5a1 1 0 01-1-1zM2 4a1 1 0 110-2 1 1 0 010 2zm0 4a1 1 0 110-2 1 1 0 010 2zm0 4a1 1 0 110-2 1 1 0 010 2z"></path></svg><div class="notion-collection-column-title-body">Tags</div></div><div class="notion-collection-row-value"><span class="notion-property notion-property-multi_select"><div class="notion-property-multi_select-item notion-item-pink">Reinforcement Learning</div><div class="notion-property-multi_select-item notion-item-default">Policy Iteration</div><div class="notion-property-multi_select-item notion-item-green">Value Iteration</div></span></div></div><div class="notion-collection-row-property"><div class="notion-collection-column-title"><svg viewBox="0 0 14 14" class="notion-collection-column-title-icon"><path d="M10.889 5.5H3.11v1.556h7.778V5.5zm1.555-4.444h-.777V0H10.11v1.056H3.89V0H2.333v1.056h-.777c-.864 0-1.548.7-1.548 1.555L0 12.5c0 .856.692 1.5 1.556 1.5h10.888C13.3 14 14 13.356 14 12.5V2.611c0-.855-.7-1.555-1.556-1.555zm0 11.444H1.556V3.944h10.888V12.5zM8.556 8.611H3.11v1.556h5.445V8.61z"></path></svg><div class="notion-collection-column-title-body">Published</div></div><div class="notion-collection-row-value"><span class="notion-property notion-property-date">January 24, 2021</span></div></div><div class="notion-collection-row-property"><div class="notion-collection-column-title"><svg viewBox="0 0 14 14" class="notion-collection-column-title-icon"><path d="M7 4.568a.5.5 0 00-.5-.5h-6a.5.5 0 00-.5.5v1.046a.5.5 0 00.5.5h6a.5.5 0 00.5-.5V4.568zM.5 1a.5.5 0 00-.5.5v1.045a.5.5 0 00.5.5h12a.5.5 0 00.5-.5V1.5a.5.5 0 00-.5-.5H.5zM0 8.682a.5.5 0 00.5.5h11a.5.5 0 00.5-.5V7.636a.5.5 0 00-.5-.5H.5a.5.5 0 00-.5.5v1.046zm0 3.068a.5.5 0 00.5.5h9a.5.5 0 00.5-.5v-1.045a.5.5 0 00-.5-.5h-9a.5.5 0 00-.5.5v1.045z"></path></svg><div class="notion-collection-column-title-body">Author</div></div><div class="notion-collection-row-value"><span class="notion-property notion-property-text"><b>Jay</b></span></div></div></div></div></div><div class="notion-page-content notion-page-content-has-aside"><article class="notion-page-content-inner"><blockquote class="notion-quote notion-block-9156e6576cba499ca1c3e86bfc6bf15e"><div>본 포스트는 “파이썬과 케라스로 배우는 강화학습” 도서의 다섯번째 리뷰 포스트입니다.</div></blockquote><div class="notion-text notion-block-64bde1c573b54ea6b91c30590e7b68b9"><a target="_blank" rel="noopener noreferrer" class="notion-link" href="http://www.yes24.com/Product/Goods/44136413">http://www.yes24.com/Product/Goods/44136413</a></div><div class="notion-blank notion-block-64d4ca183be0405dbc83b24467a26974"> </div><div class="notion-table-of-contents notion-gray notion-block-f9ffb4a3ee5f4e14a482623b05c2d183"><a href="#777e671428374db3ba65e27dce561aa3" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:0">3장 강화학습 기초 2: 그리드월드와 다이내믹 프로그래밍</span></a><a href="#e08796237ada41158472b63ff9c0cb59" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">정책 이터레이션 (Policy Iteration)</span></a><a href="#787cf98b8e4d4accba03c4c2d929dd3d" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">정책 평가 (Policy Evaluation)</span></a><a href="#a96ebf40fd1f4f278ee4cd878a3e5cfe" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">한 번의 정책 평가 과정을 순서대로 나타내면 다음과 같다.</span></a><a href="#9620387bd7ce449b99c17bd9ba846a16" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">정책 발전 (Policy Improvement)</span></a><a href="#fbaef7fb05e547b7a5dfce10b63598c0" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">정책 이터레이션 코드</span></a><a href="#c8f4ecdbb65548378aaace4e9fbb7385" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">policy_evaluation</span></a><a href="#ac6e2f4599ac49b99a1d6204dbacecea" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">policy_improvement</span></a><a href="#4a6c2369474243a1a5dd01af6117af60" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">가치 이터레이션</span></a><a href="#a292d19a038545b9bf571b2f2f4b4592" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">다이내믹 프로그래밍의 한계와 강화학습</span></a><a href="#cfbdb24641b14df69f29db86499c30b6" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">모델없이 학습하는 강화학습</span></a><a href="#a73badffd6df476c8807d22c8d5d0a32" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">정리</span></a><a href="#c28457b02739408c9396dfdd77117fad" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">3장 한줄평</span></a></div><div class="notion-blank notion-block-2bc1cd01713543c6a4ced18f3d1d0fa5"> </div><h3 class="notion-h notion-h2 notion-h-indent-0 notion-block-777e671428374db3ba65e27dce561aa3" data-id="777e671428374db3ba65e27dce561aa3"><span><div id="777e671428374db3ba65e27dce561aa3" class="notion-header-anchor"></div><a class="notion-hash-link" href="#777e671428374db3ba65e27dce561aa3" title="3장 강화학습 기초 2: 그리드월드와 다이내믹 프로그래밍"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>3장 강화학습 기초 2: 그리드월드와 다이내믹 프로그래밍</b></span></span></h3><hr class="notion-hr notion-block-838d7f6c31a547fc8747922f84cc3822"/><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-e08796237ada41158472b63ff9c0cb59" data-id="e08796237ada41158472b63ff9c0cb59"><span><div id="e08796237ada41158472b63ff9c0cb59" class="notion-header-anchor"></div><a class="notion-hash-link" href="#e08796237ada41158472b63ff9c0cb59" title="정책 이터레이션 (Policy Iteration)"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>정책 이터레이션 (Policy Iteration)</b></span></span></h4><div class="notion-text notion-block-7b4892bcd58d40f2bed44b6cd3880d61">결국 MDP로 정의되는 문제에서 알고 싶은 것은 <b>가장 높은 보상을 얻게 하는 정책</b> 을 찾는 것이다. 하지만 처음에는 이 정책을 알 수가 없다. 보통 처음에는 무작위로 행동을 정하는 정책으로부터 시작하여 계속 발전시켜 나간다.</div><div class="notion-text notion-block-d8e71f87aa8d46a4b01a1fa2f2c13392">하지만 우리가 얻고자 하는 최적 정책은 무작위정책(random policy)이 아니다. 그렇다면 현재의 정책을 <b>평가</b> 하고 더 나은 정책으로 <b>발전</b> 해야한다. 어떤 정책이 있을 때 <code class="notion-inline-code">정책평가(Policy Evaluation)</code>를 통해 얼마나 좋은지 평가하고, 그 평가를 기준으로 <code class="notion-inline-code">정책발전(Policy Improvement)</code>을 통해 좀 더 나은 정책으로 발전 시킨다. 이러한 과정을 무한히 반복하면 <b>정책은 최적 정책으로 수렴</b> 한다.</div><div class="notion-text notion-block-90464eef41e04339ab20d48a0e859f6b"><b>안선생의 안소리🤬 예상</b> : 무작위 정책에서 출발해서 도대체 어떻게 최적 정책에 수렴하지요? 자세히 설명해보세요!</div><div class="notion-text notion-block-e389e67c0f644fb69a4d5e5c48a567de">–&gt; <a target="_blank" rel="noopener noreferrer" class="notion-link" href="https://www.youtube.com/watch?v=rrTxOkbHj-M&amp;t=19s">https://www.youtube.com/watch?v=rrTxOkbHj-M&amp;t=19s</a> 팡요랩 영상의 18분 즈음부터 참고</div><div class="notion-text notion-block-8decffcdb7b64b3ba3409abdfc6c1dbe">(정리해보자면, 주위의 상태의 값들은 쓰레기 값이지만 <b>보상</b> 과 같은 작은 정보들을 토대로 점진적으로 업데이트 되어 최적에 다가가게 되는 것이다. 정수기에서 물이 정수되는 것처럼 말이다! 사실 아직도 업데이트가 되는것이 신기하다)</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-f7963807437e4ebfaeb5fe626a84c086"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:480px;max-width:100%;flex-direction:column"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%272000%27%20height=%271339%27/%3e"/></span><img alt="정책평가와 정책발전" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/webp;base64,UklGRkQAAABXRUJQVlA4IDgAAACwAQCdASoQAAsABUB8JaQAAjjp6GAAAP7x4KZLg7pTNeU/RSNLrgbqZiMaybu0SKd2Zm0i1Z0AAA==&quot;)"/><noscript><img alt="정책평가와 정책발전" src="https://www.notion.so/image/https%3A%2F%2Flongshiine.github.io%2F2021%2F01%2F24%2FReinforcement-Learning-Basic-4%2Fpolicy.png?table=block&amp;id=f7963807-437e-4ebf-aeb5-fe626a84c086&amp;cache=v2" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></span><figcaption class="notion-asset-caption">정책평가와 정책발전</figcaption></div></figure><div class="notion-blank notion-block-0ef6ede6f51e4f3dbd7cf4387c105252"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-787cf98b8e4d4accba03c4c2d929dd3d" data-id="787cf98b8e4d4accba03c4c2d929dd3d"><span><div id="787cf98b8e4d4accba03c4c2d929dd3d" class="notion-header-anchor"></div><a class="notion-hash-link" href="#787cf98b8e4d4accba03c4c2d929dd3d" title="정책 평가 (Policy Evaluation)"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>정책 평가 (Policy Evaluation)</b></span></span></h4><div class="notion-text notion-block-7542df8e59cd44f7b62ae76203110a6d">가치함수는 정책이 얼마나 좋은지 판단하는 근거가 된다. 가치함수는 <b>현재 정책 </b><b>를 따라갔을 때 받을 보상에 대한 기댓값</b>⭐️ 이다. 에이전트의 목표는 어떻게 하면 보상을 많이 받을 수 있을지를 알아내는 것이므로 현재 정책에 따라 받을 보상에 대한 정보가 정책의 가치가 되는 것이다.</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-af15637cd0d041e385414e752ac98b39"><span></span></span><div class="notion-text notion-block-d754d914e6ba44b6b255fb6844a1457f">정책 이터레이션에서는 위의 벨만 기대 방정식을 사용하여 문제를 풀 것이다. 핵심은 <b>주변 상태의 가치함수와 한 타임스텝의 보상만 고려해서 현재 상태의 다음 가치함수를 계산</b> 하겠다는 것이다. 이 과정은 한 타임스텝의 보상만 고려하고 주변 상태의 가치함수들이 참 가치함수들이 아니기 때문에 이렇게 계산해도 이 값은 실제값이 아니다. 하지만 이러한 계산을 <b>여러번 반복</b> 한다면 <b>참 값으로 수렴</b> 한다는 것이 메인 아이디어이다.</div><div class="notion-text notion-block-7a7663860fc94d348c63abc1f48cbe93">이전 포스트에서 위의 식을 그리드월드 예제에서 계산 가능한 형태로 변환해 보았었다. 아래는 k번째 가치함수를 통해 k+1번째 가치함수를 계산하는 식이다. 우리는 아래의 식을 반복적으로 계산해 나갈 것이다.</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-33f92c7378de4dc08f73ba8930cbc551"><span></span></span><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-a96ebf40fd1f4f278ee4cd878a3e5cfe" data-id="a96ebf40fd1f4f278ee4cd878a3e5cfe"><span><div id="a96ebf40fd1f4f278ee4cd878a3e5cfe" class="notion-header-anchor"></div><a class="notion-hash-link" href="#a96ebf40fd1f4f278ee4cd878a3e5cfe" title="한 번의 정책 평가 과정을 순서대로 나타내면 다음과 같다."><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>한 번의 정책 평가 과정을 순서대로 나타내면 다음과 같다.</b></span></span></h4><ol start="1" class="notion-list notion-list-numbered notion-block-2972ab52349a4c18b62cd7a73270b793"><li><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>번째 가치함수 matrix에서 현재 상태 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>에서 갈 수 있는 다음상태 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>에 저장돼 있는 가치함수 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>을 불러온다. (보라색 부분중의 하나)</li></ol><ol start="2" class="notion-list notion-list-numbered notion-block-436b37a6ac2e4fce8297616e5656b474"><li><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>에 할인율 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>를 곱하고 그 상태로 가는 행동에 대한 보상 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>을 더한다.</li><ol class="notion-list notion-list-numbered notion-block-436b37a6ac2e4fce8297616e5656b474"><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-449c67837e76446fa280b651c67f7319"><span></span></span></ol></ol><ol start="3" class="notion-list notion-list-numbered notion-block-508c7fc4005a42f3a9d9edb16908fe71"><li>2번에서 구한 값에 그 행동을 할 확률, 즉 정책 값을 곱한다.</li><ol class="notion-list notion-list-numbered notion-block-508c7fc4005a42f3a9d9edb16908fe71"><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-3488e7b6b1f84a988eb3fcdee2ba9949"><span></span></span></ol></ol><ol start="4" class="notion-list notion-list-numbered notion-block-c4d918e62f0549ae95c04b06c238a31f"><li>3번을 모든 선택 가능한 행동에 대해 반복하고 그 값들을 더한다.</li><ol class="notion-list notion-list-numbered notion-block-c4d918e62f0549ae95c04b06c238a31f"><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-0dbe93fdb9ed4a378f56875b97e88879"><span></span></span></ol></ol><ol start="5" class="notion-list notion-list-numbered notion-block-e189dadb04c54d4f861b74868ddfbdc7"><li>4번 과정을 통해 더한 값을 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>번째 가치함수 matrix의 상태 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>자리에 저장한다.</li><ol class="notion-list notion-list-numbered notion-block-e189dadb04c54d4f861b74868ddfbdc7"><div class="notion-blank notion-block-5b219ef30e3845f8868e4fa24836b764"> </div></ol></ol><ol start="6" class="notion-list notion-list-numbered notion-block-f6797a9efe50427a86a87cacec5d4d31"><li>1-5 과정을 모든 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>에 대해 반복한다.</li></ol><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-07999fe15d0840879540ace96e7cd206"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:576px;max-width:100%;flex-direction:column"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%272000%27%20height=%271177%27/%3e"/></span><img alt="policy evaluation" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/webp;base64,UklGRjgAAABXRUJQVlA4ICwAAADQAQCdASoQAAkABUB8JZwAAudfJ7B4gAD+8dtkJ5dFQvI7obbWTKNlIfgAAA==&quot;)"/><noscript><img alt="policy evaluation" src="https://www.notion.so/image/https%3A%2F%2Flongshiine.github.io%2F2021%2F01%2F24%2FReinforcement-Learning-Basic-4%2FpolicyIteration.png?table=block&amp;id=07999fe1-5d08-4087-9540-ace96e7cd206&amp;cache=v2" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></span><figcaption class="notion-asset-caption">policy evaluation</figcaption></div></figure><div class="notion-text notion-block-f2b89802416741a0a6a208a87ad96d1b">이것은 한번의 정책평가 과정이다. 하지만 한번의 정책평가로서는 제대로 평가를 할 수 없어, 이 과정을 여러번 반복하는데, v_1으로 시작해서 무한히 반복하면 참 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>가 될 수 있다.</div><div class="notion-blank notion-block-08a4fc78b9f341cf96e57242b28d032e"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-9620387bd7ce449b99c17bd9ba846a16" data-id="9620387bd7ce449b99c17bd9ba846a16"><span><div id="9620387bd7ce449b99c17bd9ba846a16" class="notion-header-anchor"></div><a class="notion-hash-link" href="#9620387bd7ce449b99c17bd9ba846a16" title="정책 발전 (Policy Improvement)"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>정책 발전 (Policy Improvement)</b></span></span></h4><div class="notion-text notion-block-975bb71b14a04722a9cce3d3c7316963">애초에 정책을 발전 시키지 않는다면 정책에 대한 평가는 의미가 없다. 그렇다면 정책 평가를 바탕으로 어떻게 정책을 발전시킬 수 있을까? 사실 정책 발전의 방법이 정해져있는 것은 아니다. 하지만 이 책에서는 가장 널리 알려진 <code class="notion-inline-code">탐욕 정책 발전(Greedy Policy Improvement)</code>을 사용한다.</div><div class="notion-text notion-block-6aabb5306ea8467e86f58e159708f6f5">에이전트가 해야할 일은 단순하다. 상태 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>에서 선택 가능한 행동의 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>  즉 큐함수 값을 비교하고 그중에서 가장 큰 값을 가지는 행동을 선택하면 된다. 이것을 <b>탐욕 정책 발전</b> 이라고 하는데, 눈앞에 보이는 것 중에서 <b>당장에 가장 큰 이익을 추구하는 것과 같은 모습</b> 이기 때문에 이러한 이름이 붙었다.</div><div class="notion-text notion-block-3b0bc53b45dd4f7e99eb78ba22f11ae4">탐욕 정책 발전을 통해 업데이트된 정책은 아래와 같다. max 함수와는 다르게 반환되는 것이 <b>행동</b> 이다.</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-63923709581247db92ad19a8f7b9c5da"><span></span></span><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-46e63dfb3cfd4d59b1af03fccc419a69"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:480px;max-width:100%;flex-direction:column"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%272000%27%20height=%27996%27/%3e"/></span><img alt="Greedy Policy Improvement" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/webp;base64,UklGRjgAAABXRUJQVlA4ICwAAACwAQCdASoQAAgABUB8JaQAAlxaOLoAAP7x37RourXQ7PpcU2VkPcioGGwAAA==&quot;)"/><noscript><img alt="Greedy Policy Improvement" src="https://www.notion.so/image/https%3A%2F%2Flongshiine.github.io%2F2021%2F01%2F24%2FReinforcement-Learning-Basic-4%2Fgreedy.png?table=block&amp;id=46e63dfb-3cfd-4d59-b1af-03fccc419a69&amp;cache=v2" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></span><figcaption class="notion-asset-caption">Greedy Policy Improvement</figcaption></div></figure><div class="notion-text notion-block-3dcd404f31cf4b7396ff915705f568d2">탐욕 정책 발전을 통해 정책을 업데이트하면 이전 가치함수에 비해 업데이트된 정책으로 움직였을 때 받을 가치함수가 <b>무조건 크거나 같다.</b> 다이타믹 프로그래밍에서는 이처럼 탐욕 정책 발전을 사용하여 가장 큰값의 가치함수를 가지는 최적 정책에 수렴할 수 있다.</div><div class="notion-blank notion-block-02638e30844c41548f9063676415fd26"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-fbaef7fb05e547b7a5dfce10b63598c0" data-id="fbaef7fb05e547b7a5dfce10b63598c0"><span><div id="fbaef7fb05e547b7a5dfce10b63598c0" class="notion-header-anchor"></div><a class="notion-hash-link" href="#fbaef7fb05e547b7a5dfce10b63598c0" title="정책 이터레이션 코드"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>정책 이터레이션 코드</b></span></span></h4><div class="notion-text notion-block-4ab1dc50b72d4245ad69e2ed90878bf3">우선 에이전트가 해야할 역할을 고려해서 전체 코드의 흐름을 보면 다음과 같다.</div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python">class PolicyIteration:
  def __init__(self, env):
    # 환경에 대한 객체
    self.env = env

  # 정책 평가
  def policy_evaluation(self):
    pass

  # 정책 발전
  def policy_improvement(self):
    pass

  # 특정 상태에서 정책에 따른 행동
  def get_action(self, state):
    return action

if __name__ == &quot;__main__&quot;:
  env = Env()
  policy_iteration = PolicyIteration(env)
  grid_world = GraphicDisplay(policy_iteration)
  grid_world.mainloop()</code></pre><div class="notion-text notion-block-7feb467c2ebf4403be8eadbc3d10d7fb">에이전트가 알고 있는 환경(env)의 정보는 다음과 같다.</div><ul class="notion-list notion-list-disc notion-block-d6eded1356f74abaa6bd0af751cbc17c"><li><code class="notion-inline-code">env.width, env.height</code>: 그리드월드의 너비와 높이</li></ul><ul class="notion-list notion-list-disc notion-block-8e983f53678b4d75b03f8a4a6151ad78"><li><code class="notion-inline-code">env.state_after_aciton(state, action)</code>: 특정 상태에서 특정 행동을 했을 때 에이전트가 가는 다음 상태</li></ul><ul class="notion-list notion-list-disc notion-block-40965d2396d748dba42d4c5fceda0f69"><li><code class="notion-inline-code">env.get_all_states()</code>: 존재하는 모든 상태</li></ul><ul class="notion-list notion-list-disc notion-block-664fab5a2e7f4db8a9d3c70876a724ea"><li><code class="notion-inline-code">env.get_reward(state, action)</code>: 특정 상태의 보상</li></ul><ul class="notion-list notion-list-disc notion-block-7a0a384c4cb04c7ab275824826799cc5"><li><code class="notion-inline-code">env.possible_actions</code>: 상,하,좌,우</li></ul><div class="notion-text notion-block-dd091679b8e947f5af768fe661decfb5">정책 이터레이션은 <b>정책 평가</b> 와 <b>정책 발전</b> 으로 이뤄져 있다. 따라서 각각의 함수를 정의 한다.</div><div class="notion-blank notion-block-5dc03d3466b84f229c6513c05cf38d86"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-c8f4ecdbb65548378aaace4e9fbb7385" data-id="c8f4ecdbb65548378aaace4e9fbb7385"><span><div id="c8f4ecdbb65548378aaace4e9fbb7385" class="notion-header-anchor"></div><a class="notion-hash-link" href="#c8f4ecdbb65548378aaace4e9fbb7385" title="policy_evaluation"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>policy_evaluation</b></span></span></h4><div class="notion-text notion-block-aa3964728773452494f66926f768afd6">정책 평가를 통해 에이전트는 모든 상태의 가치함수를 업데이트 한다. 모든 상태에 대해 <b>벨반 기대 방정식</b> 의 계산이 끝나면 현재의 value_table에 next_value_table을 덮어쓰는 식으로 policy_evalutation을 진행한다.</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-8b3e5264f22f46d6b0f50410505d26cc"><span></span></span><div class="notion-text notion-block-eea7bdcba15f4e20bea6a49a0306cc94">정책 평가에 사용되는 벨만 기대 방정식이다. 이때, <b>상태 변환 확률</b> 을 1이라고 설정 했기 때문에, 다음상태 <!-- -->은 만약 행동이 왼쪽일 경우 왼쪽에 있는 상태가 된다.</div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python"># 벨만 기대 방정식을 통해 다음 가치함수를 계산하는 정책 평가
def policy_evaluation(self):
    # 다음 가치함수 초기화
    next_value_table = [[0.00] * self.env.width
                        for _ in range(self.env.height)]

    # 모든 상태에 대해서 벨만 기대방정식을 계산
    for state in self.env.get_all_states():
        value = 0.0
        # 마침 상태의 가치 함수 = 0
        if state == [2, 2]:
            next_value_table[state[0]][state[1]] = value
            continue

        # 벨만 기대 방정식
        for action in self.env.possible_actions:
            next_state = self.env.state_after_action(state, action)
            reward = self.env.get_reward(state, action)
            next_value = self.get_value(next_state)
            value += (self.get_policy(state)[action] *
                      (reward + self.discount_factor * next_value))

        next_value_table[state[0]][state[1]] = value

    self.value_table = next_value_table</code></pre><ul class="notion-list notion-list-disc notion-block-5972dd6fbed748e69f57f0d9db892cb3"><li><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>: <code class="notion-inline-code">for action in self.env.possible_actions:</code></li></ul><ul class="notion-list notion-list-disc notion-block-9fd587ada8274613977e88529b9495d5"><li><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>: <code class="notion-inline-code">self.get_policy(state)[action]</code></li></ul><ul class="notion-list notion-list-disc notion-block-31fa1f8012eb4fa48fcbf9eaf5d4591f"><li><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>: <code class="notion-inline-code">reward = self.env.get_reward(state, action)</code></li></ul><ul class="notion-list notion-list-disc notion-block-a994bd675f1f4d39a7eb89dbbab1389b"><li><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>: <code class="notion-inline-code">self.discount_factor</code> (0.9)</li></ul><ul class="notion-list notion-list-disc notion-block-9051a7d7beb14813a1973aec9b85fe01"><li><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>: <code class="notion-inline-code">next_state = self.env.state_after_action(state, action)</code></li></ul><ul class="notion-list notion-list-disc notion-block-fa27001f1a1b41799d8c089e75bf3e58"><li><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>: <code class="notion-inline-code">self.get_value(next_state)</code></li></ul><ul class="notion-list notion-list-disc notion-block-5331209bb3d24e0c80bef4f11e67131d"><li><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>: <code class="notion-inline-code">next_value_table[state[0]][state[1]]</code></li></ul><div class="notion-text notion-block-b93ea5cb595d492ea880b93a05e55483">get_policy 함수를 통해 각 상태에서 각 행동에 대한 확률값을 구한다. 그리고 다음 상태로 갔을 때 받을 보상과 다음상태의 가치함수를 할인율을 적용하여 더한다. 정책이 각 행동에 대한 확률을 나타내기 때문에 모든 행동에 대해 value를 계산하고 더하면 기댓값을 계산한 것이 된다.</div><div class="notion-blank notion-block-0485d50946d2495eaec92ea6d0152164"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-ac6e2f4599ac49b99a1d6204dbacecea" data-id="ac6e2f4599ac49b99a1d6204dbacecea"><span><div id="ac6e2f4599ac49b99a1d6204dbacecea" class="notion-header-anchor"></div><a class="notion-hash-link" href="#ac6e2f4599ac49b99a1d6204dbacecea" title="policy_improvement"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>policy_improvement</b></span></span></h4><div class="notion-text notion-block-cd6323af8e5d4a56be485101440de333">정책 평가를 통해 정책을 평가하면 그에 따른 새로운 가치함수를 얻는다. 에이전트는 이제 새로운 가치함수를 통해 정책을 업데이트 해야한다. 정책발전에는 탐욕 정책 발전을 사용한다.</div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python"># 현재 가치 함수에 대해서 탐욕 정책 발전
def policy_improvement(self):
    next_policy = self.policy_table
    for state in self.env.get_all_states():
        if state == [2, 2]:
            continue

        value_list = []
        # 반환할 정책 초기화
        result = [0.0, 0.0, 0.0, 0.0]

        # 모든 행동에 대해서 [보상 + (할인율 * 다음 상태 가치함수)] 계산
        for index, action in enumerate(self.env.possible_actions):
            next_state = self.env.state_after_action(state, action)
            reward = self.env.get_reward(state, action)
            next_value = self.get_value(next_state)
            value = reward + self.discount_factor * next_value
            value_list.append(value)

        # 받을 보상이 최대인 행동들에 대해 탐욕 정책 발전
        max_idx_list = np.argwhere(value_list == np.amax(value_list))
        max_idx_list = max_idx_list.flatten().tolist()
        prob = 1 / len(max_idx_list)

        for idx in max_idx_list:
            result[idx] = prob

        next_policy[state[0]][state[1]] = result

    self.policy_table = next_policy</code></pre><div class="notion-text notion-block-d830b7390bf44fd1b04950e33b3fc1da">탐욕 정책 발전은 가치가 가장 높은 하나의 행동을 선택하는 것이다. 하지만 이 예제에서와 같이 현재 상태에서 가장 좋은 행동이 여러개일 수도 있다. 그럴때에는 가장 좋은 행동들을 동일한 확률로 선택하는 정책으로 업데이트한다.</div><ul class="notion-list notion-list-disc notion-block-4220183bfff34ff08e7e8d3431586557"><li>현재 상태에서 가능한 행동에 대해 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>을 계산한다.</li></ul><ul class="notion-list notion-list-disc notion-block-9992fc68e14b494fbdb8797373f67e32"><li>계산한 값을 value_list에 저장한다.</li></ul><ul class="notion-list notion-list-disc notion-block-e3f0a5a9d592456d9a555b4d7a1d813f"><li>max 함수를 통해 value_list에 담긴 값 중 가장 큰 값을 알아낸다.</li></ul><ul class="notion-list notion-list-disc notion-block-4d27cf555b63447f91bcbe5ced1462e6"><li>argwhere를 통해 가장 큰 값의 index를 알아내고(여러개면 여러개 반환) max_idx_list에 저장한다.</li></ul><ul class="notion-list notion-list-disc notion-block-788dbad4fa04421d87ca95d978c9cab4"><li>max_idx_list의 길이를 바탕으로 확률을 계산하여 확률값을 계산하고 result에 저장한다.</li></ul><div class="notion-text notion-block-3dbf01fd455549d7baa0958c5ed84f9a">이렇게 되면 policy_table에는 업데이트 된 정책(각 상태의 행동에 대한 확률)이 저장된다.</div><div class="notion-blank notion-block-6665b56aee1b43dcb33d9bafaa3688ce"> </div><blockquote class="notion-quote notion-block-1c341cfb44274ef59976fd8b9ecf6e6e"><div>정책 이터레이션에서 에이전트는 정책 평가와 정책 발전을 반복하여 최적 정책을 찾아낸다.</div></blockquote><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-06599b0b2dcd461f93ccb2526f62de3f"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:432px;max-width:100%;flex-direction:column"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%272000%27%20height=%272012%27/%3e"/></span><img alt="정책 이터레이션으로 구한 최적 정책" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/webp;base64,UklGRjgAAABXRUJQVlA4ICwAAADQAQCdASoQABAABUB8JYwAAudfJ8MmAAD+77XsPFqKk6yP/3+vXuFwuqAAAA==&quot;)"/><noscript><img alt="정책 이터레이션으로 구한 최적 정책" src="https://www.notion.so/image/https%3A%2F%2Flongshiine.github.io%2F2021%2F01%2F24%2FReinforcement-Learning-Basic-4%2Foptimalpolicy.png?table=block&amp;id=06599b0b-2dcd-461f-93cc-b2526f62de3f&amp;cache=v2" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></span><figcaption class="notion-asset-caption">정책 이터레이션으로 구한 최적 정책</figcaption></div></figure><div class="notion-blank notion-block-d29295f007574d8dad72e2bfd3d09c25"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-4a6c2369474243a1a5dd01af6117af60" data-id="4a6c2369474243a1a5dd01af6117af60"><span><div id="4a6c2369474243a1a5dd01af6117af60" class="notion-header-anchor"></div><a class="notion-hash-link" href="#4a6c2369474243a1a5dd01af6117af60" title="가치 이터레이션"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>가치 이터레이션</b></span></span></h4><div class="notion-text notion-block-d8c99204031d468b9b514f1fd15a20c5">정책 이터레이션과 가치 이터레이션의 중요한 차이점은 정책 이터레이션에서는 <b>정책 평가와 정책 발전</b> 으로 단계가 나누어져 있다면 가치 이터레이션에서는 그렇지 않다는 것이다.</div><div class="notion-text notion-block-57daf93060bb4aaab41725378a3dbbeb">가치 이터레이션은 현재의 가치함수가 최적 정책에 대한 가치함수라고 가정하기 때문에 정책을 발전하는 함수가 따로 필요하지 않다. 따라서 최적 행동을 반환하는 get_action 함수를 정책을 출력하는 데 대신 사용한다.</div><div class="notion-text notion-block-0dac76b41fa14826813629ca94a52c5f">주요 코드의 전체적인 흐름은 다음과 같다.</div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python">class ValueIteration:
  def __init__(self, env):
    # 환경 객체 생성
    self.env = env

  # 벨만 최적 방정식을 통해 다음 가치함수 계산
  def value_iteration(self):
    return

  # 현재 가치함수로부터 행동을 반환
  def get_action(self, state):
    return

  def get_value(self, state):
    return

if __name__ == &quot;__main__&quot;:
  env = Env()
  value_iteration = ValueIteration(env)
  grid_world = GraphicDisplay(value_iteration)
  gird_world.mainloop()</code></pre><div class="notion-text notion-block-9a5a193229324f30b040e6a972e308da">정책 이터레이션에서는 policy_evaluation 함수에서 벨만 기대 방정식을 통해 다음 가치함수를 계산했다. 가치 이터레이션에서는 value_iteration 함수를 통해 다음 가치함수를 계산한다.</div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python"># 벨만 최적 방정식을 통해 다음 가치 함수 계산
def value_iteration(self):
    # 다음 가치함수 초기화
    next_value_table = [[0.0] * self.env.width
                        for _ in range(self.env.height)]

    # 모든 상태에 대해서 벨만 최적방정식을 계산
    for state in self.env.get_all_states():
        # 마침 상태의 가치 함수 = 0
        if state == [2, 2]:
            next_value_table[state[0]][state[1]] = 0.0
            continue

        # 벨만 최적 방정식
        value_list = []
        for action in self.env.possible_actions:
            next_state = self.env.state_after_action(state, action)
            reward = self.env.get_reward(state, action)
            next_value = self.get_value(next_state)
            value_list.append((reward + self.discount_factor * next_value))

        # 최댓값을 다음 가치 함수로 대입
        next_value_table[state[0]][state[1]] = max(value_list)

    self.value_table = next_value_table</code></pre><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-a735e9be65ce4a2eb704a9d0870073eb"><span></span></span><ul class="notion-list notion-list-disc notion-block-3cb6e9fb96324560bb2b0a338fc0740c"><li>위의 벨만 최적 방정식을 계산하여 value_list에 저장한다.</li></ul><ul class="notion-list notion-list-disc notion-block-95780d7bb83145ac9f5bf4bfa179dec1"><li>value_list에 저장된 값중 최대의 값을 새로운 가치함수로 저장한다.</li></ul><ul class="notion-list notion-list-disc notion-block-ac2684f348d64c61a72c4292a535a030"></ul><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python"># 현재 가치 함수로부터 행동을 반환
def get_action(self, state):
    if state == [2, 2]:
        return []

    # 모든 행동에 대해 큐함수 (보상 + (감가율 * 다음 상태 가치함수))를 계산
    value_list = []
    for action in self.env.possible_actions:
        next_state = self.env.state_after_action(state, action)
        reward = self.env.get_reward(state, action)
        next_value = self.get_value(next_state)
        value = (reward + self.discount_factor * next_value)
        value_list.append(value)

    # 최대 큐 함수를 가진 행동(복수일 경우 여러 개)을 반환
    max_idx_list = np.argwhere(value_list == np.amax(value_list))
    action_list = max_idx_list.flatten().tolist()
    return action_list</code></pre><ul class="notion-list notion-list-disc notion-block-aa388efab0ca46edacec67dfa3ccb638"><li>모든 행동에 대해 큐함수를 구한다.</li></ul><ul class="notion-list notion-list-disc notion-block-3d6bd3598bea4217ab0da55111c021ce"><li>그 중 가장 큰 value 값을 가지는 행동의 인덱스를 가져온다(여러개라면 여러개 모두 가져온다)</li></ul><ul class="notion-list notion-list-disc notion-block-b642b5d2dbdc40f3914557e09fe0d740"><li>행동들을 모두 action_list에 저장한다.</li></ul><div class="notion-text notion-block-64ebfc2b455d4503b71f6227143e45f8">이 예제에 대해서는 Calculate 6번 정도에 가치함수가 거의 수렴하게 된다. 이때 수렴한 가치함수의 값을 토대로 정책을 출력해보면 아래의 그림과 같다.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-8448cf4f6eba488eaae47a935dfbeee0"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:384px;max-width:100%;flex-direction:column"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%272000%27%20height=%272092%27/%3e"/></span><img alt="가치 이터레이션으로 구한 최적 정책" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/webp;base64,UklGRj4AAABXRUJQVlA4IDIAAADQAQCdASoPABAABUB8JYwAAveFRo5yAAD+7XDqZlYJ7j4ljR5PRr/9ISMQC2/qjKYAAA==&quot;)"/><noscript><img alt="가치 이터레이션으로 구한 최적 정책" src="https://www.notion.so/image/https%3A%2F%2Flongshiine.github.io%2F2021%2F01%2F24%2FReinforcement-Learning-Basic-4%2Fvalueiteration.png?table=block&amp;id=8448cf4f-6eba-488e-aae4-7a935dfbeee0&amp;cache=v2" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></span><figcaption class="notion-asset-caption">가치 이터레이션으로 구한 최적 정책</figcaption></div></figure><div class="notion-blank notion-block-719ae05498664f7facfe1e886304f81c"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-a292d19a038545b9bf571b2f2f4b4592" data-id="a292d19a038545b9bf571b2f2f4b4592"><span><div id="a292d19a038545b9bf571b2f2f4b4592" class="notion-header-anchor"></div><a class="notion-hash-link" href="#a292d19a038545b9bf571b2f2f4b4592" title="다이내믹 프로그래밍의 한계와 강화학습"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>다이내믹 프로그래밍의 한계와 강화학습</b></span></span></h4><div class="notion-text notion-block-d1464267b3ee4e008df17bf05db00290">벨만 방정식을 이용한 다이내믹 프로그래밍으로서 정책 이터레이션과 가치 이터레이션을 살펴봤다. 하지만 다이내믹 프로그래밍은 <b>계산을 빠르게 하는 것이지 학습을 하는 것</b> 은 아니다. 그렇다면 이러한 다이내믹 프로그래밍의 한계는 무엇일까?</div><div class="notion-text notion-block-22c280db1ea04feb92e0ef6ab84300ba"><b>1. 계산복잡도</b> : 문제의 규모가 커지만 계산으로 푸는데에 한계. <code class="notion-inline-code">계산복잡도</code> = 상태 크기의 3제곱<b>2. 차원의 저주</b> : 그리드월드의 상태의 차원은 2차원. 상태의 차원이 늘어나면 상태의 수가 지수적으로 증가<b>3. 환경에 대한 완벽한 정보가 필요</b> : 보상과 상태 변환 확률을 정확히 안다는 가정 필요. 보통은 이 정보를 확실히 알 수 없음.</div><div class="notion-text notion-block-02bb229e82e54e2499d29c461e8b892e">현실세계의 환경에 놓인 문제를 풀어내는 데에는 위의 세 가지 한계가 치명적으로 작용한다. 때문에 이러한 한계를 극복하기 위해 환경을 모르지만 <b>환경과의 상호작용을 통해 경험을 바탕으로 학습하는 방법</b> 이 등장한다. 바로 <code class="notion-inline-code">강화학습</code>이다.</div><div class="notion-blank notion-block-fced0da06e5f44fca591f842a73c6543"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-cfbdb24641b14df69f29db86499c30b6" data-id="cfbdb24641b14df69f29db86499c30b6"><span><div id="cfbdb24641b14df69f29db86499c30b6" class="notion-header-anchor"></div><a class="notion-hash-link" href="#cfbdb24641b14df69f29db86499c30b6" title="모델없이 학습하는 강화학습"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>모델없이 학습하는 강화학습</b></span></span></h4><div class="notion-text notion-block-fcab1ba8fec449c594df22771a834403">환경의 모델이란 무엇일까? MDP에서 환경의 모델은 상태 변환 확률과 보상이다.</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-e33832df0d2c4253b2b8960e377c6b14"><span></span></span><div class="notion-text notion-block-d2a9aea3d3c9447b9bc2497ed74757a3">현재 이 책에서 다루고 싶은 모델은 <b>수학적 모델</b> 로서 시스템에 입력이 들어왔을 때 시스템이 어떤 출력을 내는지에 대한 방정식이다. 이처럼 입력과 출력의 관계를 식으로 나타내는 과정을 <code class="notion-inline-code">모델링(Modeling)</code>이라고 한다.</div><div class="notion-text notion-block-22d14221a92b4cc3b43f6a38b8c11b10">사실 입력과 출력 사이의 방정식은 정확할 수가 없다. 방정식에서는 A라는 입력이 들어와서 B라는 출력이 나오더라도 실제 세상에서는 B라는 출력이 절대로 나오지 않는다. 모델은 정확하면 정확할수록 복잡하며 공기나 바람같은 <b>자연현상을 정확하게 모델링하는 것은 불가능</b> 에 가깝다. 게임에서는 사실 사람이 환경을 만들었고, 사람이 정해준대로만 게임이 작동하므로 <b>모델링 오차는 없다</b> 고 볼 수 있다. 하지만 게임을 벗어난다면 글쎄…🤔</div><div class="notion-text notion-block-dd69fead1a5644ee819004300d36b709">모델을 정확히 알기 어려운 경우, 시스템의 입력과 출력 사이의 관계를 알기 위해 두가지 접근 방법으로 접근해 볼 수 있다.</div><ol start="1" class="notion-list notion-list-numbered notion-block-42484c425aea40a4aca6592b6e4b4030"><li>할 수 있는 선에서 정확한 모델링을 한 다음, 모델링 오차에 대한 부분을 실험을 통해 조정한다.</li></ol><ol start="2" class="notion-list notion-list-numbered notion-block-ffc848bbb4a84cf7825c3006065ab0f1"><li>모델 없이 환경과의 상호작용을 통해 입력과 출력 사이의 관계를 학습한다.</li></ol><div class="notion-text notion-block-9f7c36d7cb3b4dcabd21bd6a6cd30122">1번은 학습의 개념없이 고전적으로 많이 적용하는 방법이며 시스템의 안정성을 보장한다. 하지만 문제가 복잡해지고 어려워질수록 한계가 있다.</div><div class="notion-text notion-block-4a2910a44c434d75a3de6c1512cf727d">2번은 학습의 개념이 들어간다. 학습의 특성상 모든 상황에서 동일하게 작동한다고 보장할 수 없지만 많은 복잡한 문제에서 모델이 필요없는 것은 장점이다. 2번 방법이 바로 이 책의 주제인 <code class="notion-inline-code">강화학습</code> 이다.</div><div class="notion-blank notion-block-e79781b5513e4675a77bfc1b7a8cb5f4"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-a73badffd6df476c8807d22c8d5d0a32" data-id="a73badffd6df476c8807d22c8d5d0a32"><span><div id="a73badffd6df476c8807d22c8d5d0a32" class="notion-header-anchor"></div><a class="notion-hash-link" href="#a73badffd6df476c8807d22c8d5d0a32" title="정리"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>정리</b></span></span></h4><ul class="notion-list notion-list-disc notion-block-e25a77786478407c926a590b0e6eef33"><li><code class="notion-inline-code">정책 이터레이션</code>: 벨만 기대 방정식을 이용해 정책을 평가하고, 탐욕 정책 발전을 이용해 정책 발전</li></ul><ul class="notion-list notion-list-disc notion-block-9008d223e0bc40b495570334cddb9499"><li><code class="notion-inline-code">가치 이터레이션</code>: 최적 정책을 가정하고 벨만 최적 방정식 이용. 정책이 직접적으로 주어지지 않아 큐함수를 통해 행동 선택</li></ul><ul class="notion-list notion-list-disc notion-block-8d6dabcc07da4e35922a9ca5ccda00e9"><li><code class="notion-inline-code">다이내믹 프로그래밍의 한계</code>: 계산 복잡도, 차원의 저주, 환경에 대한 완벽한 정보 필요</li></ul><div class="notion-blank notion-block-c105cbac1a9a45de9ed3f45372210cc7"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-c28457b02739408c9396dfdd77117fad" data-id="c28457b02739408c9396dfdd77117fad"><span><div id="c28457b02739408c9396dfdd77117fad" class="notion-header-anchor"></div><a class="notion-hash-link" href="#c28457b02739408c9396dfdd77117fad" title="3장 한줄평"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>3장 한줄평</b></span></span></h4><blockquote class="notion-quote notion-block-5de2d98e72f6438cb359fdf0fbb16bb3"><div>짧고 간결하게 쓰는게 오히려 힘들다..</div></blockquote></article><aside class="notion-aside"></aside></div></main></div></div></div><div style="width:100%;background-color:#ffffff;color:#373534;padding:20px"><div id="disqus_recommendations"></div><div id="disqus_thread"></div><footer class="styles_footer__RBpyk"><div class="styles_copyright__nhL_k">Copyright 2023 <!-- -->Jang Yeong</div><div class="styles_settings__GyEhi"></div><div class="styles_social__ptL3p"><a class="styles_github__0JN7a" href="https://github.com/longshiine" title="GitHub @longshiine" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a class="styles_linkedin__bgwDi" href="https://www.linkedin.com/in/jangyeong-kim-b7924422a" title="LinkedIn Jang Yeong" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a><a class="styles_instagram__BY5Hj" href="https://instagram.com/jang.inspiration" title="Instagram Jang Yeong" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><g><path fill="none" d="M0 0h24v24H0z"></path><path fill-rule="nonzero" d="M12 2c2.717 0 3.056.01 4.122.06 1.065.05 1.79.217 2.428.465.66.254 1.216.598 1.772 1.153a4.908 4.908 0 0 1 1.153 1.772c.247.637.415 1.363.465 2.428.047 1.066.06 1.405.06 4.122 0 2.717-.01 3.056-.06 4.122-.05 1.065-.218 1.79-.465 2.428a4.883 4.883 0 0 1-1.153 1.772 4.915 4.915 0 0 1-1.772 1.153c-.637.247-1.363.415-2.428.465-1.066.047-1.405.06-4.122.06-2.717 0-3.056-.01-4.122-.06-1.065-.05-1.79-.218-2.428-.465a4.89 4.89 0 0 1-1.772-1.153 4.904 4.904 0 0 1-1.153-1.772c-.248-.637-.415-1.363-.465-2.428C2.013 15.056 2 14.717 2 12c0-2.717.01-3.056.06-4.122.05-1.066.217-1.79.465-2.428a4.88 4.88 0 0 1 1.153-1.772A4.897 4.897 0 0 1 5.45 2.525c.638-.248 1.362-.415 2.428-.465C8.944 2.013 9.283 2 12 2zm0 5a5 5 0 1 0 0 10 5 5 0 0 0 0-10zm6.5-.25a1.25 1.25 0 0 0-2.5 0 1.25 1.25 0 0 0 2.5 0zM12 9a3 3 0 1 1 0 6 3 3 0 0 1 0-6z"></path></g></svg></a></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"site":{"domain":"jang-inspiration.com","name":"Jang. Inspiration","rootNotionPageId":"6246082f40144d0698ab59e9840b298a","rootNotionSpaceId":null,"description":"장영의 영감노트"},"recordMap":{"block":{"3931006b-7fcb-4441-b508-0a5ba536daaf":{"role":"reader","value":{"id":"3931006b-7fcb-4441-b508-0a5ba536daaf","version":1052,"type":"page","properties":{"==~K":[["Yes"]],"AfoN":[["강화학습"]],"BN]P":[["Reinforcement Learning,Policy Iteration,Value Iteration"]],"NVm^":[["policy-value-iteration"]],"a\u003cql":[["‣",[["d",{"type":"date","start_date":"2021-01-24"}]]]],"}nqi":[["Jay"]],"~]S\u003c":[["정책 이터레이션(Policy Itertaion)과 가치 이터레이션(Value Iteration)에 대해 살펴보자. 또한 다이나믹 프로그래밍의 한계와 모델없이 학습하는 강화학습 등에 대해 톺아보자."]],"title":[["\u003c파이썬과 케라스로 배우는 강화학습\u003e - (5) "],["정책 이터레이션, 가치 이터레이션",[["b"]]]]},"content":["9156e657-6cba-499c-a1c3-e86bfc6bf15e","64bde1c5-73b5-4ea6-b91c-30590e7b68b9","64d4ca18-3be0-405d-bc83-b24467a26974","f9ffb4a3-ee5f-4e14-a482-623b05c2d183","2bc1cd01-7135-43c6-a4ce-d18f3d1d0fa5","777e6714-2837-4db3-ba65-e27dce561aa3","838d7f6c-31a5-47fc-8747-922f84cc3822","e0879623-7ada-4115-8472-b63ff9c0cb59","7b4892bc-d58d-40f2-bed4-4b6cd3880d61","d8e71f87-aa8d-46a4-b01a-1fa2f2c13392","90464eef-41e0-4339-ab20-d48a0e859f6b","e389e67c-0f64-4fb6-9a4d-5e5c48a567de","8decffcd-b7b6-4b3b-a340-9abdfc6c1dbe","f7963807-437e-4ebf-aeb5-fe626a84c086","0ef6ede6-f51e-4f3d-bd7c-f4387c105252","787cf98b-8e4d-4acc-ba03-c4c2d929dd3d","7542df8e-59cd-44f7-b62a-e76203110a6d","af15637c-d0d0-41e3-8541-4e752ac98b39","d754d914-e6ba-44b6-b255-fb6844a1457f","7a766386-0fc9-4d34-8c63-abc1f48cbe93","33f92c73-78de-4dc0-8f73-ba8930cbc551","a96ebf40-fd1f-4f27-8ee4-cd878a3e5cfe","2972ab52-349a-4c18-b62c-d7a73270b793","436b37a6-ac2e-4fce-8297-616e5656b474","508c7fc4-005a-42f3-a9d9-edb16908fe71","c4d918e6-2f05-49ae-95c0-4b06c238a31f","e189dadb-04c5-4d4f-861b-74868ddfbdc7","f6797a9e-fe50-427a-86a8-7cacec5d4d31","07999fe1-5d08-4087-9540-ace96e7cd206","f2b89802-4167-41a0-a6a2-08a87ad96d1b","08a4fc78-b9f3-41cf-96e5-7242b28d032e","9620387b-d7ce-449b-99c1-7bd9ba846a16","975bb71b-14a0-4722-a9cc-e3d3c7316963","6aabb530-6ea8-467e-86f5-8e159708f6f5","3b0bc53b-45dd-4f7e-99eb-78ba22f11ae4","63923709-5812-47db-92ad-19a8f7b9c5da","46e63dfb-3cfd-4d59-b1af-03fccc419a69","3dcd404f-31cf-4b73-96ff-915705f568d2","02638e30-844c-4154-8f90-63676415fd26","fbaef7fb-05e5-47b7-a5df-ce10b63598c0","4ab1dc50-b72d-4245-ad69-e2ed90878bf3","f6f719bc-1287-4264-ac29-29d473c17b8b","7feb467c-2ebf-4403-be8e-adbc3d10d7fb","d6eded13-56f7-4aba-a6bd-0af751cbc17c","8e983f53-678b-4d75-b03f-8a4a6151ad78","40965d23-96d7-48db-a42d-4c5fceda0f69","664fab5a-2e7f-4db8-a9d3-c70876a724ea","7a0a384c-4cb0-4c7a-b275-824826799cc5","dd091679-b8e9-47f5-af76-8fe661decfb5","5dc03d34-66b8-4f22-9c65-13c05cf38d86","c8f4ecdb-b655-4837-8aaa-ce4e9fbb7385","aa396472-8773-4524-94f6-6926f768afd6","8b3e5264-f22f-46d6-b0f5-0410505d26cc","eea7bdcb-a15f-4e20-bea6-a49a0306cc94","aab9a3c3-ea53-4eac-8706-b99b01746f29","5972dd6f-bed7-48e6-9f57-f0d9db892cb3","9fd587ad-a827-4613-977e-88529b9495d5","31fa1f80-12eb-4fa4-8fcb-f9eaf5d4591f","a994bd67-5f1f-4d39-a7eb-89dbbab1389b","9051a7d7-beb1-4813-a197-3aec9b85fe01","fa27001f-1a1b-4179-9d8c-089e75bf3e58","5331209b-b3d2-4e0c-80be-f4f11e67131d","b93ea5cb-595d-492e-a880-b93a05e55483","0485d509-46d2-495e-aec9-2ea6d0152164","ac6e2f45-99ac-49b9-9a1d-6204dbacecea","cd6323af-8e5d-4a56-be48-5101440de333","78698025-422e-4a27-9baa-b31967071d4e","d830b739-0bf4-4fd1-b049-50e33b3fc1da","4220183b-fff3-4ff0-8e7e-8d3431586557","9992fc68-e14b-494f-bdb8-797373f67e32","e3f0a5a9-d592-456d-9a55-5b4d7a1d813f","4d27cf55-5b63-447f-91bc-be5ced1462e6","788dbad4-fa04-421d-87ca-95d978c9cab4","3dbf01fd-4555-49d7-baa0-958c5ed84f9a","6665b56a-ee1b-43dc-b33d-9bafaa3688ce","1c341cfb-4427-4ef5-9976-fd8b9ecf6e6e","06599b0b-2dcd-461f-93cc-b2526f62de3f","d29295f0-0757-4d8d-ad72-e2bfd3d09c25","4a6c2369-4742-43a1-a5dd-01af6117af60","d8c99204-031d-468b-9b51-4f1fd15a20c5","57daf930-60bb-4aaa-b417-25378a3dbbeb","0dac76b4-1fa1-4826-8136-29ca94a52c5f","e508dc87-4740-4adf-b110-57734c7dbde2","9a5a1932-2932-4f30-b040-e6a972e308da","1859020c-d801-4027-aece-863b850b4c15","a735e9be-65ce-4a2e-b704-a9d0870073eb","3cb6e9fb-9632-4560-bb2b-0a338fc0740c","95780d7b-b831-45ac-9f5b-f4bfa179dec1","ac2684f3-48d6-4c61-a72c-4292a535a030","60766959-e793-4704-8972-a8036e3d694e","aa388efa-b0ca-46ed-acec-67dfa3ccb638","3d6bd359-8bea-4217-ab0d-a55111c021ce","b642b5d2-dbdc-40f3-9145-57e09fe0d740","64ebfc2b-455d-4503-b71f-6227143e45f8","8448cf4f-6eba-488e-aae4-7a935dfbeee0","719ae054-9866-4f7f-acfe-1e886304f81c","a292d19a-0385-45b9-bf57-1b2f2f4b4592","d1464267-b3ee-4e00-8df1-7bf05db00290","22c280db-1ea0-4feb-92e0-ef6ab84300ba","02bb229e-82e5-4e24-99d2-9c461e8b892e","fced0da0-6e5f-44fc-a591-f842a73c6543","cfbdb246-41b1-4df6-9f29-db86499c30b6","fcab1ba8-fec4-49c5-94df-22771a834403","e33832df-0d2c-4253-b2b8-960e377c6b14","d2a9aea3-d3c9-447b-9bc2-497ed74757a3","22d14221-a92b-4cc3-b43f-6a38b8c11b10","dd69fead-1a56-44ee-8190-04300d36b709","42484c42-5aea-40a4-aca6-592b6e4b4030","ffc848bb-b4a8-4cf7-825c-3006065ab0f1","9f7c36d7-cb3b-4dca-bd21-bd6a6cd30122","4a2910a4-4c43-4d75-a3de-6c1512cf727d","e79781b5-513e-4675-a77b-fc1b7a8cb5f4","a73badff-d6df-476c-8807-d22c8d5d0a32","e25a7778-6478-407c-926a-590b0e6eef33","9008d223-e0bc-40b4-9557-0334cddb9499","8d6dabcc-07da-4e35-922a-9ca5ccda00e9","c105cbac-1a9a-45de-9ed3-f45372210cc7","c28457b0-2739-408c-9396-dfdd77117fad","5de2d98e-72f6-438c-b359-fdf0fbb16bb3"],"format":{"page_icon":"🎮","page_cover":"https://images.unsplash.com/photo-1563209259-b2fa97148ce1?ixlib=rb-4.0.3\u0026q=80\u0026fm=jpg\u0026crop=entropy\u0026cs=tinysrgb","page_cover_position":0.5},"created_time":1676047922095,"last_edited_time":1677651627337,"parent_id":"ba8460cf-4781-486e-8976-01358ef4659d","parent_table":"collection","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"4f20ede8-ccf7-4ae1-82e5-819e100dd032":{"role":"reader","value":{"id":"4f20ede8-ccf7-4ae1-82e5-819e100dd032","version":129,"type":"collection_view","view_ids":["d91647c5-6a81-48b1-a1ff-a04529d0ddba","27bc73a3-5779-44e0-b617-2f0d26f5aa2f"],"collection_id":"ba8460cf-4781-486e-8976-01358ef4659d","format":{"collection_pointer":{"id":"ba8460cf-4781-486e-8976-01358ef4659d","table":"collection","spaceId":"4af10338-3e65-4b50-af9f-798d59d5c8f6"},"copied_from_pointer":{"id":"3e3073e9-7aee-481c-b831-765e112ec7b5","table":"block","spaceId":"fde5ac74-eea3-4527-8f00-4482710e1af3"}},"created_time":1675998770865,"last_edited_time":1677136438825,"parent_id":"6246082f-4014-4d06-98ab-59e9840b298a","parent_table":"block","alive":true,"copied_from":"3e3073e9-7aee-481c-b831-765e112ec7b5","created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"6246082f-4014-4d06-98ab-59e9840b298a":{"role":"reader","value":{"id":"6246082f-4014-4d06-98ab-59e9840b298a","version":609,"type":"page","properties":{"title":[["Jang. Inspiration"]]},"content":["651dcdf3-689d-42d2-8497-64f8509d3504","0bd6df0e-6679-499e-b382-c9dc2c597776","f4c89bb7-a90e-41d7-a678-588b3deff765","d5557a1e-de5e-4085-896d-362a19928b69","4f20ede8-ccf7-4ae1-82e5-819e100dd032","ce518f27-4e46-4e98-ac75-13c467c1370c","dfee9c57-3be6-41db-8113-a54eeef675a7","1109cf3f-d8f9-4532-a13e-1af177ad4fdd","5855fe5e-17e2-4f14-8b66-702838bcc734"],"format":{"page_icon":"https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b29e9b03-c79c-4e52-a45e-7228163ba524/compass-circular-tool_(3).png","page_cover":"/images/page-cover/nasa_reduced_gravity_walking_simulator.jpg","copied_from_pointer":{"id":"78754261-97cf-4616-9880-9def95960ebf","table":"block","spaceId":"fde5ac74-eea3-4527-8f00-4482710e1af3"},"page_cover_position":0.5},"permissions":[{"role":"editor","type":"user_permission","user_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb"},{"role":"reader","type":"public_permission","added_timestamp":1675998834095}],"created_time":1675998770872,"last_edited_time":1677392850666,"parent_id":"3a2d56c9-1da9-4a62-b698-f0ad3576c8c1","parent_table":"block","alive":true,"copied_from":"78754261-97cf-4616-9880-9def95960ebf","file_ids":["f70d3dc6-ce97-4be2-9cde-b86606147b41","a2bd3317-78e4-48bc-8d27-9b733175a416","7fae9664-8795-4723-844e-0adecdea62dc","4235c094-2110-4aa6-b058-6b5fe220dbb7","c8194a03-81d0-482d-a7de-f491a6e85f54","7eb95609-c81b-48c1-969e-5ef2f220bc5a","160057d8-120e-4f9f-8c1f-6bcf31a50f15","0cb9278b-708b-4da1-929a-6696aa8cdfa3","3eb9471e-9b71-4bd3-a13d-c33158a442be","b29e9b03-c79c-4e52-a45e-7228163ba524"],"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"9156e657-6cba-499c-a1c3-e86bfc6bf15e":{"role":"reader","value":{"id":"9156e657-6cba-499c-a1c3-e86bfc6bf15e","version":8,"type":"quote","properties":{"title":[["본 포스트는 “파이썬과 케라스로 배우는 강화학습” 도서의 다섯번째 리뷰 포스트입니다."]]},"created_time":1676048004603,"last_edited_time":1676055517915,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"64bde1c5-73b5-4ea6-b91c-30590e7b68b9":{"role":"reader","value":{"id":"64bde1c5-73b5-4ea6-b91c-30590e7b68b9","version":7,"type":"text","properties":{"title":[["http://www.yes24.com/Product/Goods/44136413",[["a","http://www.yes24.com/Product/Goods/44136413"]]]]},"created_time":1676055518451,"last_edited_time":1676055518730,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"64d4ca18-3be0-405d-bc83-b24467a26974":{"role":"reader","value":{"id":"64d4ca18-3be0-405d-bc83-b24467a26974","version":7,"type":"text","created_time":1676049263182,"last_edited_time":1676049263466,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"f9ffb4a3-ee5f-4e14-a482-623b05c2d183":{"role":"reader","value":{"id":"f9ffb4a3-ee5f-4e14-a482-623b05c2d183","version":4,"type":"table_of_contents","format":{"block_color":"gray"},"created_time":1676049270866,"last_edited_time":1676049270867,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"2bc1cd01-7135-43c6-a4ce-d18f3d1d0fa5":{"role":"reader","value":{"id":"2bc1cd01-7135-43c6-a4ce-d18f3d1d0fa5","version":4,"type":"text","created_time":1676049263182,"last_edited_time":1676049263755,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"777e6714-2837-4db3-ba65-e27dce561aa3":{"role":"reader","value":{"id":"777e6714-2837-4db3-ba65-e27dce561aa3","version":4,"type":"sub_header","properties":{"title":[["3장 강화학습 기초 2: 그리드월드와 다이내믹 프로그래밍",[["b"]]]]},"created_time":1676048004603,"last_edited_time":1676048071225,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"838d7f6c-31a5-47fc-8747-922f84cc3822":{"role":"reader","value":{"id":"838d7f6c-31a5-47fc-8747-922f84cc3822","version":18,"type":"divider","created_time":1676048069730,"last_edited_time":1676048070476,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"e0879623-7ada-4115-8472-b63ff9c0cb59":{"role":"reader","value":{"id":"e0879623-7ada-4115-8472-b63ff9c0cb59","version":2,"type":"sub_sub_header","properties":{"title":[["정책 이터레이션 (Policy Iteration)",[["b"]]]]},"created_time":1676048004603,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"7b4892bc-d58d-40f2-bed4-4b6cd3880d61":{"role":"reader","value":{"id":"7b4892bc-d58d-40f2-bed4-4b6cd3880d61","version":2,"type":"text","properties":{"title":[["결국 MDP로 정의되는 문제에서 알고 싶은 것은 "],["가장 높은 보상을 얻게 하는 정책",[["b"]]],[" 을 찾는 것이다. 하지만 처음에는 이 정책을 알 수가 없다. 보통 처음에는 무작위로 행동을 정하는 정책으로부터 시작하여 계속 발전시켜 나간다."]]},"created_time":1676048004603,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"d8e71f87-aa8d-46a4-b01a-1fa2f2c13392":{"role":"reader","value":{"id":"d8e71f87-aa8d-46a4-b01a-1fa2f2c13392","version":2,"type":"text","properties":{"title":[["하지만 우리가 얻고자 하는 최적 정책은 무작위정책(random policy)이 아니다. 그렇다면 현재의 정책을 "],["평가",[["b"]]],[" 하고 더 나은 정책으로 "],["발전",[["b"]]],[" 해야한다. 어떤 정책이 있을 때 "],["정책평가(Policy Evaluation)",[["c"]]],["를 통해 얼마나 좋은지 평가하고, 그 평가를 기준으로 "],["정책발전(Policy Improvement)",[["c"]]],["을 통해 좀 더 나은 정책으로 발전 시킨다. 이러한 과정을 무한히 반복하면 "],["정책은 최적 정책으로 수렴",[["b"]]],[" 한다."]]},"created_time":1676048004604,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"90464eef-41e0-4339-ab20-d48a0e859f6b":{"role":"reader","value":{"id":"90464eef-41e0-4339-ab20-d48a0e859f6b","version":5,"type":"text","properties":{"title":[["안선생의 안소리🤬 예상",[["b"]]],[" : 무작위 정책에서 출발해서 도대체 어떻게 최적 정책에 수렴하지요? 자세히 설명해보세요!"]]},"created_time":1676048004605,"last_edited_time":1676048131832,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"e389e67c-0f64-4fb6-9a4d-5e5c48a567de":{"role":"reader","value":{"id":"e389e67c-0f64-4fb6-9a4d-5e5c48a567de","version":8,"type":"text","properties":{"title":[["–\u003e "],["https://www.youtube.com/watch?v=rrTxOkbHj-M\u0026t=19s",[["a","https://www.youtube.com/watch?v=rrTxOkbHj-M\u0026t=19s"]]],[" 팡요랩 영상의 18분 즈음부터 참고"]]},"created_time":1676048131826,"last_edited_time":1676048136937,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"8decffcd-b7b6-4b3b-a340-9abdfc6c1dbe":{"role":"reader","value":{"id":"8decffcd-b7b6-4b3b-a340-9abdfc6c1dbe","version":5,"type":"text","properties":{"title":[["(정리해보자면, 주위의 상태의 값들은 쓰레기 값이지만 "],["보상",[["b"]]],[" 과 같은 작은 정보들을 토대로 점진적으로 업데이트 되어 최적에 다가가게 되는 것이다. 정수기에서 물이 정수되는 것처럼 말이다! 사실 아직도 업데이트가 되는것이 신기하다)"]]},"created_time":1676048136934,"last_edited_time":1676048136937,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"f7963807-437e-4ebf-aeb5-fe626a84c086":{"role":"reader","value":{"id":"f7963807-437e-4ebf-aeb5-fe626a84c086","version":6,"type":"image","properties":{"source":[["https://longshiine.github.io/2021/01/24/Reinforcement-Learning-Basic-4/policy.png"]],"caption":[["정책평가와 정책발전"]]},"format":{"block_width":480,"block_full_width":false,"block_page_width":false},"created_time":1676048004605,"last_edited_time":1676048153928,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"0ef6ede6-f51e-4f3d-bd7c-f4387c105252":{"role":"reader","value":{"id":"0ef6ede6-f51e-4f3d-bd7c-f4387c105252","version":4,"type":"text","created_time":1676048004607,"last_edited_time":1676048151078,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"787cf98b-8e4d-4acc-ba03-c4c2d929dd3d":{"role":"reader","value":{"id":"787cf98b-8e4d-4acc-ba03-c4c2d929dd3d","version":2,"type":"sub_sub_header","properties":{"title":[["정책 평가 (Policy Evaluation)",[["b"]]]]},"created_time":1676048004607,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"7542df8e-59cd-44f7-b62a-e76203110a6d":{"role":"reader","value":{"id":"7542df8e-59cd-44f7-b62a-e76203110a6d","version":2,"type":"text","properties":{"title":[["가치함수는 정책이 얼마나 좋은지 판단하는 근거가 된다. 가치함수는 "],["현재 정책 ",[["b"]]],["를 따라갔을 때 받을 보상에 대한 기댓값",[["b"]]],["⭐️ 이다. 에이전트의 목표는 어떻게 하면 보상을 많이 받을 수 있을지를 알아내는 것이므로 현재 정책에 따라 받을 보상에 대한 정보가 정책의 가치가 되는 것이다."]]},"created_time":1676048004607,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"af15637c-d0d0-41e3-8541-4e752ac98b39":{"role":"reader","value":{"id":"af15637c-d0d0-41e3-8541-4e752ac98b39","version":16,"type":"equation","properties":{"title":[["v*\\pi (s) = E[R*{t+1}+\\gamma G\\_{t+1}|S_t=s]"]]},"created_time":1676048004607,"last_edited_time":1676048180740,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"d754d914-e6ba-44b6-b255-fb6844a1457f":{"role":"reader","value":{"id":"d754d914-e6ba-44b6-b255-fb6844a1457f","version":2,"type":"text","properties":{"title":[["정책 이터레이션에서는 위의 벨만 기대 방정식을 사용하여 문제를 풀 것이다. 핵심은 "],["주변 상태의 가치함수와 한 타임스텝의 보상만 고려해서 현재 상태의 다음 가치함수를 계산",[["b"]]],[" 하겠다는 것이다. 이 과정은 한 타임스텝의 보상만 고려하고 주변 상태의 가치함수들이 참 가치함수들이 아니기 때문에 이렇게 계산해도 이 값은 실제값이 아니다. 하지만 이러한 계산을 "],["여러번 반복",[["b"]]],[" 한다면 "],["참 값으로 수렴",[["b"]]],[" 한다는 것이 메인 아이디어이다."]]},"created_time":1676048004608,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"7a766386-0fc9-4d34-8c63-abc1f48cbe93":{"role":"reader","value":{"id":"7a766386-0fc9-4d34-8c63-abc1f48cbe93","version":2,"type":"text","properties":{"title":[["이전 포스트에서 위의 식을 그리드월드 예제에서 계산 가능한 형태로 변환해 보았었다. 아래는 k번째 가치함수를 통해 k+1번째 가치함수를 계산하는 식이다. 우리는 아래의 식을 반복적으로 계산해 나갈 것이다."]]},"created_time":1676048004608,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"33f92c73-78de-4dc0-8f73-ba8930cbc551":{"role":"reader","value":{"id":"33f92c73-78de-4dc0-8f73-ba8930cbc551","version":9,"type":"equation","properties":{"title":[["v_{k+1}(s) = \\sum_{a \\in A} \\pi (a|s)(r_{(s,a)}+ \\gamma v_k(s'))"]]},"created_time":1676048197588,"last_edited_time":1676048201954,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"a96ebf40-fd1f-4f27-8ee4-cd878a3e5cfe":{"role":"reader","value":{"id":"a96ebf40-fd1f-4f27-8ee4-cd878a3e5cfe","version":2,"type":"sub_sub_header","properties":{"title":[["한 번의 정책 평가 과정을 순서대로 나타내면 다음과 같다.",[["b"]]]]},"created_time":1676048004608,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"2972ab52-349a-4c18-b62c-d7a73270b793":{"role":"reader","value":{"id":"2972ab52-349a-4c18-b62c-d7a73270b793","version":63,"type":"numbered_list","properties":{"title":[["⁍",[["e","k"]]],["번째 가치함수 matrix에서 현재 상태 "],["⁍",[["e","s"]]],["에서 갈 수 있는 다음상태 "],["⁍",[["e","s’"]]],["에 저장돼 있는 가치함수 "],["⁍",[["e","v_{k}(s’)"]]],["을 불러온다. (보라색 부분중의 하나)"]]},"created_time":1676048004608,"last_edited_time":1676048254800,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"436b37a6-ac2e-4fce-8297-616e5656b474":{"role":"reader","value":{"id":"436b37a6-ac2e-4fce-8297-616e5656b474","version":23,"type":"numbered_list","properties":{"title":[["⁍",[["e","v_k(s')"]]],["에 할인율 "],["⁍",[["e","\\gamma"]]],["를 곱하고 그 상태로 가는 행동에 대한 보상 "],["⁍",[["e","R_s^a"]]],["을 더한다."]]},"content":["449c6783-7e76-446f-a280-b651c67f7319"],"created_time":1676048004608,"last_edited_time":1676048312055,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"449c6783-7e76-446f-a280-b651c67f7319":{"role":"reader","value":{"id":"449c6783-7e76-446f-a280-b651c67f7319","version":14,"type":"equation","properties":{"title":[["r_{(s,a)} + \\gamma v_k(s')"]]},"created_time":1676048311923,"last_edited_time":1676048317819,"parent_id":"436b37a6-ac2e-4fce-8297-616e5656b474","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"508c7fc4-005a-42f3-a9d9-edb16908fe71":{"role":"reader","value":{"id":"508c7fc4-005a-42f3-a9d9-edb16908fe71","version":4,"type":"numbered_list","properties":{"title":[["2번에서 구한 값에 그 행동을 할 확률, 즉 정책 값을 곱한다."]]},"content":["3488e7b6-b1f8-4a98-8eb3-fcdee2ba9949"],"created_time":1676048004608,"last_edited_time":1676048321787,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"3488e7b6-b1f8-4a98-8eb3-fcdee2ba9949":{"role":"reader","value":{"id":"3488e7b6-b1f8-4a98-8eb3-fcdee2ba9949","version":14,"type":"equation","properties":{"title":[["\\pi (a|s)(r_{(s,a)} + \\gamma v_k(s'))"]]},"created_time":1676048320009,"last_edited_time":1676048331085,"parent_id":"508c7fc4-005a-42f3-a9d9-edb16908fe71","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"c4d918e6-2f05-49ae-95c0-4b06c238a31f":{"role":"reader","value":{"id":"c4d918e6-2f05-49ae-95c0-4b06c238a31f","version":4,"type":"numbered_list","properties":{"title":[["3번을 모든 선택 가능한 행동에 대해 반복하고 그 값들을 더한다."]]},"content":["0dbe93fd-b9ed-4a37-8f56-875b97e88879"],"created_time":1676048004608,"last_edited_time":1676048333753,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"0dbe93fd-b9ed-4a37-8f56-875b97e88879":{"role":"reader","value":{"id":"0dbe93fd-b9ed-4a37-8f56-875b97e88879","version":14,"type":"equation","properties":{"title":[["v_{k+1}(s) = \\sum_{a \\in A} \\pi (a|s)(r_{(s,a)}+ \\gamma v_k(s'))"]]},"created_time":1676048332482,"last_edited_time":1676048343518,"parent_id":"c4d918e6-2f05-49ae-95c0-4b06c238a31f","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"e189dadb-04c5-4d4f-861b-74868ddfbdc7":{"role":"reader","value":{"id":"e189dadb-04c5-4d4f-861b-74868ddfbdc7","version":18,"type":"numbered_list","properties":{"title":[["4번 과정을 통해 더한 값을 "],["⁍",[["e","K+1"]]],["번째 가치함수 matrix의 상태 "],["⁍",[["e","s"]]],["자리에 저장한다."]]},"content":["5b219ef3-0e38-45f8-868e-4fa24836b764"],"created_time":1676048004608,"last_edited_time":1676048378320,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"5b219ef3-0e38-45f8-868e-4fa24836b764":{"role":"reader","value":{"id":"5b219ef3-0e38-45f8-868e-4fa24836b764","version":10,"type":"text","created_time":1676048347013,"last_edited_time":1676048347808,"parent_id":"e189dadb-04c5-4d4f-861b-74868ddfbdc7","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"f6797a9e-fe50-427a-86a8-7cacec5d4d31":{"role":"reader","value":{"id":"f6797a9e-fe50-427a-86a8-7cacec5d4d31","version":7,"type":"numbered_list","properties":{"title":[["1-5 과정을 모든 "],["⁍",[["e","s \\in S"]]],["에 대해 반복한다."]]},"created_time":1676048004608,"last_edited_time":1676048410876,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"07999fe1-5d08-4087-9540-ace96e7cd206":{"role":"reader","value":{"id":"07999fe1-5d08-4087-9540-ace96e7cd206","version":10,"type":"image","properties":{"source":[["https://longshiine.github.io/2021/01/24/Reinforcement-Learning-Basic-4/policyIteration.png"]],"caption":[["policy evaluation"]]},"format":{"block_width":576,"block_full_width":false,"block_page_width":false},"created_time":1676048004608,"last_edited_time":1676048423668,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"f2b89802-4167-41a0-a6a2-08a87ad96d1b":{"role":"reader","value":{"id":"f2b89802-4167-41a0-a6a2-08a87ad96d1b","version":46,"type":"text","properties":{"title":[["이것은 한번의 정책평가 과정이다. 하지만 한번의 정책평가로서는 제대로 평가를 할 수 없어, 이 과정을 여러번 반복하는데, v_1으로 시작해서 무한히 반복하면 참 "],["⁍",[["e","v_\\pi"]]],["가 될 수 있다."]]},"created_time":1676048004609,"last_edited_time":1676048453775,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"08a4fc78-b9f3-41cf-96e5-7242b28d032e":{"role":"reader","value":{"id":"08a4fc78-b9f3-41cf-96e5-7242b28d032e","version":5,"type":"text","created_time":1676048424791,"last_edited_time":1676048424795,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"9620387b-d7ce-449b-99c1-7bd9ba846a16":{"role":"reader","value":{"id":"9620387b-d7ce-449b-99c1-7bd9ba846a16","version":2,"type":"sub_sub_header","properties":{"title":[["정책 발전 (Policy Improvement)",[["b"]]]]},"created_time":1676048004609,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"975bb71b-14a0-4722-a9cc-e3d3c7316963":{"role":"reader","value":{"id":"975bb71b-14a0-4722-a9cc-e3d3c7316963","version":2,"type":"text","properties":{"title":[["애초에 정책을 발전 시키지 않는다면 정책에 대한 평가는 의미가 없다. 그렇다면 정책 평가를 바탕으로 어떻게 정책을 발전시킬 수 있을까? 사실 정책 발전의 방법이 정해져있는 것은 아니다. 하지만 이 책에서는 가장 널리 알려진 "],["탐욕 정책 발전(Greedy Policy Improvement)",[["c"]]],["을 사용한다."]]},"created_time":1676048004609,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"6aabb530-6ea8-467e-86f5-8e159708f6f5":{"role":"reader","value":{"id":"6aabb530-6ea8-467e-86f5-8e159708f6f5","version":12,"type":"text","properties":{"title":[["에이전트가 해야할 일은 단순하다. 상태 "],["⁍",[["e","s"]]],["에서 선택 가능한 행동의 "],["⁍",[["e","q_\\pi (s,a)"]]],["  즉 큐함수 값을 비교하고 그중에서 가장 큰 값을 가지는 행동을 선택하면 된다. 이것을 "],["탐욕 정책 발전",[["b"]]],[" 이라고 하는데, 눈앞에 보이는 것 중에서 "],["당장에 가장 큰 이익을 추구하는 것과 같은 모습",[["b"]]],[" 이기 때문에 이러한 이름이 붙었다."]]},"created_time":1676048004609,"last_edited_time":1676048481217,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"3b0bc53b-45dd-4f7e-99eb-78ba22f11ae4":{"role":"reader","value":{"id":"3b0bc53b-45dd-4f7e-99eb-78ba22f11ae4","version":2,"type":"text","properties":{"title":[["탐욕 정책 발전을 통해 업데이트된 정책은 아래와 같다. max 함수와는 다르게 반환되는 것이 "],["행동",[["b"]]],[" 이다."]]},"created_time":1676048004609,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"63923709-5812-47db-92ad-19a8f7b9c5da":{"role":"reader","value":{"id":"63923709-5812-47db-92ad-19a8f7b9c5da","version":9,"type":"equation","properties":{"title":[["\\pi'(s) = argmax_{a \\in A} \\\\\\; q_\\pi (s,a)"]]},"created_time":1676048496345,"last_edited_time":1676048500482,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"46e63dfb-3cfd-4d59-b1af-03fccc419a69":{"role":"reader","value":{"id":"46e63dfb-3cfd-4d59-b1af-03fccc419a69","version":8,"type":"image","properties":{"source":[["https://longshiine.github.io/2021/01/24/Reinforcement-Learning-Basic-4/greedy.png"]],"caption":[["Greedy Policy Improvement"]]},"format":{"block_width":480,"block_full_width":false,"block_page_width":false},"created_time":1676048004609,"last_edited_time":1676048512411,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"3dcd404f-31cf-4b73-96ff-915705f568d2":{"role":"reader","value":{"id":"3dcd404f-31cf-4b73-96ff-915705f568d2","version":2,"type":"text","properties":{"title":[["탐욕 정책 발전을 통해 정책을 업데이트하면 이전 가치함수에 비해 업데이트된 정책으로 움직였을 때 받을 가치함수가 "],["무조건 크거나 같다.",[["b"]]],[" 다이타믹 프로그래밍에서는 이처럼 탐욕 정책 발전을 사용하여 가장 큰값의 가치함수를 가지는 최적 정책에 수렴할 수 있다."]]},"created_time":1676048004609,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"02638e30-844c-4154-8f90-63676415fd26":{"role":"reader","value":{"id":"02638e30-844c-4154-8f90-63676415fd26","version":5,"type":"text","created_time":1676048514271,"last_edited_time":1676048514273,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"fbaef7fb-05e5-47b7-a5df-ce10b63598c0":{"role":"reader","value":{"id":"fbaef7fb-05e5-47b7-a5df-ce10b63598c0","version":2,"type":"sub_sub_header","properties":{"title":[["정책 이터레이션 코드",[["b"]]]]},"created_time":1676048004609,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"4ab1dc50-b72d-4245-ad69-e2ed90878bf3":{"role":"reader","value":{"id":"4ab1dc50-b72d-4245-ad69-e2ed90878bf3","version":2,"type":"text","properties":{"title":[["우선 에이전트가 해야할 역할을 고려해서 전체 코드의 흐름을 보면 다음과 같다."]]},"created_time":1676048004609,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"f6f719bc-1287-4264-ac29-29d473c17b8b":{"role":"reader","value":{"id":"f6f719bc-1287-4264-ac29-29d473c17b8b","version":13,"type":"code","properties":{"title":[["class PolicyIteration:\n  def __init__(self, env):\n    # 환경에 대한 객체\n    self.env = env\n\n  # 정책 평가\n  def policy_evaluation(self):\n    pass\n\n  # 정책 발전\n  def policy_improvement(self):\n    pass\n\n  # 특정 상태에서 정책에 따른 행동\n  def get_action(self, state):\n    return action\n\nif __name__ == \"__main__\":\n  env = Env()\n  policy_iteration = PolicyIteration(env)\n  grid_world = GraphicDisplay(policy_iteration)\n  grid_world.mainloop()"]],"language":[["Python"]]},"created_time":1676048526899,"last_edited_time":1676048531568,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"7feb467c-2ebf-4403-be8e-adbc3d10d7fb":{"role":"reader","value":{"id":"7feb467c-2ebf-4403-be8e-adbc3d10d7fb","version":2,"type":"text","properties":{"title":[["에이전트가 알고 있는 환경(env)의 정보는 다음과 같다."]]},"created_time":1676048004611,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"d6eded13-56f7-4aba-a6bd-0af751cbc17c":{"role":"reader","value":{"id":"d6eded13-56f7-4aba-a6bd-0af751cbc17c","version":2,"type":"bulleted_list","properties":{"title":[["env.width, env.height",[["c"]]],[": 그리드월드의 너비와 높이"]]},"created_time":1676048004611,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"8e983f53-678b-4d75-b03f-8a4a6151ad78":{"role":"reader","value":{"id":"8e983f53-678b-4d75-b03f-8a4a6151ad78","version":2,"type":"bulleted_list","properties":{"title":[["env.state_after_aciton(state, action)",[["c"]]],[": 특정 상태에서 특정 행동을 했을 때 에이전트가 가는 다음 상태"]]},"created_time":1676048004611,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"40965d23-96d7-48db-a42d-4c5fceda0f69":{"role":"reader","value":{"id":"40965d23-96d7-48db-a42d-4c5fceda0f69","version":2,"type":"bulleted_list","properties":{"title":[["env.get_all_states()",[["c"]]],[": 존재하는 모든 상태"]]},"created_time":1676048004611,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"664fab5a-2e7f-4db8-a9d3-c70876a724ea":{"role":"reader","value":{"id":"664fab5a-2e7f-4db8-a9d3-c70876a724ea","version":2,"type":"bulleted_list","properties":{"title":[["env.get_reward(state, action)",[["c"]]],[": 특정 상태의 보상"]]},"created_time":1676048004611,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"7a0a384c-4cb0-4c7a-b275-824826799cc5":{"role":"reader","value":{"id":"7a0a384c-4cb0-4c7a-b275-824826799cc5","version":2,"type":"bulleted_list","properties":{"title":[["env.possible_actions",[["c"]]],[": 상,하,좌,우"]]},"created_time":1676048004611,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"dd091679-b8e9-47f5-af76-8fe661decfb5":{"role":"reader","value":{"id":"dd091679-b8e9-47f5-af76-8fe661decfb5","version":2,"type":"text","properties":{"title":[["정책 이터레이션은 "],["정책 평가",[["b"]]],[" 와 "],["정책 발전",[["b"]]],[" 으로 이뤄져 있다. 따라서 각각의 함수를 정의 한다."]]},"created_time":1676048004611,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"5dc03d34-66b8-4f22-9c65-13c05cf38d86":{"role":"reader","value":{"id":"5dc03d34-66b8-4f22-9c65-13c05cf38d86","version":5,"type":"text","created_time":1676048880836,"last_edited_time":1676048880839,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"c8f4ecdb-b655-4837-8aaa-ce4e9fbb7385":{"role":"reader","value":{"id":"c8f4ecdb-b655-4837-8aaa-ce4e9fbb7385","version":2,"type":"sub_sub_header","properties":{"title":[["policy_evaluation",[["b"]]]]},"created_time":1676048004611,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"aa396472-8773-4524-94f6-6926f768afd6":{"role":"reader","value":{"id":"aa396472-8773-4524-94f6-6926f768afd6","version":2,"type":"text","properties":{"title":[["정책 평가를 통해 에이전트는 모든 상태의 가치함수를 업데이트 한다. 모든 상태에 대해 "],["벨반 기대 방정식",[["b"]]],[" 의 계산이 끝나면 현재의 value_table에 next_value_table을 덮어쓰는 식으로 policy_evalutation을 진행한다."]]},"created_time":1676048004611,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"8b3e5264-f22f-46d6-b0f5-0410505d26cc":{"role":"reader","value":{"id":"8b3e5264-f22f-46d6-b0f5-0410505d26cc","version":9,"type":"equation","properties":{"title":[["v_{k+1}(s) = \\sum_{a \\in A} \\pi (a|s)(r_{(s,a)}+ \\gamma v_k(s'))"]]},"created_time":1676048551654,"last_edited_time":1676048555947,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"eea7bdcb-a15f-4e20-bea6-a49a0306cc94":{"role":"reader","value":{"id":"eea7bdcb-a15f-4e20-bea6-a49a0306cc94","version":2,"type":"text","properties":{"title":[["정책 평가에 사용되는 벨만 기대 방정식이다. 이때, "],["상태 변환 확률",[["b"]]],[" 을 1이라고 설정 했기 때문에, 다음상태 "],["은 만약 행동이 왼쪽일 경우 왼쪽에 있는 상태가 된다."]]},"created_time":1676048004611,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"aab9a3c3-ea53-4eac-8706-b99b01746f29":{"role":"reader","value":{"id":"aab9a3c3-ea53-4eac-8706-b99b01746f29","version":13,"type":"code","properties":{"title":[["# 벨만 기대 방정식을 통해 다음 가치함수를 계산하는 정책 평가\ndef policy_evaluation(self):\n    # 다음 가치함수 초기화\n    next_value_table = [[0.00] * self.env.width\n                        for _ in range(self.env.height)]\n\n    # 모든 상태에 대해서 벨만 기대방정식을 계산\n    for state in self.env.get_all_states():\n        value = 0.0\n        # 마침 상태의 가치 함수 = 0\n        if state == [2, 2]:\n            next_value_table[state[0]][state[1]] = value\n            continue\n\n        # 벨만 기대 방정식\n        for action in self.env.possible_actions:\n            next_state = self.env.state_after_action(state, action)\n            reward = self.env.get_reward(state, action)\n            next_value = self.get_value(next_state)\n            value += (self.get_policy(state)[action] *\n                      (reward + self.discount_factor * next_value))\n\n        next_value_table[state[0]][state[1]] = value\n\n    self.value_table = next_value_table"]],"language":[["Python"]]},"created_time":1676048566458,"last_edited_time":1676048571101,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"5972dd6f-bed7-48e6-9f57-f0d9db892cb3":{"role":"reader","value":{"id":"5972dd6f-bed7-48e6-9f57-f0d9db892cb3","version":7,"type":"bulleted_list","properties":{"title":[["⁍",[["e","\\sum_{a \\in A}"]]],[": "],["for action in self.env.possible_actions:",[["c"]]]]},"created_time":1676048004612,"last_edited_time":1676048601145,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"9fd587ad-a827-4613-977e-88529b9495d5":{"role":"reader","value":{"id":"9fd587ad-a827-4613-977e-88529b9495d5","version":7,"type":"bulleted_list","properties":{"title":[["⁍",[["e","\\pi (a|s)"]]],[": "],["self.get_policy(state)[action]",[["c"]]]]},"created_time":1676048004612,"last_edited_time":1676048596450,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"31fa1f80-12eb-4fa4-8fcb-f9eaf5d4591f":{"role":"reader","value":{"id":"31fa1f80-12eb-4fa4-8fcb-f9eaf5d4591f","version":7,"type":"bulleted_list","properties":{"title":[["⁍",[["e","r(s,a)"]]],[": "],["reward = self.env.get_reward(state, action)",[["c"]]]]},"created_time":1676048004612,"last_edited_time":1676048609549,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"a994bd67-5f1f-4d39-a7eb-89dbbab1389b":{"role":"reader","value":{"id":"a994bd67-5f1f-4d39-a7eb-89dbbab1389b","version":7,"type":"bulleted_list","properties":{"title":[["⁍",[["e","\\gamma"]]],[": "],["self.discount_factor",[["c"]]],[" (0.9)"]]},"created_time":1676048004613,"last_edited_time":1676048618734,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"9051a7d7-beb1-4813-a197-3aec9b85fe01":{"role":"reader","value":{"id":"9051a7d7-beb1-4813-a197-3aec9b85fe01","version":10,"type":"bulleted_list","properties":{"title":[["⁍",[["e","s’"]]],[": "],["next_state = self.env.state_after_action(state, action)",[["c"]]]]},"created_time":1676048004613,"last_edited_time":1676048625125,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"fa27001f-1a1b-4179-9d8c-089e75bf3e58":{"role":"reader","value":{"id":"fa27001f-1a1b-4179-9d8c-089e75bf3e58","version":7,"type":"bulleted_list","properties":{"title":[["⁍",[["e","v_k(s')"]]],[": "],["self.get_value(next_state)",[["c"]]]]},"created_time":1676048004613,"last_edited_time":1676048632689,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"5331209b-b3d2-4e0c-80be-f4f11e67131d":{"role":"reader","value":{"id":"5331209b-b3d2-4e0c-80be-f4f11e67131d","version":7,"type":"bulleted_list","properties":{"title":[["⁍",[["e","v_{k+1}(s)"]]],[": "],["next_value_table[state[0]][state[1]]",[["c"]]]]},"created_time":1676048004613,"last_edited_time":1676048640866,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"b93ea5cb-595d-492e-a880-b93a05e55483":{"role":"reader","value":{"id":"b93ea5cb-595d-492e-a880-b93a05e55483","version":2,"type":"text","properties":{"title":[["get_policy 함수를 통해 각 상태에서 각 행동에 대한 확률값을 구한다. 그리고 다음 상태로 갔을 때 받을 보상과 다음상태의 가치함수를 할인율을 적용하여 더한다. 정책이 각 행동에 대한 확률을 나타내기 때문에 모든 행동에 대해 value를 계산하고 더하면 기댓값을 계산한 것이 된다."]]},"created_time":1676048004613,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"0485d509-46d2-495e-aec9-2ea6d0152164":{"role":"reader","value":{"id":"0485d509-46d2-495e-aec9-2ea6d0152164","version":5,"type":"text","created_time":1676048885873,"last_edited_time":1676048885875,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"ac6e2f45-99ac-49b9-9a1d-6204dbacecea":{"role":"reader","value":{"id":"ac6e2f45-99ac-49b9-9a1d-6204dbacecea","version":2,"type":"sub_sub_header","properties":{"title":[["policy_improvement",[["b"]]]]},"created_time":1676048004613,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"cd6323af-8e5d-4a56-be48-5101440de333":{"role":"reader","value":{"id":"cd6323af-8e5d-4a56-be48-5101440de333","version":2,"type":"text","properties":{"title":[["정책 평가를 통해 정책을 평가하면 그에 따른 새로운 가치함수를 얻는다. 에이전트는 이제 새로운 가치함수를 통해 정책을 업데이트 해야한다. 정책발전에는 탐욕 정책 발전을 사용한다."]]},"created_time":1676048004613,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"78698025-422e-4a27-9baa-b31967071d4e":{"role":"reader","value":{"id":"78698025-422e-4a27-9baa-b31967071d4e","version":13,"type":"code","properties":{"title":[["# 현재 가치 함수에 대해서 탐욕 정책 발전\ndef policy_improvement(self):\n    next_policy = self.policy_table\n    for state in self.env.get_all_states():\n        if state == [2, 2]:\n            continue\n\n        value_list = []\n        # 반환할 정책 초기화\n        result = [0.0, 0.0, 0.0, 0.0]\n\n        # 모든 행동에 대해서 [보상 + (할인율 * 다음 상태 가치함수)] 계산\n        for index, action in enumerate(self.env.possible_actions):\n            next_state = self.env.state_after_action(state, action)\n            reward = self.env.get_reward(state, action)\n            next_value = self.get_value(next_state)\n            value = reward + self.discount_factor * next_value\n            value_list.append(value)\n\n        # 받을 보상이 최대인 행동들에 대해 탐욕 정책 발전\n        max_idx_list = np.argwhere(value_list == np.amax(value_list))\n        max_idx_list = max_idx_list.flatten().tolist()\n        prob = 1 / len(max_idx_list)\n\n        for idx in max_idx_list:\n            result[idx] = prob\n\n        next_policy[state[0]][state[1]] = result\n\n    self.policy_table = next_policy"]],"language":[["Python"]]},"created_time":1676048658736,"last_edited_time":1676048662560,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"d830b739-0bf4-4fd1-b049-50e33b3fc1da":{"role":"reader","value":{"id":"d830b739-0bf4-4fd1-b049-50e33b3fc1da","version":2,"type":"text","properties":{"title":[["탐욕 정책 발전은 가치가 가장 높은 하나의 행동을 선택하는 것이다. 하지만 이 예제에서와 같이 현재 상태에서 가장 좋은 행동이 여러개일 수도 있다. 그럴때에는 가장 좋은 행동들을 동일한 확률로 선택하는 정책으로 업데이트한다."]]},"created_time":1676048004614,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"4220183b-fff3-4ff0-8e7e-8d3431586557":{"role":"reader","value":{"id":"4220183b-fff3-4ff0-8e7e-8d3431586557","version":7,"type":"bulleted_list","properties":{"title":[["현재 상태에서 가능한 행동에 대해 "],["⁍",[["e","r_{(s,a)}+\\gamma v_k(s')"]]],["을 계산한다."]]},"created_time":1676048004614,"last_edited_time":1676048676947,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"9992fc68-e14b-494f-bdb8-797373f67e32":{"role":"reader","value":{"id":"9992fc68-e14b-494f-bdb8-797373f67e32","version":2,"type":"bulleted_list","properties":{"title":[["계산한 값을 value_list에 저장한다."]]},"created_time":1676048004614,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"e3f0a5a9-d592-456d-9a55-5b4d7a1d813f":{"role":"reader","value":{"id":"e3f0a5a9-d592-456d-9a55-5b4d7a1d813f","version":2,"type":"bulleted_list","properties":{"title":[["max 함수를 통해 value_list에 담긴 값 중 가장 큰 값을 알아낸다."]]},"created_time":1676048004614,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"4d27cf55-5b63-447f-91bc-be5ced1462e6":{"role":"reader","value":{"id":"4d27cf55-5b63-447f-91bc-be5ced1462e6","version":2,"type":"bulleted_list","properties":{"title":[["argwhere를 통해 가장 큰 값의 index를 알아내고(여러개면 여러개 반환) max_idx_list에 저장한다."]]},"created_time":1676048004614,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"788dbad4-fa04-421d-87ca-95d978c9cab4":{"role":"reader","value":{"id":"788dbad4-fa04-421d-87ca-95d978c9cab4","version":2,"type":"bulleted_list","properties":{"title":[["max_idx_list의 길이를 바탕으로 확률을 계산하여 확률값을 계산하고 result에 저장한다."]]},"created_time":1676048004614,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"3dbf01fd-4555-49d7-baa0-958c5ed84f9a":{"role":"reader","value":{"id":"3dbf01fd-4555-49d7-baa0-958c5ed84f9a","version":2,"type":"text","properties":{"title":[["이렇게 되면 policy_table에는 업데이트 된 정책(각 상태의 행동에 대한 확률)이 저장된다."]]},"created_time":1676048004614,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"6665b56a-ee1b-43dc-b33d-9bafaa3688ce":{"role":"reader","value":{"id":"6665b56a-ee1b-43dc-b33d-9bafaa3688ce","version":5,"type":"text","created_time":1676048733120,"last_edited_time":1676048733124,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"1c341cfb-4427-4ef5-9976-fd8b9ecf6e6e":{"role":"reader","value":{"id":"1c341cfb-4427-4ef5-9976-fd8b9ecf6e6e","version":2,"type":"quote","properties":{"title":[["정책 이터레이션에서 에이전트는 정책 평가와 정책 발전을 반복하여 최적 정책을 찾아낸다."]]},"created_time":1676048004614,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"06599b0b-2dcd-461f-93cc-b2526f62de3f":{"role":"reader","value":{"id":"06599b0b-2dcd-461f-93cc-b2526f62de3f","version":6,"type":"image","properties":{"source":[["https://longshiine.github.io/2021/01/24/Reinforcement-Learning-Basic-4/optimalpolicy.png"]],"caption":[["정책 이터레이션으로 구한 최적 정책"]]},"format":{"block_width":432,"block_full_width":false,"block_page_width":false},"created_time":1676048004615,"last_edited_time":1676048742686,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"d29295f0-0757-4d8d-ad72-e2bfd3d09c25":{"role":"reader","value":{"id":"d29295f0-0757-4d8d-ad72-e2bfd3d09c25","version":4,"type":"text","created_time":1676048004615,"last_edited_time":1676048740982,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"4a6c2369-4742-43a1-a5dd-01af6117af60":{"role":"reader","value":{"id":"4a6c2369-4742-43a1-a5dd-01af6117af60","version":2,"type":"sub_sub_header","properties":{"title":[["가치 이터레이션",[["b"]]]]},"created_time":1676048004615,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"d8c99204-031d-468b-9b51-4f1fd15a20c5":{"role":"reader","value":{"id":"d8c99204-031d-468b-9b51-4f1fd15a20c5","version":2,"type":"text","properties":{"title":[["정책 이터레이션과 가치 이터레이션의 중요한 차이점은 정책 이터레이션에서는 "],["정책 평가와 정책 발전",[["b"]]],[" 으로 단계가 나누어져 있다면 가치 이터레이션에서는 그렇지 않다는 것이다."]]},"created_time":1676048004615,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"57daf930-60bb-4aaa-b417-25378a3dbbeb":{"role":"reader","value":{"id":"57daf930-60bb-4aaa-b417-25378a3dbbeb","version":2,"type":"text","properties":{"title":[["가치 이터레이션은 현재의 가치함수가 최적 정책에 대한 가치함수라고 가정하기 때문에 정책을 발전하는 함수가 따로 필요하지 않다. 따라서 최적 행동을 반환하는 get_action 함수를 정책을 출력하는 데 대신 사용한다."]]},"created_time":1676048004615,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"0dac76b4-1fa1-4826-8136-29ca94a52c5f":{"role":"reader","value":{"id":"0dac76b4-1fa1-4826-8136-29ca94a52c5f","version":2,"type":"text","properties":{"title":[["주요 코드의 전체적인 흐름은 다음과 같다."]]},"created_time":1676048004615,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"e508dc87-4740-4adf-b110-57734c7dbde2":{"role":"reader","value":{"id":"e508dc87-4740-4adf-b110-57734c7dbde2","version":13,"type":"code","properties":{"title":[["class ValueIteration:\n  def __init__(self, env):\n    # 환경 객체 생성\n    self.env = env\n\n  # 벨만 최적 방정식을 통해 다음 가치함수 계산\n  def value_iteration(self):\n    return\n\n  # 현재 가치함수로부터 행동을 반환\n  def get_action(self, state):\n    return\n\n  def get_value(self, state):\n    return\n\nif __name__ == \"__main__\":\n  env = Env()\n  value_iteration = ValueIteration(env)\n  grid_world = GraphicDisplay(value_iteration)\n  gird_world.mainloop()"]],"language":[["Python"]]},"created_time":1676048756350,"last_edited_time":1676048759122,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"9a5a1932-2932-4f30-b040-e6a972e308da":{"role":"reader","value":{"id":"9a5a1932-2932-4f30-b040-e6a972e308da","version":2,"type":"text","properties":{"title":[["정책 이터레이션에서는 policy_evaluation 함수에서 벨만 기대 방정식을 통해 다음 가치함수를 계산했다. 가치 이터레이션에서는 value_iteration 함수를 통해 다음 가치함수를 계산한다."]]},"created_time":1676048004622,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"1859020c-d801-4027-aece-863b850b4c15":{"role":"reader","value":{"id":"1859020c-d801-4027-aece-863b850b4c15","version":13,"type":"code","properties":{"title":[["# 벨만 최적 방정식을 통해 다음 가치 함수 계산\ndef value_iteration(self):\n    # 다음 가치함수 초기화\n    next_value_table = [[0.0] * self.env.width\n                        for _ in range(self.env.height)]\n\n    # 모든 상태에 대해서 벨만 최적방정식을 계산\n    for state in self.env.get_all_states():\n        # 마침 상태의 가치 함수 = 0\n        if state == [2, 2]:\n            next_value_table[state[0]][state[1]] = 0.0\n            continue\n\n        # 벨만 최적 방정식\n        value_list = []\n        for action in self.env.possible_actions:\n            next_state = self.env.state_after_action(state, action)\n            reward = self.env.get_reward(state, action)\n            next_value = self.get_value(next_state)\n            value_list.append((reward + self.discount_factor * next_value))\n\n        # 최댓값을 다음 가치 함수로 대입\n        next_value_table[state[0]][state[1]] = max(value_list)\n\n    self.value_table = next_value_table"]],"language":[["Python"]]},"created_time":1676048770597,"last_edited_time":1676048773691,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"a735e9be-65ce-4a2e-b704-a9d0870073eb":{"role":"reader","value":{"id":"a735e9be-65ce-4a2e-b704-a9d0870073eb","version":10,"type":"equation","properties":{"title":[["v_{k+1}(s) = max_{a \\in A}(r_{s,a} + \\gamma v_k(s'))"]]},"created_time":1676048784404,"last_edited_time":1676048791963,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"3cb6e9fb-9632-4560-bb2b-0a338fc0740c":{"role":"reader","value":{"id":"3cb6e9fb-9632-4560-bb2b-0a338fc0740c","version":2,"type":"bulleted_list","properties":{"title":[["위의 벨만 최적 방정식을 계산하여 value_list에 저장한다."]]},"created_time":1676048004623,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"95780d7b-b831-45ac-9f5b-f4bfa179dec1":{"role":"reader","value":{"id":"95780d7b-b831-45ac-9f5b-f4bfa179dec1","version":2,"type":"bulleted_list","properties":{"title":[["value_list에 저장된 값중 최대의 값을 새로운 가치함수로 저장한다."]]},"created_time":1676048004623,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"ac2684f3-48d6-4c61-a72c-4292a535a030":{"role":"reader","value":{"id":"ac2684f3-48d6-4c61-a72c-4292a535a030","version":5,"type":"bulleted_list","created_time":1676048802041,"last_edited_time":1676048802044,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"60766959-e793-4704-8972-a8036e3d694e":{"role":"reader","value":{"id":"60766959-e793-4704-8972-a8036e3d694e","version":7,"type":"code","properties":{"title":[["# 현재 가치 함수로부터 행동을 반환\ndef get_action(self, state):\n    if state == [2, 2]:\n        return []\n\n    # 모든 행동에 대해 큐함수 (보상 + (감가율 * 다음 상태 가치함수))를 계산\n    value_list = []\n    for action in self.env.possible_actions:\n        next_state = self.env.state_after_action(state, action)\n        reward = self.env.get_reward(state, action)\n        next_value = self.get_value(next_state)\n        value = (reward + self.discount_factor * next_value)\n        value_list.append(value)\n\n    # 최대 큐 함수를 가진 행동(복수일 경우 여러 개)을 반환\n    max_idx_list = np.argwhere(value_list == np.amax(value_list))\n    action_list = max_idx_list.flatten().tolist()\n    return action_list"]],"language":[["Python"]]},"created_time":1676048802542,"last_edited_time":1676048805611,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"aa388efa-b0ca-46ed-acec-67dfa3ccb638":{"role":"reader","value":{"id":"aa388efa-b0ca-46ed-acec-67dfa3ccb638","version":2,"type":"bulleted_list","properties":{"title":[["모든 행동에 대해 큐함수를 구한다."]]},"created_time":1676048004624,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"3d6bd359-8bea-4217-ab0d-a55111c021ce":{"role":"reader","value":{"id":"3d6bd359-8bea-4217-ab0d-a55111c021ce","version":2,"type":"bulleted_list","properties":{"title":[["그 중 가장 큰 value 값을 가지는 행동의 인덱스를 가져온다(여러개라면 여러개 모두 가져온다)"]]},"created_time":1676048004624,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"b642b5d2-dbdc-40f3-9145-57e09fe0d740":{"role":"reader","value":{"id":"b642b5d2-dbdc-40f3-9145-57e09fe0d740","version":2,"type":"bulleted_list","properties":{"title":[["행동들을 모두 action_list에 저장한다."]]},"created_time":1676048004624,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"64ebfc2b-455d-4503-b71f-6227143e45f8":{"role":"reader","value":{"id":"64ebfc2b-455d-4503-b71f-6227143e45f8","version":2,"type":"text","properties":{"title":[["이 예제에 대해서는 Calculate 6번 정도에 가치함수가 거의 수렴하게 된다. 이때 수렴한 가치함수의 값을 토대로 정책을 출력해보면 아래의 그림과 같다."]]},"created_time":1676048004624,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"8448cf4f-6eba-488e-aae4-7a935dfbeee0":{"role":"reader","value":{"id":"8448cf4f-6eba-488e-aae4-7a935dfbeee0","version":6,"type":"image","properties":{"source":[["https://longshiine.github.io/2021/01/24/Reinforcement-Learning-Basic-4/valueiteration.png"]],"caption":[["가치 이터레이션으로 구한 최적 정책"]]},"format":{"block_width":384,"block_full_width":false,"block_page_width":false},"created_time":1676048004624,"last_edited_time":1676048817668,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"719ae054-9866-4f7f-acfe-1e886304f81c":{"role":"reader","value":{"id":"719ae054-9866-4f7f-acfe-1e886304f81c","version":4,"type":"text","created_time":1676048004625,"last_edited_time":1676048815842,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"a292d19a-0385-45b9-bf57-1b2f2f4b4592":{"role":"reader","value":{"id":"a292d19a-0385-45b9-bf57-1b2f2f4b4592","version":2,"type":"sub_sub_header","properties":{"title":[["다이내믹 프로그래밍의 한계와 강화학습",[["b"]]]]},"created_time":1676048004625,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"d1464267-b3ee-4e00-8df1-7bf05db00290":{"role":"reader","value":{"id":"d1464267-b3ee-4e00-8df1-7bf05db00290","version":2,"type":"text","properties":{"title":[["벨만 방정식을 이용한 다이내믹 프로그래밍으로서 정책 이터레이션과 가치 이터레이션을 살펴봤다. 하지만 다이내믹 프로그래밍은 "],["계산을 빠르게 하는 것이지 학습을 하는 것",[["b"]]],[" 은 아니다. 그렇다면 이러한 다이내믹 프로그래밍의 한계는 무엇일까?"]]},"created_time":1676048004625,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"22c280db-1ea0-4feb-92e0-ef6ab84300ba":{"role":"reader","value":{"id":"22c280db-1ea0-4feb-92e0-ef6ab84300ba","version":2,"type":"text","properties":{"title":[["1. 계산복잡도",[["b"]]],[" : 문제의 규모가 커지만 계산으로 푸는데에 한계. "],["계산복잡도",[["c"]]],[" = 상태 크기의 3제곱"],["2. 차원의 저주",[["b"]]],[" : 그리드월드의 상태의 차원은 2차원. 상태의 차원이 늘어나면 상태의 수가 지수적으로 증가"],["3. 환경에 대한 완벽한 정보가 필요",[["b"]]],[" : 보상과 상태 변환 확률을 정확히 안다는 가정 필요. 보통은 이 정보를 확실히 알 수 없음."]]},"created_time":1676048004625,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"02bb229e-82e5-4e24-99d2-9c461e8b892e":{"role":"reader","value":{"id":"02bb229e-82e5-4e24-99d2-9c461e8b892e","version":2,"type":"text","properties":{"title":[["현실세계의 환경에 놓인 문제를 풀어내는 데에는 위의 세 가지 한계가 치명적으로 작용한다. 때문에 이러한 한계를 극복하기 위해 환경을 모르지만 "],["환경과의 상호작용을 통해 경험을 바탕으로 학습하는 방법",[["b"]]],[" 이 등장한다. 바로 "],["강화학습",[["c"]]],["이다."]]},"created_time":1676048004625,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"fced0da0-6e5f-44fc-a591-f842a73c6543":{"role":"reader","value":{"id":"fced0da0-6e5f-44fc-a591-f842a73c6543","version":5,"type":"text","created_time":1676048828066,"last_edited_time":1676048828071,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"cfbdb246-41b1-4df6-9f29-db86499c30b6":{"role":"reader","value":{"id":"cfbdb246-41b1-4df6-9f29-db86499c30b6","version":2,"type":"sub_sub_header","properties":{"title":[["모델없이 학습하는 강화학습",[["b"]]]]},"created_time":1676048004625,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"fcab1ba8-fec4-49c5-94df-22771a834403":{"role":"reader","value":{"id":"fcab1ba8-fec4-49c5-94df-22771a834403","version":2,"type":"text","properties":{"title":[["환경의 모델이란 무엇일까? MDP에서 환경의 모델은 상태 변환 확률과 보상이다."]]},"created_time":1676048004625,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"e33832df-0d2c-4253-b2b8-960e377c6b14":{"role":"reader","value":{"id":"e33832df-0d2c-4253-b2b8-960e377c6b14","version":9,"type":"equation","properties":{"title":[["Model = P_{ss'}^a, \\\\\\; r(s,a)"]]},"created_time":1676048832846,"last_edited_time":1676048843786,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"d2a9aea3-d3c9-447b-9bc2-497ed74757a3":{"role":"reader","value":{"id":"d2a9aea3-d3c9-447b-9bc2-497ed74757a3","version":2,"type":"text","properties":{"title":[["현재 이 책에서 다루고 싶은 모델은 "],["수학적 모델",[["b"]]],[" 로서 시스템에 입력이 들어왔을 때 시스템이 어떤 출력을 내는지에 대한 방정식이다. 이처럼 입력과 출력의 관계를 식으로 나타내는 과정을 "],["모델링(Modeling)",[["c"]]],["이라고 한다."]]},"created_time":1676048004625,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"22d14221-a92b-4cc3-b43f-6a38b8c11b10":{"role":"reader","value":{"id":"22d14221-a92b-4cc3-b43f-6a38b8c11b10","version":2,"type":"text","properties":{"title":[["사실 입력과 출력 사이의 방정식은 정확할 수가 없다. 방정식에서는 A라는 입력이 들어와서 B라는 출력이 나오더라도 실제 세상에서는 B라는 출력이 절대로 나오지 않는다. 모델은 정확하면 정확할수록 복잡하며 공기나 바람같은 "],["자연현상을 정확하게 모델링하는 것은 불가능",[["b"]]],[" 에 가깝다. 게임에서는 사실 사람이 환경을 만들었고, 사람이 정해준대로만 게임이 작동하므로 "],["모델링 오차는 없다",[["b"]]],[" 고 볼 수 있다. 하지만 게임을 벗어난다면 글쎄…🤔"]]},"created_time":1676048004625,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"dd69fead-1a56-44ee-8190-04300d36b709":{"role":"reader","value":{"id":"dd69fead-1a56-44ee-8190-04300d36b709","version":2,"type":"text","properties":{"title":[["모델을 정확히 알기 어려운 경우, 시스템의 입력과 출력 사이의 관계를 알기 위해 두가지 접근 방법으로 접근해 볼 수 있다."]]},"created_time":1676048004625,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"42484c42-5aea-40a4-aca6-592b6e4b4030":{"role":"reader","value":{"id":"42484c42-5aea-40a4-aca6-592b6e4b4030","version":2,"type":"numbered_list","properties":{"title":[["할 수 있는 선에서 정확한 모델링을 한 다음, 모델링 오차에 대한 부분을 실험을 통해 조정한다."]]},"created_time":1676048004625,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"ffc848bb-b4a8-4cf7-825c-3006065ab0f1":{"role":"reader","value":{"id":"ffc848bb-b4a8-4cf7-825c-3006065ab0f1","version":2,"type":"numbered_list","properties":{"title":[["모델 없이 환경과의 상호작용을 통해 입력과 출력 사이의 관계를 학습한다."]]},"created_time":1676048004625,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"9f7c36d7-cb3b-4dca-bd21-bd6a6cd30122":{"role":"reader","value":{"id":"9f7c36d7-cb3b-4dca-bd21-bd6a6cd30122","version":2,"type":"text","properties":{"title":[["1번은 학습의 개념없이 고전적으로 많이 적용하는 방법이며 시스템의 안정성을 보장한다. 하지만 문제가 복잡해지고 어려워질수록 한계가 있다."]]},"created_time":1676048004625,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"4a2910a4-4c43-4d75-a3de-6c1512cf727d":{"role":"reader","value":{"id":"4a2910a4-4c43-4d75-a3de-6c1512cf727d","version":2,"type":"text","properties":{"title":[["2번은 학습의 개념이 들어간다. 학습의 특성상 모든 상황에서 동일하게 작동한다고 보장할 수 없지만 많은 복잡한 문제에서 모델이 필요없는 것은 장점이다. 2번 방법이 바로 이 책의 주제인 "],["강화학습",[["c"]]],[" 이다."]]},"created_time":1676048004625,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"e79781b5-513e-4675-a77b-fc1b7a8cb5f4":{"role":"reader","value":{"id":"e79781b5-513e-4675-a77b-fc1b7a8cb5f4","version":5,"type":"text","created_time":1676048024802,"last_edited_time":1676048024804,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"a73badff-d6df-476c-8807-d22c8d5d0a32":{"role":"reader","value":{"id":"a73badff-d6df-476c-8807-d22c8d5d0a32","version":2,"type":"sub_sub_header","properties":{"title":[["정리",[["b"]]]]},"created_time":1676048004625,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"e25a7778-6478-407c-926a-590b0e6eef33":{"role":"reader","value":{"id":"e25a7778-6478-407c-926a-590b0e6eef33","version":2,"type":"bulleted_list","properties":{"title":[["정책 이터레이션",[["c"]]],[": 벨만 기대 방정식을 이용해 정책을 평가하고, 탐욕 정책 발전을 이용해 정책 발전"]]},"created_time":1676048004625,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"9008d223-e0bc-40b4-9557-0334cddb9499":{"role":"reader","value":{"id":"9008d223-e0bc-40b4-9557-0334cddb9499","version":2,"type":"bulleted_list","properties":{"title":[["가치 이터레이션",[["c"]]],[": 최적 정책을 가정하고 벨만 최적 방정식 이용. 정책이 직접적으로 주어지지 않아 큐함수를 통해 행동 선택"]]},"created_time":1676048004625,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"8d6dabcc-07da-4e35-922a-9ca5ccda00e9":{"role":"reader","value":{"id":"8d6dabcc-07da-4e35-922a-9ca5ccda00e9","version":2,"type":"bulleted_list","properties":{"title":[["다이내믹 프로그래밍의 한계",[["c"]]],[": 계산 복잡도, 차원의 저주, 환경에 대한 완벽한 정보 필요"]]},"created_time":1676048004625,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"c105cbac-1a9a-45de-9ed3-f45372210cc7":{"role":"reader","value":{"id":"c105cbac-1a9a-45de-9ed3-f45372210cc7","version":7,"type":"text","created_time":1676048023976,"last_edited_time":1676048026321,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"c28457b0-2739-408c-9396-dfdd77117fad":{"role":"reader","value":{"id":"c28457b0-2739-408c-9396-dfdd77117fad","version":2,"type":"sub_sub_header","properties":{"title":[["3장 한줄평",[["b"]]]]},"created_time":1676048004625,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"5de2d98e-72f6-438c-b359-fdf0fbb16bb3":{"role":"reader","value":{"id":"5de2d98e-72f6-438c-b359-fdf0fbb16bb3","version":2,"type":"quote","properties":{"title":[["짧고 간결하게 쓰는게 오히려 힘들다.."]]},"created_time":1676048004625,"last_edited_time":1676048004645,"parent_id":"3931006b-7fcb-4441-b508-0a5ba536daaf","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"3d335e41-f00a-45df-8432-34a266c7566a":{"role":"reader","value":{"id":"3d335e41-f00a-45df-8432-34a266c7566a","version":480,"type":"page","properties":{"title":[["About"]]},"content":["e1067486-5fb5-4dc3-9415-54abc63fc3a5","ab253a7b-1dec-4620-8c4e-e99f94e1f6c9","460c887f-cc3d-4351-8671-7cfbc6d3bc1d","c62b8a9d-e8cb-4c5b-be5a-48412e956c95","abe79af7-1342-438a-9a59-2d0b1ce59007","88d551b8-6b87-4540-89b9-3619a8fe4059","ae31ce74-317a-45ae-9dd3-16c14e764361","78f83c2a-05c2-4033-bcd8-945d700b2952","fd280029-9a4c-47fe-8e3c-ce8d3f146671","6a0d0c32-f1bc-4277-a128-33da8613ede4","2bcccf09-dccf-4896-8819-255cf75aabaf","d84b4bd5-3b6b-45dd-a807-5fea62c5213b","8b7fb4d3-0195-4f61-a0eb-330792f84053","e9394173-e197-4fad-a1db-ac74fdba3738","cb29948c-7258-4b05-9bfa-25bf19ad0946","a4e4243c-25aa-480d-a9ee-0af2230c7d76","172e2e25-e319-4766-ad38-02fc168994cd","6c428956-d84e-4855-a998-7a79f1fc8ea5","90f1cecf-6258-4b54-b0ee-26ee4d950872","3eec82e9-0211-4271-8c30-1dbc86ab3be1","8ce5f430-8c4c-4e8f-b6a4-23396731b27d","aeb9d85e-fffa-4c34-b253-cb0499f047a2","3d37f4c7-8064-4792-a863-234d8b4d05a6","77daa708-adbc-4092-b7f7-9e3c8a988049","26e3aca8-3e09-4a37-8c92-3c95cf579bbb"],"format":{"page_icon":"https://s3-us-west-2.amazonaws.com/secure.notion-static.com/fc853118-df50-43ed-96d9-0711493d5e25/jang_inspiration_logo.png","page_cover":"/images/page-cover/nasa_reduced_gravity_walking_simulator.jpg","copied_from_pointer":{"id":"f1199d37-579b-41cb-abfc-0b5174f4256a","table":"block","spaceId":"fde5ac74-eea3-4527-8f00-4482710e1af3"},"page_cover_position":0.5},"created_time":1675998770866,"last_edited_time":1677390731360,"parent_id":"d83b5165-627c-41b3-82f0-f1cbf904e176","parent_table":"block","alive":true,"copied_from":"f1199d37-579b-41cb-abfc-0b5174f4256a","file_ids":["c8028bee-f4c7-4736-8c36-fffcab5d977e","9407e769-d877-4de9-bbaa-9e5626d971ed","07d5a5b4-de2a-4322-bfcc-c4ade3a63b86","fc853118-df50-43ed-96d9-0711493d5e25"],"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"d83b5165-627c-41b3-82f0-f1cbf904e176":{"role":"reader","value":{"id":"d83b5165-627c-41b3-82f0-f1cbf904e176","version":17,"type":"column","content":["e1fe2c9c-6eb8-412c-9874-a8ac2fce9ed8","3d335e41-f00a-45df-8432-34a266c7566a"],"format":{"column_ratio":0.25},"created_time":1676020529237,"last_edited_time":1676091451534,"parent_id":"1109cf3f-d8f9-4532-a13e-1af177ad4fdd","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"1109cf3f-d8f9-4532-a13e-1af177ad4fdd":{"role":"reader","value":{"id":"1109cf3f-d8f9-4532-a13e-1af177ad4fdd","version":13,"type":"column_list","content":["91ac285d-9a69-4ec4-96f6-9b046a15647c","028efbb4-417e-4742-8474-dd7a2ddeb8ee","d63c036c-b99a-4aa9-a068-87abc40d37a6","d83b5165-627c-41b3-82f0-f1cbf904e176"],"format":{"block_width":720,"block_full_width":false,"block_page_width":true},"created_time":1676020365844,"last_edited_time":1676091451534,"parent_id":"6246082f-4014-4d06-98ab-59e9840b298a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"e1067486-5fb5-4dc3-9415-54abc63fc3a5":{"role":"reader","value":{"id":"e1067486-5fb5-4dc3-9415-54abc63fc3a5","version":86,"type":"text","properties":{"title":[["Instagram",[["h","blue_background"],["a","https://www.instagram.com/jang.inspiration/"]]],[" • "],["GitHub",[["h","teal_background"],["a","https://github.com/longshiine"]]],[" • "],["LinkedIn",[["h","pink_background"],["a","https://www.linkedin.com/in/jangyeong-kim-b7924422a/"]]]]},"format":{"copied_from_pointer":{"id":"0ab4ae84-463a-4b31-9430-f4ed513c5fc2","table":"block","spaceId":"fde5ac74-eea3-4527-8f00-4482710e1af3"}},"created_time":1675998770872,"last_edited_time":1676960330564,"parent_id":"3d335e41-f00a-45df-8432-34a266c7566a","parent_table":"block","alive":true,"copied_from":"0ab4ae84-463a-4b31-9430-f4ed513c5fc2","created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}}},"collection":{"ba8460cf-4781-486e-8976-01358ef4659d":{"role":"reader","value":{"id":"ba8460cf-4781-486e-8976-01358ef4659d","version":117,"schema":{";KhU":{"name":"Last Updated","type":"last_edited_time"},"==~K":{"name":"Public","type":"checkbox"},"=bhc":{"name":"Curated","type":"checkbox"},"AfoN":{"name":"Category","type":"select","options":[{"id":"20579d4b-5ad0-469e-8946-2560e458bb81","color":"orange","value":"강화학습"},{"id":"b78b3694-0089-4efb-802c-c288e6190037","color":"green","value":"알고리즘"},{"id":"34c6aef1-b3b3-490e-9369-b5a385f7da4e","color":"blue","value":"딥러닝"},{"id":"67465999-8b72-4fe4-92ce-db5a812b880a","color":"brown","value":"공학수학"},{"id":"8b385c1c-8e95-4a02-b009-6215a718ebef","color":"pink","value":"글쓰기"},{"id":"435e1850-8e40-4b73-8496-0a3f694b6aeb","color":"purple","value":"스타트업"},{"id":"2b34c718-9490-4d2c-8c1d-949ea1a5010c","color":"gray","value":"독서"}]},"BN]P":{"name":"Tags","type":"multi_select","options":[{"id":"210cfb45-7eae-44e5-83dc-dba99aa3a853","color":"green","value":"Node.js"},{"id":"02b16a55-92ee-455d-9449-5e5e0a67cd04","color":"brown","value":"Computer Science"},{"id":"7993ec69-9767-4b84-adb9-5f1c907a6c77","color":"blue","value":"React.js"},{"id":"264c4a74-71d6-4015-bc91-5742970abd89","color":"yellow","value":"OSS"},{"id":"da38c5e1-f969-4abe-b28f-389b37aa22e5","color":"pink","value":"Startups"},{"id":"ceb7f269-10b7-49a9-bd4e-ba99533dee7b","color":"red","value":"Career"},{"id":"46b1d846-537f-43b5-a18c-298386278e63","color":"default","value":"Video"},{"id":"e485cc4c-e0de-4363-b442-22a0f6f588a9","color":"orange","value":"Saasify"},{"id":"5b6ee85e-ba0a-4c12-8e27-00e5b868939e","color":"gray","value":"SaaS"},{"id":"01c8627e-ae07-4f7b-b248-f877f88c8c4f","color":"purple","value":"Web Dev"},{"id":"136170f8-fe43-466b-b790-087508e8bc27","color":"blue","value":"Software Development"},{"id":"77f16ef0-4622-4f47-8f39-f5913a96421d","color":"pink","value":"Projects"},{"id":"aba9d5d2-c4f2-40ad-9f37-db6211536f92","color":"blue","value":"App Dev"},{"id":"84f76e57-8d2d-4078-bb7d-cf85520ba75c","color":"orange","value":"Lifestyle"},{"id":"38b5979d-877b-4cf4-920b-8b5da5ce9ba1","color":"yellow","value":"Thought Experiments"},{"id":"452e3943-dbcc-4d48-95fc-20d5d6339ddd","color":"orange","value":"Research"},{"id":"534e3ab2-fe89-487b-9784-71d76d5396a1","color":"purple","value":"Passion Economy"},{"id":"aa48c5a8-cfe3-4195-bfaa-25a72353299a","color":"yellow","value":"Tech"},{"id":"ff6898c4-f3f1-4024-b6a9-1cb871133744","color":"blue","value":"Creator Economy"},{"id":"ec59ba49-f2d4-4317-9a0e-3c7fb0bc60b8","color":"green","value":"Crypto"},{"id":"3fc961e4-4408-4d65-b750-7c136977ce61","color":"gray","value":"Deep Learning"},{"id":"a1419788-3341-407e-8df0-cd3d1f4ff636","color":"brown","value":"Gradient Vanishing"},{"id":"50ebe91f-e808-4056-b8ec-3b0bae45d464","color":"yellow","value":"Convolution Layer"},{"id":"ae8b3ff9-00e0-4d25-8d8d-e7389fc7fc8c","color":"orange","value":"Dot Product"},{"id":"8f6c2d82-17e0-4fb1-82eb-a6047ec75d02","color":"orange","value":"Vector"},{"id":"96ed8c32-a637-4b4f-ba99-3b397d04435a","color":"red","value":"Auction Theory"},{"id":"d4090af3-0dfb-480d-9503-0d0328774c16","color":"pink","value":"Reinforcement Learning"},{"id":"6fbb9d5c-c0f4-4791-8397-6e89c63e0c82","color":"yellow","value":"MDP"},{"id":"92885d4f-2302-43ac-87b0-e9e1c7a8cd8b","color":"yellow","value":"Introduction"},{"id":"f5ee4f74-bef9-4701-9d7d-1b9e7068d702","color":"blue","value":"Value Function"},{"id":"a575fa70-0999-44a3-b96c-262d57984b63","color":"green","value":"bellman equation"},{"id":"807f640e-1abc-476a-8300-1fe0ec0a15a3","color":"gray","value":"Grid World"},{"id":"e959edcb-aeb9-4a66-91e9-7677c68a85a8","color":"purple","value":"Dynamic Programming"},{"id":"3c5ba2f8-7ec5-4d9c-89b8-2ae975f4740a","color":"default","value":"Policy Iteration"},{"id":"b79852f3-5c3c-4de7-8d6d-731673caae40","color":"green","value":"Value Iteration"},{"id":"c2bd765b-351d-4a6d-a75d-94e8c476a180","color":"red","value":"Policy Evaluation"},{"id":"4a911c23-ca00-4f29-ad2f-463eb166f8f2","color":"brown","value":"SARSA"},{"id":"563d1a3e-095d-4ad8-bb7e-cc8c8195a9c1","color":"green","value":"Q-Learning"},{"id":"9e1bb3b2-a545-4e9d-b1eb-c47d2db8f4a8","color":"gray","value":"Writing"},{"id":"fcaf8148-917c-44bd-8ca1-fce87ba28f55","color":"blue","value":"Nepal"},{"id":"d93da022-471d-4d13-8dbc-051aa3e4639f","color":"orange","value":"Travel"},{"id":"f20c5e6f-58fe-4208-baa3-20704f665d21","color":"purple","value":"Algorithm"},{"id":"447d9963-8cfc-4c4c-9adb-88f0dc85f249","color":"red","value":"Python"},{"id":"633d7256-ffbf-4e5b-9912-906130d85250","color":"yellow","value":"Big-O"},{"id":"1a1af600-6b9b-4b8e-9483-c038712cda7c","color":"default","value":"String"},{"id":"b24d85c4-b7af-411c-905f-c9ae68eab612","color":"pink","value":"Array"},{"id":"87516a34-7662-4a77-a219-149777b960dc","color":"red","value":"LinkedList"},{"id":"d63330f6-ba17-4b9c-b370-579f4c99358a","color":"blue","value":"Stack"},{"id":"04eff27e-18a0-4883-8915-6ba26059106d","color":"yellow","value":"Queue"},{"id":"328cf039-a0b4-4b5c-90f7-be587808a1dc","color":"orange","value":"Deque"},{"id":"3364095f-3a21-47c6-8460-16ddc7f02c13","color":"brown","value":"HashTable"},{"id":"64841489-db5b-45ba-972a-7af2f53f41c7","color":"yellow","value":"Graph"},{"id":"b6dea43b-7633-4290-91f0-9761023838b0","color":"purple","value":"Tree"},{"id":"9dc575d0-d0d5-4282-8430-aae294857404","color":"gray","value":"Heap"},{"id":"edc331ab-bf6d-4985-9ecc-6884de83e8fc","color":"brown","value":"Trie"},{"id":"f2fdcca6-c69d-4b53-ab23-665ca7a223b9","color":"yellow","value":"Sort"},{"id":"5e02ca54-b743-4501-9776-62e030442114","color":"default","value":"BinarySearch"},{"id":"9a317b05-c2df-41ff-bf3f-32c39b5c451b","color":"red","value":"Greedy"},{"id":"536f3cec-7932-4444-b8c7-154672ad5fe6","color":"pink","value":"DivideAndConquer"},{"id":"1fe05376-d59a-4e4a-8596-fcd70f194621","color":"orange","value":"Basic"},{"id":"6fdde3f1-6554-4e54-ba5b-af1fc95b0ce4","color":"green","value":"Linear"},{"id":"28fc1aec-d1dd-44bb-a3ab-78ebc6f74837","color":"yellow","value":"NonLinear"},{"id":"e74e9521-68b1-4c11-ab42-a034a44dfd24","color":"blue","value":"알고리즘"},{"id":"42f167a6-7e2e-4d8e-854d-426689132f08","color":"brown","value":"Amazon"},{"id":"8aefb99d-7b6b-453b-941f-e114e32c6438","color":"blue","value":"book"},{"id":"d5083966-d1d3-4116-a615-5107104890c2","color":"pink","value":"Business"},{"id":"df607bdb-4379-4273-a45e-e52d436aef0d","color":"red","value":"Diffusion Model"},{"id":"da21253b-a1e7-4fe8-9ed3-6073a347c457","color":"orange","value":"Paper Review"}]},"NVm^":{"name":"Slug","type":"text"},"QSi`":{"name":"Series","type":"checkbox"},"a\u003cql":{"name":"Published","type":"date"},"jhf;":{"name":"Tweet","type":"text"},"nAX{":{"name":"Created","type":"created_time"},"}nqi":{"name":"Author","type":"text"},"~]S\u003c":{"name":"Description","type":"text"},"title":{"name":"Name","type":"title"}},"format":{"copied_from_pointer":{"id":"e5fdcb8e-6e29-4bc9-828d-263749307808","table":"collection","spaceId":"fde5ac74-eea3-4527-8f00-4482710e1af3"},"property_visibility":[{"property":"AfoN","visibility":"hide_if_empty"},{"property":"BN]P","visibility":"hide_if_empty"},{"property":"a\u003cql","visibility":"hide_if_empty"},{"property":"}nqi","visibility":"hide_if_empty"},{"property":"~]S\u003c","visibility":"hide"},{"property":"==~K","visibility":"hide"},{"property":"jhf;","visibility":"hide"},{"property":"=bhc","visibility":"hide"},{"property":"NVm^","visibility":"hide"},{"property":"nAX{","visibility":"hide"},{"property":";KhU","visibility":"hide"},{"property":"QSi`","visibility":"hide"}],"collection_page_properties":[{"visible":false,"property":"AfoN"},{"visible":true,"property":"BN]P"},{"visible":true,"property":"a\u003cql"},{"visible":false,"property":"}nqi"},{"visible":true,"property":"~]S\u003c"},{"visible":true,"property":"==~K"},{"visible":true,"property":"jhf;"},{"visible":false,"property":"=bhc"},{"visible":false,"property":"NVm^"},{"visible":true,"property":"nAX{"},{"visible":false,"property":";KhU"}]},"parent_id":"4f20ede8-ccf7-4ae1-82e5-819e100dd032","parent_table":"block","alive":true,"copied_from":"e5fdcb8e-6e29-4bc9-828d-263749307808","template_pages":["cacb6437-50cd-4a3d-9d43-58962f75e40b"],"migrated":true,"space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6","deleted_schema":{":nQy":{"name":"Curating","type":"text"}}}}},"collection_view":{"d91647c5-6a81-48b1-a1ff-a04529d0ddba":{"role":"reader","value":{"id":"d91647c5-6a81-48b1-a1ff-a04529d0ddba","version":33,"type":"gallery","name":"Gallery view","format":{"gallery_cover":{"type":"page_cover"},"gallery_cover_size":"medium","gallery_properties":[{"visible":false,"property":"AfoN"},{"visible":false,"property":"jhf;"},{"visible":false,"property":"NVm^"},{"visible":false,"property":"==~K"},{"visible":false,"property":";KhU"},{"visible":false,"property":"=bhc"},{"visible":false,"property":"}nqi"},{"visible":false,"property":"nAX{"},{"visible":false,"property":"BN]P"},{"visible":true,"property":"title"},{"visible":true,"property":"~]S\u003c"},{"visible":true,"property":"a\u003cql"}],"gallery_cover_aspect":"cover","hide_linked_collection_name":true,"inline_collection_first_load_limit":{"type":"load_limit","limit":10}},"parent_id":"4f20ede8-ccf7-4ae1-82e5-819e100dd032","parent_table":"block","alive":true,"page_sort":["e3df5b79-eb34-4bec-a82f-628699f43852","7a8e9c32-39af-4bca-a26e-3f8f1c9a1762","7864e0d8-d646-4df5-a4ae-057371b7559d","36a57bd6-d63a-40f0-a0a2-6ba247885fab","c2994f3d-696a-4015-b7d1-2b71883562ad","58fc3d75-92a2-44e9-ab0d-84cb61086232","30ac6075-b07c-41fa-b684-15c16f1134ba","4c804cac-3119-4cc3-a621-a82040d9a0db","6ce6c0ba-0d6f-40b3-b7d4-afe8ca715c73","64158711-b8f3-4c3d-ba2f-3999059c5581","0c679f04-0865-4142-a65b-28668aa9daea","54fa8c94-ead7-4b14-b068-174936adb37f","ab2e2675-7daf-464f-a2c4-563136630232","ef52bf7e-8726-4804-a0d5-e9b5f7919284","460f888e-b262-4f14-a39f-93ae0090bb9d","eff2e07b-ee44-45e0-b46a-622deb611d0b","c94f3f08-d65f-4f7d-a409-72c353f9786d","daac7673-f388-475e-8f43-a345dc350bef","6ff46924-9cc3-4445-badc-28e7daa5fa5a","c114dc9f-b09a-4e30-9c0d-5ace92e6a8e3","51b78f8b-69ff-4a46-9ec6-74f0d5400d3e","06adcb1d-3acf-4266-a8d9-6cb0d3c439ce","22c49483-380f-4b94-be90-b437b7cbab52","45a3d677-b3df-4cc1-ac40-7e0ab214aeef","7a177274-38ce-4f03-b3f6-06e20b0ba4f8","15aa9135-7cca-40c7-b656-ba4457524924","599373d0-99e2-4c28-89de-d273b43aca5c","48756e2f-e604-4421-9b34-be2c90e20589","b94b3e79-f161-45a8-bf8e-5315f85dca99","3e71fe2e-c827-4fc6-b610-7d71147ae4a7","298f8e1a-a9f2-4274-b5f1-7279f7bc4e5b","c7f629b2-6f4e-43fe-8720-4a3a10d4e651","b1fded7b-ceea-46d0-89e2-32010807d35c","bd99ffd5-ce6d-4418-be0e-6db2cd8ae070","5e852682-71e3-45b9-8056-d40e972563fd","1db23f4a-8d7e-4445-b241-a8e4be5bd02a","70df362d-5c3a-4d2c-a3da-ef27e1f207e3","dae737f3-5216-4c50-b708-2bf7c2662020","a13c9d7e-5e2f-44c3-bd8c-17fd50ca4892","f3236b1d-73c7-45d5-920e-f7cac8c19573","c181e327-c3bf-42f8-b8aa-21a367ce64f2","f5613c52-6b68-4073-b593-03d26f51e710"],"query2":{"sort":[{"property":"=bhc","direction":"descending"},{"property":"a\u003cql","direction":"descending"}],"filter":{"filters":[{"filters":[{"filter":{"value":{"type":"exact","value":true},"operator":"checkbox_is"},"property":"==~K"},{"filter":{"value":{"type":"exact","value":true},"operator":"checkbox_is"},"property":"=bhc"},{"filter":{"value":{"type":"exact","value":true},"operator":"checkbox_is"},"property":"QSi`"}],"operator":"and"}],"operator":"and"},"aggregations":[{"aggregator":"count"}]},"space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"27bc73a3-5779-44e0-b617-2f0d26f5aa2f":{"role":"reader","value":{"id":"27bc73a3-5779-44e0-b617-2f0d26f5aa2f","version":1,"type":"table","name":"","format":{"table_properties":[{"width":293,"visible":true,"property":"title"},{"width":398,"visible":true,"property":"~]S\u003c"},{"width":81,"visible":true,"property":"==~K"},{"width":105,"visible":true,"property":"=bhc"},{"width":146,"visible":true,"property":"a\u003cql"},{"width":200,"visible":true,"property":"BN]P"},{"width":122,"visible":true,"property":"}nqi"},{"width":156,"visible":true,"property":"jhf;"},{"width":146,"visible":true,"property":"NVm^"},{"width":200,"visible":true,"property":";KhU"},{"width":200,"visible":false,"property":"nAX{"}]},"parent_id":"4f20ede8-ccf7-4ae1-82e5-819e100dd032","parent_table":"block","alive":true,"page_sort":["f3d44d4c-975d-4b11-8396-c68b35bfdb26","70df362d-5c3a-4d2c-a3da-ef27e1f207e3","f3236b1d-73c7-45d5-920e-f7cac8c19573","4117e62e-18ec-4503-a32a-6c31806a5e2a","6b9736df-63cb-4e7c-ba8e-d6e54f26d6c9","c2f261a2-c616-4198-b1a9-2caf6be162a0","a082287f-d4c9-4cfb-b3b2-72b8bbf043c6","4801bc76-a763-46a8-981d-79dc38c5d85a","e3df5b79-eb34-4bec-a82f-628699f43852","36a57bd6-d63a-40f0-a0a2-6ba247885fab","c2994f3d-696a-4015-b7d1-2b71883562ad","58fc3d75-92a2-44e9-ab0d-84cb61086232","30ac6075-b07c-41fa-b684-15c16f1134ba","4c804cac-3119-4cc3-a621-a82040d9a0db","6ce6c0ba-0d6f-40b3-b7d4-afe8ca715c73","64158711-b8f3-4c3d-ba2f-3999059c5581","0c679f04-0865-4142-a65b-28668aa9daea","54fa8c94-ead7-4b14-b068-174936adb37f","ab2e2675-7daf-464f-a2c4-563136630232","ef52bf7e-8726-4804-a0d5-e9b5f7919284","460f888e-b262-4f14-a39f-93ae0090bb9d","eff2e07b-ee44-45e0-b46a-622deb611d0b","c94f3f08-d65f-4f7d-a409-72c353f9786d","daac7673-f388-475e-8f43-a345dc350bef","6ff46924-9cc3-4445-badc-28e7daa5fa5a","c114dc9f-b09a-4e30-9c0d-5ace92e6a8e3","51b78f8b-69ff-4a46-9ec6-74f0d5400d3e","06adcb1d-3acf-4266-a8d9-6cb0d3c439ce","22c49483-380f-4b94-be90-b437b7cbab52","1db23f4a-8d7e-4445-b241-a8e4be5bd02a","dae737f3-5216-4c50-b708-2bf7c2662020","a13c9d7e-5e2f-44c3-bd8c-17fd50ca4892","7864e0d8-d646-4df5-a4ae-057371b7559d","7a8e9c32-39af-4bca-a26e-3f8f1c9a1762","0f58fdf7-da3e-4793-93f8-b2503443020d","ca43f24a-f6c2-4611-a92a-8051ec80fb0f","36b681a4-e13e-4a78-a2d6-f56b2e3657a1","8cfc0d5a-59ba-4ea0-9609-4720753d5fba","88891af2-1639-4eff-a2c9-f3ba6668b2ea","92db0a94-6e1e-44a6-8df5-882e60ff5b3f","0d958b63-df7a-44a4-b80c-5408c78f59e4","7a177274-38ce-4f03-b3f6-06e20b0ba4f8","ae0b3ee0-531c-4f81-8d4b-cc6dd040fea1","bf9953ee-2589-4969-948c-0d31106a9deb","fdb7d90f-a355-4e7d-8f9a-3d07a58a457e","561e9779-b56c-4310-8913-d54675e02c74","6951e99b-10cc-4833-a27f-f0e08f8941d0","76a4bb7d-be00-45f3-bbcd-cba28c38588b","18c92268-31e5-4509-8644-9c6ad9e44c10","6463af62-efa9-47dc-91ce-99df728f66e0"],"query2":{"sort":[{"property":"=bhc","direction":"descending"},{"property":"a\u003cql","direction":"descending"}],"aggregations":[{"property":"title","aggregator":"count"}]},"space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}}},"notion_user":{},"collection_query":{},"signed_urls":{},"preview_images":{"images.unsplash.com/photo-1563209259-b2fa97148ce1":{"originalWidth":4509,"originalHeight":3433,"dataURIBase64":"data:image/webp;base64,UklGRkQAAABXRUJQVlA4IDgAAAAQAgCdASoQAAwABUB8JZQCdAEN4Z4/0jgAAP7nI6g5q7g7NuXgHKw945x+RhNJbTW2vaRJ2E2wAA=="},"notion.so/image/notion.so%2Fimages%2Fpage-cover%2Fnasa_reduced_gravity_walking_simulator.jpg":{"originalWidth":2000,"originalHeight":1597,"dataURIBase64":"data:image/webp;base64,UklGRjwAAABXRUJQVlA4IDAAAADQAQCdASoQAA0ABUB8JaQAAuUwaXLIHAD+3O9cAAKxBQKBBYBng9tx6GTwHd24mAA="},"notion.so/image/s3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fb29e9b03-c79c-4e52-a45e-7228163ba524%2Fcompass-circular-tool_(3).png":{"originalWidth":2000,"originalHeight":2000,"dataURIBase64":"data:image/webp;base64,UklGRv4AAABXRUJQVlA4WAoAAAAQAAAADwAADwAAQUxQSFUAAAARL6CmkRQ4eF5eaio0EBGB5+2AUWxbbR5t/yWkKiDsUwVQnn8xoViI6P8EAOK9AiCF/BQQSXKHsKrOmo21PddC4xsKsbafNSc1jfQLgBSSGsD0HBoAAFZQOCCCAAAAsAIAnQEqEAAQAAVAfCWkAA+DSIAfdDvk/UudpKAAAPYcuQdG3VZft4V447Ryb0FWml/BLapzhboSZ6SLFp2WFSojeqJNq2VAANIrwpK47u1M/geoV8ECX9+TEWBDeQC4c2g3HcSldGi6FylcOf6PWeqJaegO6jdPFlsvX04FC2gAAA=="},"notion.so/image/longshiine.github.io%2F2021%2F01%2F24%2FReinforcement-Learning-Basic-4%2Fpolicy.png":{"originalWidth":2000,"originalHeight":1339,"dataURIBase64":"data:image/webp;base64,UklGRkQAAABXRUJQVlA4IDgAAACwAQCdASoQAAsABUB8JaQAAjjp6GAAAP7x4KZLg7pTNeU/RSNLrgbqZiMaybu0SKd2Zm0i1Z0AAA=="},"notion.so/image/longshiine.github.io%2F2021%2F01%2F24%2FReinforcement-Learning-Basic-4%2FpolicyIteration.png":{"originalWidth":2000,"originalHeight":1177,"dataURIBase64":"data:image/webp;base64,UklGRjgAAABXRUJQVlA4ICwAAADQAQCdASoQAAkABUB8JZwAAudfJ7B4gAD+8dtkJ5dFQvI7obbWTKNlIfgAAA=="},"notion.so/image/longshiine.github.io%2F2021%2F01%2F24%2FReinforcement-Learning-Basic-4%2Fgreedy.png":{"originalWidth":2000,"originalHeight":996,"dataURIBase64":"data:image/webp;base64,UklGRjgAAABXRUJQVlA4ICwAAACwAQCdASoQAAgABUB8JaQAAlxaOLoAAP7x37RourXQ7PpcU2VkPcioGGwAAA=="},"notion.so/image/longshiine.github.io%2F2021%2F01%2F24%2FReinforcement-Learning-Basic-4%2Foptimalpolicy.png":{"originalWidth":2000,"originalHeight":2012,"dataURIBase64":"data:image/webp;base64,UklGRjgAAABXRUJQVlA4ICwAAADQAQCdASoQABAABUB8JYwAAudfJ8MmAAD+77XsPFqKk6yP/3+vXuFwuqAAAA=="},"notion.so/image/longshiine.github.io%2F2021%2F01%2F24%2FReinforcement-Learning-Basic-4%2Fvalueiteration.png":{"originalWidth":2000,"originalHeight":2092,"dataURIBase64":"data:image/webp;base64,UklGRj4AAABXRUJQVlA4IDIAAADQAQCdASoPABAABUB8JYwAAveFRo5yAAD+7XDqZlYJ7j4ljR5PRr/9ISMQC2/qjKYAAA=="},"notion.so/image/s3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Ffc853118-df50-43ed-96d9-0711493d5e25%2Fjang_inspiration_logo.png":{"originalWidth":2000,"originalHeight":2000,"dataURIBase64":"data:image/webp;base64,UklGRv4AAABXRUJQVlA4WAoAAAAQAAAADwAADwAAQUxQSFUAAAARL6CmkRQ4eF5eaio0EBGB5+2AUWxbbR5t/yWkKiDsUwVQnn8xoViI6P8EAOK9AiCF/BQQSXKHsKrOmo21PddC4xsKsbafNSc1jfQLgBSSGsD0HBoAAFZQOCCCAAAAsAIAnQEqEAAQAAVAfCWkAA+DSIAfdDvk/UudpKAAAPYcuQdG3VZft4V447Ryb0FWml/BLapzhboSZ6SLFp2WFSojeqJNq2VAANIrwpK47u1M/geoV8ECX9+TEWBDeQC4c2g3HcSldGi6FylcOf6PWeqJaegO6jdPFlsvX04FC2gAAA=="}}},"pageId":"3931006b-7fcb-4441-b508-0a5ba536daaf","rawPageId":"policy-value-iteration"},"__N_SSG":true},"page":"/[pageId]","query":{"pageId":"policy-value-iteration"},"buildId":"I-ojLq_lc7g-_5LzXbOVe","isFallback":false,"dynamicIds":[635,7274,3358],"gsp":true,"scriptLoader":[]}</script></body></html>
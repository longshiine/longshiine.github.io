<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta name="robots" content="index,follow"/><meta property="og:type" content="website"/><meta property="og:site_name" content="Jang. Inspiration"/><meta property="twitter:domain" content="jang-inspiration.com"/><meta name="description" content="사람의 학습 방법과 강화학습의 학습 방법은 정확히 무엇이 다를까? 몬테카를로 근사(Monte-Carlo Prediction)과 시간차 예측(Temporal-Difference Prediction)을 톺아보자."/><meta property="og:description" content="사람의 학습 방법과 강화학습의 학습 방법은 정확히 무엇이 다를까? 몬테카를로 근사(Monte-Carlo Prediction)과 시간차 예측(Temporal-Difference Prediction)을 톺아보자."/><meta name="twitter:description" content="사람의 학습 방법과 강화학습의 학습 방법은 정확히 무엇이 다를까? 몬테카를로 근사(Monte-Carlo Prediction)과 시간차 예측(Temporal-Difference Prediction)을 톺아보자."/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:image" content="https://jang-inspiration.com/api/social-image?id=dba7311e-93e6-4f18-8cd1-41a08bd33b19"/><meta property="og:image" content="https://jang-inspiration.com/api/social-image?id=dba7311e-93e6-4f18-8cd1-41a08bd33b19"/><link rel="canonical" href="https://jang-inspiration.com/reinforcement-learning-6"/><meta property="og:url" content="https://jang-inspiration.com/reinforcement-learning-6"/><meta property="twitter:url" content="https://jang-inspiration.com/reinforcement-learning-6"/><link rel="alternate" type="application/rss+xml" href="https://jang-inspiration.com/feed" title="Jang. Inspiration"/><meta property="og:title" content="&lt;파이썬과 케라스로 배우는 강화학습&gt; - (6) 강화학습과 정책평가"/><meta name="twitter:title" content="&lt;파이썬과 케라스로 배우는 강화학습&gt; - (6) 강화학습과 정책평가"/><title>&lt;파이썬과 케라스로 배우는 강화학습&gt; - (6) 강화학습과 정책평가</title><meta name="naver-site-verification" content="3942485b5f7254d146b71f1249d907d89048a4d6"/><link rel="preload" as="image" href="https://images.unsplash.com/photo-1563209259-b2fa97148ce1?ixlib=rb-4.0.3&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb"/><meta name="next-head-count" content="22"/><link rel="shortcut icon" href="/favicon.ico"/><link rel="icon" type="image/png" sizes="32x32" href="favicon.png"/><link rel="manifest" href="/manifest.json"/><link rel="preload" href="/_next/static/css/d2e6a1cb5181bdcf.css" as="style"/><link rel="stylesheet" href="/_next/static/css/d2e6a1cb5181bdcf.css" data-n-g=""/><link rel="preload" href="/_next/static/css/4e32f0fa5eadbe4b.css" as="style"/><link rel="stylesheet" href="/_next/static/css/4e32f0fa5eadbe4b.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script defer="" src="/_next/static/chunks/3607272e.d338bf53926ee7c2.js"></script><script defer="" src="/_next/static/chunks/853.526a4df21aef109c.js"></script><script defer="" src="/_next/static/chunks/175675d1.a2f4b19cd9daa73f.js"></script><script defer="" src="/_next/static/chunks/274.59c48af6aaac8ebd.js"></script><script src="/_next/static/chunks/webpack-bcbf8f4abc46b243.js" defer=""></script><script src="/_next/static/chunks/framework-4556c45dd113b893.js" defer=""></script><script src="/_next/static/chunks/main-f08b69bdcc7bbb61.js" defer=""></script><script src="/_next/static/chunks/pages/_app-27630c741ae01a08.js" defer=""></script><script src="/_next/static/chunks/780-d9c40783d635496e.js" defer=""></script><script src="/_next/static/chunks/634-d6e2e697c346745d.js" defer=""></script><script src="/_next/static/chunks/pages/%5BpageId%5D-35899cbd792aff57.js" defer=""></script><script src="/_next/static/I-ojLq_lc7g-_5LzXbOVe/_buildManifest.js" defer=""></script><script src="/_next/static/I-ojLq_lc7g-_5LzXbOVe/_ssgManifest.js" defer=""></script></head><body><script>
/** Inlined version of noflash.js from use-dark-mode */
;(function () {
  var storageKey = 'darkMode'
  var classNameDark = 'dark-mode'
  var classNameLight = 'light-mode'
  function setClassOnDocumentBody(darkMode) {
    document.body.classList.add(darkMode ? classNameDark : classNameLight)
    document.body.classList.remove(darkMode ? classNameLight : classNameDark)
  }
  var preferDarkQuery = '(prefers-color-scheme: dark)'
  var mql = window.matchMedia(preferDarkQuery)
  var supportsColorSchemeQuery = mql.media === preferDarkQuery
  var localStorageTheme = null
  try {
    localStorageTheme = localStorage.getItem(storageKey)
  } catch (err) {}
  var localStorageExists = localStorageTheme !== null
  if (localStorageExists) {
    localStorageTheme = JSON.parse(localStorageTheme)
  }
  // Determine the source of truth
  if (localStorageExists) {
    // source of truth from localStorage
    setClassOnDocumentBody(localStorageTheme)
  } else if (supportsColorSchemeQuery) {
    // source of truth from system
    setClassOnDocumentBody(mql.matches)
    localStorage.setItem(storageKey, mql.matches)
  } else {
    // source of truth from document.body
    var isDarkMode = document.body.classList.contains(classNameDark)
    localStorage.setItem(storageKey, JSON.stringify(isDarkMode))
  }
})();
</script><div id="__next"><div class="notion notion-app light-mode notion-block-dba7311e93e64f188cd141a08bd33b19"><div class="notion-viewport"></div><div class="notion-frame"><header class="notion-header"><div class="notion-nav-header"><div class="breadcrumbs"><a class="breadcrumb" href="/"><div class="notion-page-icon-inline notion-page-icon-image"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%272000%27%20height=%272000%27/%3e"/></span><img alt="Jang. Inspiration" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" class="icon notion-page-icon" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/webp;base64,UklGRv4AAABXRUJQVlA4WAoAAAAQAAAADwAADwAAQUxQSFUAAAARL6CmkRQ4eF5eaio0EBGB5+2AUWxbbR5t/yWkKiDsUwVQnn8xoViI6P8EAOK9AiCF/BQQSXKHsKrOmo21PddC4xsKsbafNSc1jfQLgBSSGsD0HBoAAFZQOCCCAAAAsAIAnQEqEAAQAAVAfCWkAA+DSIAfdDvk/UudpKAAAPYcuQdG3VZft4V447Ryb0FWml/BLapzhboSZ6SLFp2WFSojeqJNq2VAANIrwpK47u1M/geoV8ECX9+TEWBDeQC4c2g3HcSldGi6FylcOf6PWeqJaegO6jdPFlsvX04FC2gAAA==&quot;)"/><noscript><img alt="Jang. Inspiration" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fb29e9b03-c79c-4e52-a45e-7228163ba524%2Fcompass-circular-tool_(3).png?table=block&amp;id=6246082f-4014-4d06-98ab-59e9840b298a&amp;cache=v2" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="icon notion-page-icon" loading="lazy"/></noscript></span></div><span class="title">Jang. Inspiration</span></a></div><div class="notion-nav-header-rhs breadcrumbs"><a href="/about" class="breadcrumb button">About</a><div class="breadcrumb button styles_hidden__7gYve"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="32" d="M256 48v48m0 320v48m147.08-355.08l-33.94 33.94M142.86 369.14l-33.94 33.94M464 256h-48m-320 0H48m355.08 147.08l-33.94-33.94M142.86 142.86l-33.94-33.94"></path><circle cx="256" cy="256" r="80" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="32"></circle></svg></div><div role="button" class="breadcrumb button notion-search-button"><svg class="notion-icon searchIcon" viewBox="0 0 17 17"><path d="M6.78027 13.6729C8.24805 13.6729 9.60156 13.1982 10.709 12.4072L14.875 16.5732C15.0684 16.7666 15.3232 16.8633 15.5957 16.8633C16.167 16.8633 16.5713 16.4238 16.5713 15.8613C16.5713 15.5977 16.4834 15.3516 16.29 15.1582L12.1504 11.0098C13.0205 9.86719 13.5391 8.45215 13.5391 6.91406C13.5391 3.19629 10.498 0.155273 6.78027 0.155273C3.0625 0.155273 0.0214844 3.19629 0.0214844 6.91406C0.0214844 10.6318 3.0625 13.6729 6.78027 13.6729ZM6.78027 12.2139C3.87988 12.2139 1.48047 9.81445 1.48047 6.91406C1.48047 4.01367 3.87988 1.61426 6.78027 1.61426C9.68066 1.61426 12.0801 4.01367 12.0801 6.91406C12.0801 9.81445 9.68066 12.2139 6.78027 12.2139Z"></path></svg></div></div></div></header><div class="notion-page-scroller"><div class="notion-page-cover-wrapper"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%274509%27%20height=%273433%27/%3e"/></span><img alt="&lt;파이썬과 케라스로 배우는 강화학습&gt; - (6) 강화학습과 정책평가" src="https://images.unsplash.com/photo-1563209259-b2fa97148ce1?ixlib=rb-4.0.3&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb" decoding="async" data-nimg="intrinsic" class="notion-page-cover" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-position:center 50%;background-size:cover;background-position:center 50%;filter:blur(20px);background-image:url(&quot;data:image/webp;base64,UklGRkQAAABXRUJQVlA4IDgAAAAQAgCdASoQAAwABUB8JZQCdAEN4Z4/0jgAAP7nI6g5q7g7NuXgHKw945x+RhNJbTW2vaRJ2E2wAA==&quot;)"/><noscript><img alt="&lt;파이썬과 케라스로 배우는 강화학습&gt; - (6) 강화학습과 정책평가" src="https://images.unsplash.com/photo-1563209259-b2fa97148ce1?ixlib=rb-4.0.3&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-position:center 50%" class="notion-page-cover"/></noscript></span></div><main class="notion-page notion-page-has-cover notion-page-has-icon notion-page-has-text-icon notion-full-page"><div class="notion-page-icon-hero notion-page-icon-span"><span class="notion-page-icon" role="img" aria-label="🎮">🎮</span></div><h1 class="notion-title">&lt;파이썬과 케라스로 배우는 강화학습&gt; - (6) <b>강화학습과 정책평가</b></h1><div class="notion-collection-page-properties"><div class="notion-collection-row"><div class="notion-collection-row-body"><div class="notion-collection-row-property"><div class="notion-collection-column-title"><svg viewBox="0 0 14 14" class="notion-collection-column-title-icon"><path d="M7 13A6 6 0 107 1a6 6 0 000 12zM3.751 5.323A.2.2 0 013.909 5h6.182a.2.2 0 01.158.323L7.158 9.297a.2.2 0 01-.316 0L3.751 5.323z"></path></svg><div class="notion-collection-column-title-body">Category</div></div><div class="notion-collection-row-value"><span class="notion-property notion-property-select"><div class="notion-property-select-item notion-item-orange">강화학습</div></span></div></div><div class="notion-collection-row-property"><div class="notion-collection-column-title"><svg viewBox="0 0 14 14" class="notion-collection-column-title-icon"><path d="M4 3a1 1 0 011-1h7a1 1 0 110 2H5a1 1 0 01-1-1zm0 4a1 1 0 011-1h7a1 1 0 110 2H5a1 1 0 01-1-1zm0 4a1 1 0 011-1h7a1 1 0 110 2H5a1 1 0 01-1-1zM2 4a1 1 0 110-2 1 1 0 010 2zm0 4a1 1 0 110-2 1 1 0 010 2zm0 4a1 1 0 110-2 1 1 0 010 2z"></path></svg><div class="notion-collection-column-title-body">Tags</div></div><div class="notion-collection-row-value"><span class="notion-property notion-property-multi_select"><div class="notion-property-multi_select-item notion-item-pink">Reinforcement Learning</div><div class="notion-property-multi_select-item notion-item-red">Policy Evaluation</div></span></div></div><div class="notion-collection-row-property"><div class="notion-collection-column-title"><svg viewBox="0 0 14 14" class="notion-collection-column-title-icon"><path d="M10.889 5.5H3.11v1.556h7.778V5.5zm1.555-4.444h-.777V0H10.11v1.056H3.89V0H2.333v1.056h-.777c-.864 0-1.548.7-1.548 1.555L0 12.5c0 .856.692 1.5 1.556 1.5h10.888C13.3 14 14 13.356 14 12.5V2.611c0-.855-.7-1.555-1.556-1.555zm0 11.444H1.556V3.944h10.888V12.5zM8.556 8.611H3.11v1.556h5.445V8.61z"></path></svg><div class="notion-collection-column-title-body">Published</div></div><div class="notion-collection-row-value"><span class="notion-property notion-property-date">January 25, 2021</span></div></div><div class="notion-collection-row-property"><div class="notion-collection-column-title"><svg viewBox="0 0 14 14" class="notion-collection-column-title-icon"><path d="M7 4.568a.5.5 0 00-.5-.5h-6a.5.5 0 00-.5.5v1.046a.5.5 0 00.5.5h6a.5.5 0 00.5-.5V4.568zM.5 1a.5.5 0 00-.5.5v1.045a.5.5 0 00.5.5h12a.5.5 0 00.5-.5V1.5a.5.5 0 00-.5-.5H.5zM0 8.682a.5.5 0 00.5.5h11a.5.5 0 00.5-.5V7.636a.5.5 0 00-.5-.5H.5a.5.5 0 00-.5.5v1.046zm0 3.068a.5.5 0 00.5.5h9a.5.5 0 00.5-.5v-1.045a.5.5 0 00-.5-.5h-9a.5.5 0 00-.5.5v1.045z"></path></svg><div class="notion-collection-column-title-body">Author</div></div><div class="notion-collection-row-value"><span class="notion-property notion-property-text"><b>Jay</b></span></div></div></div></div></div><div class="notion-page-content notion-page-content-has-aside"><article class="notion-page-content-inner"><blockquote class="notion-quote notion-block-9fc42b26203d4f339b411691813de72f"><div>본 포스트는 “파이썬과 케라스로 배우는 강화학습” 도서의 여섯번째 리뷰 포스트입니다.</div></blockquote><div class="notion-text notion-block-eac0a383fb19411d84867f01083141f2"><a target="_blank" rel="noopener noreferrer" class="notion-link" href="http://www.yes24.com/Product/Goods/44136413">http://www.yes24.com/Product/Goods/44136413</a></div><div class="notion-blank notion-block-407969773d884b00b68e9dbb9e1ca6b7"> </div><div class="notion-table-of-contents notion-gray notion-block-e64a346cd2f84249bb0267365cebfe16"><a href="#9b604798b9874dfbaeae2bd4ea230d28" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:0">4장 강화학습 기초 3: 강화학습과 정책평가</span></a><a href="#3f564660e60e4e17bdbb779de3976eda" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">사람의 학습 방법과 강화학습의 학습 방법</span></a><a href="#37c1adb3921044a7a2ec7c791ea4786e" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">강화학습의 예측과 제어</span></a><a href="#1481d45b9bf244768f1298c02c987b5c" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">몬테카를로 근사의 예시</span></a><a href="#bf24cb13e9004c2e8cf97aef88ba7e23" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">몬테카를로 예측(Monte-Carlo Prediction)</span></a><a href="#fc9745a4495f47b9879e0cab780cfb22" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">시간차 예측(Temporal-Difference Prediction)</span></a></div><div class="notion-blank notion-block-e22a73d6bd5a4488b8f68d34d1188889"> </div><h3 class="notion-h notion-h2 notion-h-indent-0 notion-block-9b604798b9874dfbaeae2bd4ea230d28" data-id="9b604798b9874dfbaeae2bd4ea230d28"><span><div id="9b604798b9874dfbaeae2bd4ea230d28" class="notion-header-anchor"></div><a class="notion-hash-link" href="#9b604798b9874dfbaeae2bd4ea230d28" title="4장 강화학습 기초 3: 강화학습과 정책평가"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>4장 강화학습 기초 3: 강화학습과 정책평가</b></span></span></h3><hr class="notion-hr notion-block-0824a00ad50b4e3eada99b33c9402970"/><div class="notion-text notion-block-77ba7390de0441deb411c8c5b8c9ddd3">강화학습과 다이내믹 프로그래밍의 차이는 강화학습은 환경의 모델을 몰라도 환경과의 상호작용을 통해 최적 정책을 학습한다는 것이다. 이때 에이전트는 환경과의 상호작용을 통해 <b>주어진 정책에 대한 가치함수를 학습</b> 할 수 있는데 이를 <code class="notion-inline-code">예측</code>이라고 한다. 또한 가치함수를 토대로 <b>정책을 끊임없이 발전시켜 나가서 최적 정책을 학습</b> 하려는 것이 <code class="notion-inline-code">제어</code>이다.</div><div class="notion-text notion-block-31e952fb44404f58a1b87dcc21c179a4"><b>예측</b> 에는 <code class="notion-inline-code">몬테카를로 예측</code>과 <code class="notion-inline-code">시간차 예측</code>이 있으며, <b>제어</b> 에는 시간차 제어인 <code class="notion-inline-code">살사</code>가 있고, 살사의 한계를 극복하기 위한 오프폴리시 제어인 <code class="notion-inline-code">큐러닝</code>이 있다.</div><div class="notion-blank notion-block-ec4a5ae911e24ab999f75675cd8700bb"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-3f564660e60e4e17bdbb779de3976eda" data-id="3f564660e60e4e17bdbb779de3976eda"><span><div id="3f564660e60e4e17bdbb779de3976eda" class="notion-header-anchor"></div><a class="notion-hash-link" href="#3f564660e60e4e17bdbb779de3976eda" title="사람의 학습 방법과 강화학습의 학습 방법"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>사람의 학습 방법과 강화학습의 학습 방법</b></span></span></h4><div class="notion-text notion-block-3257ea21419d41abad1b5400fa735d6a">강화학습의 에이전트는 어떻게 모델 없이 <b>순차적 행동 결정 문제</b> 를 풀 수 있을까? 강화학습의 <b>학습</b> 은 어떻게 작동하는 것일까?🧐</div><div class="notion-text notion-block-057ad687fe084d6b94adcdb83274e285">다이내믹 프로그래밍을 한번 살펴보자. 다이내믹 프로그래밍은 상태나 차원이 증가할수록 계산 복잡도가 기하급수적으로 증가하는데, 이는 환경에 대한 정확한 정보를 가지고 <b>모든 상태에 대해 동시에 계산을 진행하기 때문이다.</b> 마치 바둑을 둘때 모든 경우의 수를 고려하여 두는 형상이다. 사람도 그럴까? 사람도 바둑을 둘 때, 모든 경우의 수를 고려하여 두는 것일까? 아니다! 사람은 학습의 많은 부분을 그냥 바둑을 두면서 진행한다. 후에 복기를 하면서 어디서 잘못을 했고, 어떻게 고쳐야 할까를 고민한다. 여기에 강화학습이 학습을 어떻게 하는가에 대한 답이 있다.</div><div class="notion-blank notion-block-63a63cc15e044bf18b3e62b3e9d240e1"> </div><blockquote class="notion-quote notion-block-870d277357ec4ef28db31693a5b7f4ab"><div>강화학습 은 일단 해보고, 자신을 평가한뒤, 평가한 대로 자신을 업데이트 하며,이 과정을 무수히 반복한다.</div></blockquote><div class="notion-blank notion-block-49f95a694f60453ebaa353035fa760fe"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-37c1adb3921044a7a2ec7c791ea4786e" data-id="37c1adb3921044a7a2ec7c791ea4786e"><span><div id="37c1adb3921044a7a2ec7c791ea4786e" class="notion-header-anchor"></div><a class="notion-hash-link" href="#37c1adb3921044a7a2ec7c791ea4786e" title="강화학습의 예측과 제어"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>강화학습의 예측과 제어</b></span></span></h4><div class="notion-text notion-block-6c8661e98e704b359c6c35df00f15e9a">MDP로 정의되는 문제를 풀 때 중요한 것은 <b>벨만 기대 방정식의 기댓값인 </b><b><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span></b><b>를 어떻게 계산하는가</b> 이다. 정책 이터레이션 대로 계산을 반복한다면 한 치의 오차도 없는 정확한 기댓값을 얻을 수 있으나, 이러한 다이내믹 프로그래밍을 적용할 수 있는 문제는 많지 않다.</div><div class="notion-text notion-block-d52b7bbc68a94f59a540c2d35c5e6c8b">사람은 어떤것을 판단할 때, <b>항상 정확한 정보를 근거로 판단하지 않는다.</b> 이터레이션보다 정확하진 않지만 <b>적당한 추론</b> 을 통해 학습을 해나가는 것이 실제 세상에서는 더 <b>효율적</b> 이다. 강화학습에서는 적당한 추론을 통해 원래 참 가치함수의 값을 <code class="notion-inline-code">예측(prediction)</code>한다. 강화학습에서는 예측과 함께 정책을 발전시키는 것을 <code class="notion-inline-code">제어(Control)</code>이라 부른다.</div><div class="notion-blank notion-block-1a87b1ac3d9442fabe5784eb4ca590ca"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-1481d45b9bf244768f1298c02c987b5c" data-id="1481d45b9bf244768f1298c02c987b5c"><span><div id="1481d45b9bf244768f1298c02c987b5c" class="notion-header-anchor"></div><a class="notion-hash-link" href="#1481d45b9bf244768f1298c02c987b5c" title="몬테카를로 근사의 예시"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>몬테카를로 근사의 예시</b></span></span></h4><div class="notion-text notion-block-e568e7be87c34910a79401cccba7db7c">보통 원의 넓이를 계산하기 위해서는 원의 방정식(<span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>)을 찾아 그 방정식을 이용해 계산한다. 근데, 원의 방정식을 모른다면? 넓이를 어떻게 계산해야 할까.</div><div class="notion-text notion-block-5a6b9e12fb88409e972a277f70906fb8">이때 사용할 수 있는 방법이 <code class="notion-inline-code">몬테카를로 근사(Monte-Carlo Approximation)</code>이다. 몬테카를로라는 말은 <b>무작위로 무엇인가를 해본다</b>는 의미이며, 근사라는 것은 원래의 값은 모르지만 샘플을 통해 원래의 값이 이럴것이다라고 추정하는 것이다. 즉 <b>무작위로 무엇인가를 해서 원래의 넓이를 추정하는 것</b> 이 몬테카를로 근사이다.</div><div class="notion-text notion-block-2fb0a53504084db7a3c1b85612ba93de">몬테카를로 근사로 원의 넓이를 계산하는 예제를 한번 보자. 원(A)이 그려진 네모난 종이(B) 위에 점을 뿌린다고 생각해보자. 전체 뿌린 점들 중에서 A에 들어가 있는 점의 비율을 구하면 이미 알고 있는 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>를 통해서 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>의 값을 추정할 수 있다. <b>더 많은 샘플을 사용할수록 오차는 적어지며 무한히 반복하면 원래의 값과 동일해진다.</b></div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-b0c05f52b1484e5f83b254a3c639214e"><span></span></span><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-154e256630054c37b6e2db5f85097a42"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:432px;max-width:100%;flex-direction:column"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%272000%27%20height=%271939%27/%3e"/></span><img alt="무수히 많은 점으로 원의 넓이를 추정" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/webp;base64,UklGRoYAAABXRUJQVlA4WAoAAAAQAAAADwAADgAAQUxQSCMAAAARHyAUQGDyX2EFkUZExBjMtG2T8UeycJzyD0FE/+OqAf5XBQBWUDggPAAAAPABAJ0BKhAADwAFQHwlsAJ0AREVRVvkAAD+6xrGNEtPAxc/5uKzDNkPKO+BJyEgzI4i+ZZHoAlqLuU0AA==&quot;)"/><noscript><img alt="무수히 많은 점으로 원의 넓이를 추정" src="https://www.notion.so/image/https%3A%2F%2Flongshiine.github.io%2F2021%2F01%2F25%2FReinforcement-Learning-Basic-5%2Fmontecarlo.png?table=block&amp;id=154e2566-3005-4c37-b6e2-db5f85097a42&amp;cache=v2" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></span><figcaption class="notion-asset-caption">무수히 많은 점으로 원의 넓이를 추정</figcaption></div></figure><div class="notion-text notion-block-b4f5834a45e048bbb0cab82cd0843ba7">원의 넓이의 방정식을 알면 한번에 구할 수 있는데, 왜 이런 고생을 할까?😰 몬테카를로 근사의 장점은 <b>방정식을 몰라도 원래 값을 추정할 수 있다</b> 는 것이다. 어떤 도형이든 상관없이 모든 도형의 넓이를 정확하진 않더라도 구할 수 있다. 아래와 같은 그림도 말이다!(아래의 그림에 대한 도형의 방정식은 존재하지 않는다). 이처럼 방정식을 몰라도 반복하기만 하면 답을 구할 수 있다는 장점은 강화학습에 그대로 이용된다.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-be02466e00614c57b1f5817769bcd40b"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:384px;max-width:100%;flex-direction:column"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%272000%27%20height=%272014%27/%3e"/></span><img alt="도형의 모양에 상관없이 몬테카를로 근사로 구할 수 있다" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/webp;base64,UklGRkYAAABXRUJQVlA4IDoAAADwAQCdASoQABAABUB8JaQAAqpKCYT89NoA/vHgxZsPW1HIBDakr2BHox2PQ7+NXT3R76FpCAdwAAAA&quot;)"/><noscript><img alt="도형의 모양에 상관없이 몬테카를로 근사로 구할 수 있다" src="https://www.notion.so/image/https%3A%2F%2Flongshiine.github.io%2F2021%2F01%2F25%2FReinforcement-Learning-Basic-5%2Fdrawing.png?table=block&amp;id=be02466e-0061-4c57-b1f5-817769bcd40b&amp;cache=v2" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></span><figcaption class="notion-asset-caption">도형의 모양에 상관없이 몬테카를로 근사로 구할 수 있다</figcaption></div></figure><div class="notion-blank notion-block-a8a2fec91d47417ea1418e294462dd3e"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-bf24cb13e9004c2e8cf97aef88ba7e23" data-id="bf24cb13e9004c2e8cf97aef88ba7e23"><span><div id="bf24cb13e9004c2e8cf97aef88ba7e23" class="notion-header-anchor"></div><a class="notion-hash-link" href="#bf24cb13e9004c2e8cf97aef88ba7e23" title="몬테카를로 예측(Monte-Carlo Prediction)"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>몬테카를로 예측(Monte-Carlo Prediction)</b></span></span></h4><div class="notion-text notion-block-82a5c096d31b4bc9bd74003132b7be9b">원의 넓이를 추정하는 대신 가치함수를 추정해보자. 원의 넓이를 추정할 때는 점이 하나의 <b>샘플</b> 이며 점을 찍는 것이 <b>샘플링(sampling)</b> 이었다. 가치함수를 추정할 때는 에이전트가 <b>한 번 환경에서 에피소드를 진행하는 것</b> 이 샘플링이다. 샘플의 평균으로 참 가치함수의 값을 추정할 것인데, 이때 몬테카를로 근사를 사용하는 것을 <code class="notion-inline-code">몬테카를로 예측(Montecarlo Prediction)</code>이라고 한다.</div><div class="notion-text notion-block-53cb0e1c8a674eb3907bdaa1162bdf64">정책 이터레이션은 벨만 기대 방정식의 기댓값이란 녀석을 실제로 계산한 것이다. 샘플링을 통해 기댓값을 계산하지 않고 <b>샘플들의 평균으로 가치함수를 예측</b> 하려면 어떻게 해야할까?</div><div class="notion-text notion-block-cf860417620a4778bef0bda6d7e8f2e7">몬테 카를로 예측에서는 환경의 모델을 알아야 하는 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>를 계산하지 않는다. 환경의 모델을 몰라도 여러 에피소드를 통해 구한 <b>반환값의 평균</b> 을 통해 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>를 추정한다.</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-7b100cdcb6344c95ba2f87fff5dd7f90"><span></span></span><div class="notion-text notion-block-c665e1c6a38046a0a7c41f51ec69ae09">한 번의 에피소드로는 추정이 불가능하다. 그것은 마치 원의 넓이를 구할 때 점을 한번 찍는 것과 같으며 몬테카를로 예측을 하기 위해서는 각 상태에 대한 반환값들이 많이 모여야 한다. 각 상태에 모인 반환값들의 평균을 통해 참 가치함수의 값을 추정한다.</div><div class="notion-text notion-block-f9b7092bdcf6470a97582a19e3086a6c">현재 정책에 따라 무수히 많은 에피소드를 진행해 보면 현재 정책을 따랐을 때 지날 수 있는 모든 상태에 대해 <b>충분한 반환값</b> 들을 모을 수 있다. 따라서 상당히 정확한 가치함수의 값을 얻을 수 있다.</div><div class="notion-text notion-block-3082bb57047f46c58f6db679c96d90cc">반환값들의 평균을 취하는 식을 조금 자세히 살펴보자. 편의상 상태에 대한 표현을 생략한다. n개의 반환값을 통해 평균을 취한 가치함수를 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>이라고 하는데 여기서 대문자로 표현하는 이유는 오차가 내포되었다는 의미이다. 업데이트식을 정리해가는 과정은 다음과 같다.</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-cc9e6d5619a347208b8d604013c0b7b2"><span></span></span><div class="notion-text notion-block-880882c838b4409f9b702e2495f585e0">어떤 상태의 가치함수는 샘플링을 통해 에이전트가 그 상태를 방문할 때마다 업데이트하게 된다. 원래 가지고 있던 가치함수 값 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>에 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>를 더함으로써 업데이트 하는 것이다. 이렇게 <b>시간에 따라 평균을 업데이트 해나가는 것</b> 을 <code class="notion-inline-code">이동평균</code>이라고 하며, 아래는 가치함수의 업데이트 식이다.</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-f711c4acce3e410e8c502b812e061d95"><span></span></span><div class="notion-text notion-block-a2657e3cb3c24c0f854bfdc5c578b2ee">위의 수식에서 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>를 오차라고 하며 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>을 <code class="notion-inline-code">스텝사이즈(Step size)</code>로서 업데이트할 때 오차의 얼마를 가지고 업데이트 할지를 결정한다. 일반적으로 <b>스텝사이즈</b> 는 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>라고 표현하며, 위의 식을 일반적인 형태로 나타내면 아래와 같다. 스텝사이즈가 클수록 과거에 얻은 반환값을 지수적으로 감소시킨다. (환경이 지속적으로 변화한다면 1/n로 평균을 구하는 것보다 일정한 숫자로 고정하는 것이 좋다(?))</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-af25700348a2411791db77289eb490b0"><span></span></span><div class="notion-text notion-block-79c6c47bfe1e44b5b6788069e7596f6d">몬테카를로 예측에서 에이전트는 이 업데이트 식을 통해 에피소드 동안 경험했던 모든 상태에 대해 가치함수를 업데이트한다. 어떠한 상태의 가치함수가 업데이트 될수록 가치함수는 <b>현재 정책에 대한 참 가치함수에 수렴해간다.</b> 이후의 모든 강화학습 방법에서 가치함수를 업데이트하는 것은 위의 수식의 변형일 뿐이다. 따라서 이 식을 정확하게 이해하는 것이 중요하다.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-a88e7bcb17714d97a2119a8557f79f2a"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:384px;max-width:100%;flex-direction:column"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%272000%27%20height=%272127%27/%3e"/></span><img alt="그리드월드에서 몬테카를로 예측" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/webp;base64,UklGRkIAAABXRUJQVlA4IDYAAADQAQCdASoPABAABUB8JQAAYv8fVMFxoAD+4eEaZZEm9wbxGTLwUDnxn50cZJNUAsyNsxBAAAA=&quot;)"/><noscript><img alt="그리드월드에서 몬테카를로 예측" src="https://www.notion.so/image/https%3A%2F%2Flongshiine.github.io%2F2021%2F01%2F25%2FReinforcement-Learning-Basic-5%2Fgridworld.png?table=block&amp;id=a88e7bcb-1771-4d97-a211-9a8557f79f2a&amp;cache=v2" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></span><figcaption class="notion-asset-caption">그리드월드에서 몬테카를로 예측</figcaption></div></figure><div class="notion-text notion-block-bef0e0189d4346dea50899c352a1ca32">한 에피소드는 빨간색 선과 같으며, 에이전트는 마침 상태에 갈 때까지 아무것도 하지 않는다. 마침 상태(파란색 동그라미)에 도착하면 에이전트는 지나온 <b>모든 상태의 가치함수를 업데이트</b> 한다. 에피소드 동안 방문했던 모든상태의 가치함수를 업데이트하면 에이전트는 다시 시작부터 새로운 에피소드를 진행하며, 이러한 과정을 계속 반복하는 것이 <b>몬테카를로 예측</b> 이다.</div><div class="notion-blank notion-block-f7f6c8cd4ffd461cad118e74f755713a"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-fc9745a4495f47b9879e0cab780cfb22" data-id="fc9745a4495f47b9879e0cab780cfb22"><span><div id="fc9745a4495f47b9879e0cab780cfb22" class="notion-header-anchor"></div><a class="notion-hash-link" href="#fc9745a4495f47b9879e0cab780cfb22" title="시간차 예측(Temporal-Difference Prediction)"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>시간차 예측(Temporal-Difference Prediction)</b></span></span></h4><div class="notion-text notion-block-fb203ff37f8b436c82a1e4dc2f6db4df">몬테카를로 예측의 단점은 <b>실시간이 아니라는 점</b> 이다. 가치함수를 업데이트하기 위해 에피소드가 끝날 때까지 기다려야 한다. 또한 <b>에피소드의 끝이 없거나, 에피소드의 길이가 긴 경우</b> 에는 몬테카를로 예측이 적합하지 않다.</div><div class="notion-text notion-block-c712269030764ef699cb050eeebe2831"><code class="notion-inline-code">시간차 예측(Temporal-Difference Prediction)</code>은 몬테카를로 예측과는 다르게 타임스텝마다 가치함수를 업데이트 한다. 사람이 다음 순간을 지속적으로 예측하고 바로 학습하는 것처럼, 에이전트를 실시간으로 예측과 ground-truth 간의 차이로 학습시키기 위한 기법이다.</div><div class="notion-text notion-block-acbf267ce7544278a91ee70fc4eb6d32">몬테카를로 예측에서 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>는 에피소드가 끝나야 그 값을 알 수 있었다. 시간차 예측에서는 비슷하지만<span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>를 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>으로 나타낸 가치함수의 정의인 아래의 식을 이용한다. 다이내믹 프로그래밍처럼 기댓값을 계산하지는 않고 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>값을 샘플링해서 그 값으로 현재의 가치함수를 업데이트 한다.</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-9bdb064909004e6c87264e640d887f15"><span></span></span><div class="notion-text notion-block-afeed508d0724e5591b0b408d6d68a1e">가치함수의 업데이트는 실시간으로 이뤄지며, 몬테카를로 예측과는 달리 <b>한번에 하나의 가치함수만 업데이트</b> 한다. 에이전트는 현재 가지고 있는 가치함수 리스트에서 다음 상태에 해당하는 가치함수 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>을 가져올 수 있을 것이다. 그러면 바로 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>를 계산할 수 있고, 계산된 값은 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>의 가치함수 업데이트의 목표가 된다.</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-8a482389a9a24218aa75da497882dcd1"><span></span></span><div class="notion-text notion-block-c54ed555a70e4c878739a28ea0b7da67"><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>는 <code class="notion-inline-code">시간차 에러(Temporal-Difference Error)</code>라고 하며 시간차 예측에서 업데이트의 목표는 <b>반환값</b> 과는 달리 <b>실제의 값은 아니다.</b> <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>은 현재 에이전트가 가지고 있는 값이고, 에이전트는 이 값을 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>의 가치함수일 것이라고 <b>예측</b> 할 뿐이다. 다른 상태의 가치함수 예측값을 통해 지금 상태의 가치함수를 예측하는 이러한 방식을 <code class="notion-inline-code">부트스트랩(Bootstrap)</code>이라고 한다. 즉, <b>업데이트 목표도 정확하지 않은 상황에서 가치함수를 업데이트 하는 것이다.</b></div><div class="notion-text notion-block-9bdd2130f6cd4091816d52f00e35fabb">시간차 예측은 에피소드가 끝날 때까지 기다릴 필요없이 가치함수를 바로 업데이트 할 수 있다. 하지만 이렇게 업데이트를 해도 몬테카를로 예측과 같이 <b>원래 가치함수 값에 수렴할까?</b></div><div class="notion-text notion-block-0eb11c2715ee4df98531364a97806edc"><b>충분히 많은 샘플링을 통해 업데이트</b> 하면 참 가치함수에 수렴하며 많은 경우 몬테카를로 예측보다 <b>더 효율적으로 빠른 시간안에 참 가치함수에 근접한다</b> 고 한다. (이 부분에 대한 디테일한 설명은 나와있지 않다..😭) 하지만 시간차 예측은 몬테 카를로 예측보다 초기 가치함수 값에 따라 <b>예측 정확도가 많이 달라진다는 단점</b> 이 존재한다.</div><div class="notion-text notion-block-a85ed14fba5541f7a4f4c823e1b06299">다음 포스트에서는 강화학습 알고리즘인 <code class="notion-inline-code">살사(SARSA)</code>와 <code class="notion-inline-code">큐러닝(Q-learning)</code>을 살펴볼 것이다!</div></article><aside class="notion-aside"></aside></div></main></div></div></div><div style="width:100%;background-color:#ffffff;color:#373534;padding:20px"><div id="disqus_recommendations"></div><div id="disqus_thread"></div><footer class="styles_footer__RBpyk"><div class="styles_copyright__nhL_k">Copyright 2023 <!-- -->Jang Yeong</div><div class="styles_settings__GyEhi"></div><div class="styles_social__ptL3p"><a class="styles_github__0JN7a" href="https://github.com/longshiine" title="GitHub @longshiine" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a class="styles_linkedin__bgwDi" href="https://www.linkedin.com/in/jangyeong-kim-b7924422a" title="LinkedIn Jang Yeong" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a><a class="styles_instagram__BY5Hj" href="https://instagram.com/jang.inspiration" title="Instagram Jang Yeong" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><g><path fill="none" d="M0 0h24v24H0z"></path><path fill-rule="nonzero" d="M12 2c2.717 0 3.056.01 4.122.06 1.065.05 1.79.217 2.428.465.66.254 1.216.598 1.772 1.153a4.908 4.908 0 0 1 1.153 1.772c.247.637.415 1.363.465 2.428.047 1.066.06 1.405.06 4.122 0 2.717-.01 3.056-.06 4.122-.05 1.065-.218 1.79-.465 2.428a4.883 4.883 0 0 1-1.153 1.772 4.915 4.915 0 0 1-1.772 1.153c-.637.247-1.363.415-2.428.465-1.066.047-1.405.06-4.122.06-2.717 0-3.056-.01-4.122-.06-1.065-.05-1.79-.218-2.428-.465a4.89 4.89 0 0 1-1.772-1.153 4.904 4.904 0 0 1-1.153-1.772c-.248-.637-.415-1.363-.465-2.428C2.013 15.056 2 14.717 2 12c0-2.717.01-3.056.06-4.122.05-1.066.217-1.79.465-2.428a4.88 4.88 0 0 1 1.153-1.772A4.897 4.897 0 0 1 5.45 2.525c.638-.248 1.362-.415 2.428-.465C8.944 2.013 9.283 2 12 2zm0 5a5 5 0 1 0 0 10 5 5 0 0 0 0-10zm6.5-.25a1.25 1.25 0 0 0-2.5 0 1.25 1.25 0 0 0 2.5 0zM12 9a3 3 0 1 1 0 6 3 3 0 0 1 0-6z"></path></g></svg></a></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"site":{"domain":"jang-inspiration.com","name":"Jang. Inspiration","rootNotionPageId":"6246082f40144d0698ab59e9840b298a","rootNotionSpaceId":null,"description":"장영의 영감노트"},"recordMap":{"block":{"dba7311e-93e6-4f18-8cd1-41a08bd33b19":{"role":"reader","value":{"id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","version":746,"type":"page","properties":{"==~K":[["Yes"]],"AfoN":[["강화학습"]],"BN]P":[["Reinforcement Learning,Policy Evaluation"]],"NVm^":[["reinforcement-learning-6"]],"a\u003cql":[["‣",[["d",{"type":"date","start_date":"2021-01-25"}]]]],"}nqi":[["Jay"]],"~]S\u003c":[["사람의 학습 방법과 강화학습의 학습 방법은 정확히 무엇이 다를까? 몬테카를로 근사(Monte-Carlo Prediction)과 시간차 예측(Temporal-Difference Prediction)을 톺아보자."]],"title":[["\u003c파이썬과 케라스로 배우는 강화학습\u003e - (6) "],["강화학습과 정책평가",[["b"]]]]},"content":["9fc42b26-203d-4f33-9b41-1691813de72f","eac0a383-fb19-411d-8486-7f01083141f2","40796977-3d88-4b00-b68e-9dbb9e1ca6b7","e64a346c-d2f8-4249-bb02-67365cebfe16","e22a73d6-bd5a-4488-b8f6-8d34d1188889","9b604798-b987-4dfb-aeae-2bd4ea230d28","0824a00a-d50b-4e3e-ada9-9b33c9402970","77ba7390-de04-41de-b411-c8c5b8c9ddd3","31e952fb-4440-4f58-a1b8-7dcc21c179a4","ec4a5ae9-11e2-4ab9-99f7-5675cd8700bb","3f564660-e60e-4e17-bdbb-779de3976eda","3257ea21-419d-41ab-ad1b-5400fa735d6a","057ad687-fe08-4d6b-94ad-cdb83274e285","63a63cc1-5e04-4bf1-8b3e-62b3e9d240e1","870d2773-57ec-4ef2-8db3-1693a5b7f4ab","49f95a69-4f60-453e-baa3-53035fa760fe","37c1adb3-9210-44a7-a2ec-7c791ea4786e","6c8661e9-8e70-4b35-9c6c-35df00f15e9a","d52b7bbc-68a9-4f59-a540-c2d35c5e6c8b","1a87b1ac-3d94-42fa-be57-84eb4ca590ca","1481d45b-9bf2-4476-8f12-98c02c987b5c","e568e7be-87c3-4910-a794-01cccba7db7c","5a6b9e12-fb88-409e-972a-277f70906fb8","2fb0a535-0408-4db7-a3c1-b85612ba93de","b0c05f52-b148-4e5f-83b2-54a3c639214e","154e2566-3005-4c37-b6e2-db5f85097a42","b4f5834a-45e0-48bb-b0ca-b82cd0843ba7","be02466e-0061-4c57-b1f5-817769bcd40b","a8a2fec9-1d47-417e-a141-8e294462dd3e","bf24cb13-e900-4c2e-8cf9-7aef88ba7e23","82a5c096-d31b-4bc9-bd74-003132b7be9b","53cb0e1c-8a67-4eb3-907b-daa1162bdf64","cf860417-620a-4778-bef0-bda6d7e8f2e7","7b100cdc-b634-4c95-ba2f-87fff5dd7f90","c665e1c6-a380-46a0-a7c4-1f51ec69ae09","f9b7092b-dcf6-470a-9758-2a19e3086a6c","3082bb57-047f-46c5-8f6d-b679c96d90cc","cc9e6d56-19a3-4720-8b8d-604013c0b7b2","880882c8-38b4-409f-9b70-2e2495f585e0","f711c4ac-ce3e-410e-8c50-2b812e061d95","a2657e3c-b3c2-4c0f-854b-fdc5c578b2ee","af257003-48a2-4117-91db-77289eb490b0","79c6c47b-fe1e-44b5-b678-8069e7596f6d","a88e7bcb-1771-4d97-a211-9a8557f79f2a","bef0e018-9d43-46de-a508-99c352a1ca32","f7f6c8cd-4ffd-461c-ad11-8e74f755713a","fc9745a4-495f-47b9-879e-0cab780cfb22","fb203ff3-7f8b-436c-82a1-e4dc2f6db4df","c7122690-3076-4ef6-99cb-050eeebe2831","acbf267c-e754-4278-a91e-e70fc4eb6d32","9bdb0649-0900-4e6c-8726-4e640d887f15","afeed508-d072-4e55-91b0-b408d6d68a1e","8a482389-a9a2-4218-aa75-da497882dcd1","c54ed555-a70e-4c87-8739-a28ea0b7da67","9bdd2130-f6cd-4091-816d-52f00e35fabb","0eb11c27-15ee-4df9-8531-364a97806edc","a85ed14f-ba55-41f7-a4f4-c823e1b06299"],"format":{"page_icon":"🎮","page_cover":"https://images.unsplash.com/photo-1563209259-b2fa97148ce1?ixlib=rb-4.0.3\u0026q=80\u0026fm=jpg\u0026crop=entropy\u0026cs=tinysrgb","page_cover_position":0.5},"created_time":1676049671357,"last_edited_time":1677651631551,"parent_id":"ba8460cf-4781-486e-8976-01358ef4659d","parent_table":"collection","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"4f20ede8-ccf7-4ae1-82e5-819e100dd032":{"role":"reader","value":{"id":"4f20ede8-ccf7-4ae1-82e5-819e100dd032","version":129,"type":"collection_view","view_ids":["d91647c5-6a81-48b1-a1ff-a04529d0ddba","27bc73a3-5779-44e0-b617-2f0d26f5aa2f"],"collection_id":"ba8460cf-4781-486e-8976-01358ef4659d","format":{"collection_pointer":{"id":"ba8460cf-4781-486e-8976-01358ef4659d","table":"collection","spaceId":"4af10338-3e65-4b50-af9f-798d59d5c8f6"},"copied_from_pointer":{"id":"3e3073e9-7aee-481c-b831-765e112ec7b5","table":"block","spaceId":"fde5ac74-eea3-4527-8f00-4482710e1af3"}},"created_time":1675998770865,"last_edited_time":1677136438825,"parent_id":"6246082f-4014-4d06-98ab-59e9840b298a","parent_table":"block","alive":true,"copied_from":"3e3073e9-7aee-481c-b831-765e112ec7b5","created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"6246082f-4014-4d06-98ab-59e9840b298a":{"role":"reader","value":{"id":"6246082f-4014-4d06-98ab-59e9840b298a","version":609,"type":"page","properties":{"title":[["Jang. Inspiration"]]},"content":["651dcdf3-689d-42d2-8497-64f8509d3504","0bd6df0e-6679-499e-b382-c9dc2c597776","f4c89bb7-a90e-41d7-a678-588b3deff765","d5557a1e-de5e-4085-896d-362a19928b69","4f20ede8-ccf7-4ae1-82e5-819e100dd032","ce518f27-4e46-4e98-ac75-13c467c1370c","dfee9c57-3be6-41db-8113-a54eeef675a7","1109cf3f-d8f9-4532-a13e-1af177ad4fdd","5855fe5e-17e2-4f14-8b66-702838bcc734"],"format":{"page_icon":"https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b29e9b03-c79c-4e52-a45e-7228163ba524/compass-circular-tool_(3).png","page_cover":"/images/page-cover/nasa_reduced_gravity_walking_simulator.jpg","copied_from_pointer":{"id":"78754261-97cf-4616-9880-9def95960ebf","table":"block","spaceId":"fde5ac74-eea3-4527-8f00-4482710e1af3"},"page_cover_position":0.5},"permissions":[{"role":"editor","type":"user_permission","user_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb"},{"role":"reader","type":"public_permission","added_timestamp":1675998834095}],"created_time":1675998770872,"last_edited_time":1677392850666,"parent_id":"3a2d56c9-1da9-4a62-b698-f0ad3576c8c1","parent_table":"block","alive":true,"copied_from":"78754261-97cf-4616-9880-9def95960ebf","file_ids":["f70d3dc6-ce97-4be2-9cde-b86606147b41","a2bd3317-78e4-48bc-8d27-9b733175a416","7fae9664-8795-4723-844e-0adecdea62dc","4235c094-2110-4aa6-b058-6b5fe220dbb7","c8194a03-81d0-482d-a7de-f491a6e85f54","7eb95609-c81b-48c1-969e-5ef2f220bc5a","160057d8-120e-4f9f-8c1f-6bcf31a50f15","0cb9278b-708b-4da1-929a-6696aa8cdfa3","3eb9471e-9b71-4bd3-a13d-c33158a442be","b29e9b03-c79c-4e52-a45e-7228163ba524"],"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"9fc42b26-203d-4f33-9b41-1691813de72f":{"role":"reader","value":{"id":"9fc42b26-203d-4f33-9b41-1691813de72f","version":8,"type":"quote","properties":{"title":[["본 포스트는 “파이썬과 케라스로 배우는 강화학습” 도서의 여섯번째 리뷰 포스트입니다."]]},"created_time":1676049753415,"last_edited_time":1676055523735,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"eac0a383-fb19-411d-8486-7f01083141f2":{"role":"reader","value":{"id":"eac0a383-fb19-411d-8486-7f01083141f2","version":7,"type":"text","properties":{"title":[["http://www.yes24.com/Product/Goods/44136413",[["a","http://www.yes24.com/Product/Goods/44136413"]]]]},"created_time":1676055524723,"last_edited_time":1676055524935,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"40796977-3d88-4b00-b68e-9dbb9e1ca6b7":{"role":"reader","value":{"id":"40796977-3d88-4b00-b68e-9dbb9e1ca6b7","version":7,"type":"text","created_time":1676050312350,"last_edited_time":1676050312515,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"e64a346c-d2f8-4249-bb02-67365cebfe16":{"role":"reader","value":{"id":"e64a346c-d2f8-4249-bb02-67365cebfe16","version":4,"type":"table_of_contents","format":{"block_color":"gray"},"created_time":1676050318561,"last_edited_time":1676050318563,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"e22a73d6-bd5a-4488-b8f6-8d34d1188889":{"role":"reader","value":{"id":"e22a73d6-bd5a-4488-b8f6-8d34d1188889","version":4,"type":"text","created_time":1676050312350,"last_edited_time":1676050312888,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"9b604798-b987-4dfb-aeae-2bd4ea230d28":{"role":"reader","value":{"id":"9b604798-b987-4dfb-aeae-2bd4ea230d28","version":4,"type":"sub_header","properties":{"title":[["4장 강화학습 기초 3: 강화학습과 정책평가",[["b"]]]]},"created_time":1676049753415,"last_edited_time":1676049760920,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"0824a00a-d50b-4e3e-ada9-9b33c9402970":{"role":"reader","value":{"id":"0824a00a-d50b-4e3e-ada9-9b33c9402970","version":18,"type":"divider","created_time":1676049759672,"last_edited_time":1676049760360,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"77ba7390-de04-41de-b411-c8c5b8c9ddd3":{"role":"reader","value":{"id":"77ba7390-de04-41de-b411-c8c5b8c9ddd3","version":2,"type":"text","properties":{"title":[["강화학습과 다이내믹 프로그래밍의 차이는 강화학습은 환경의 모델을 몰라도 환경과의 상호작용을 통해 최적 정책을 학습한다는 것이다. 이때 에이전트는 환경과의 상호작용을 통해 "],["주어진 정책에 대한 가치함수를 학습",[["b"]]],[" 할 수 있는데 이를 "],["예측",[["c"]]],["이라고 한다. 또한 가치함수를 토대로 "],["정책을 끊임없이 발전시켜 나가서 최적 정책을 학습",[["b"]]],[" 하려는 것이 "],["제어",[["c"]]],["이다."]]},"created_time":1676049753415,"last_edited_time":1676049753427,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"31e952fb-4440-4f58-a1b8-7dcc21c179a4":{"role":"reader","value":{"id":"31e952fb-4440-4f58-a1b8-7dcc21c179a4","version":2,"type":"text","properties":{"title":[["예측",[["b"]]],[" 에는 "],["몬테카를로 예측",[["c"]]],["과 "],["시간차 예측",[["c"]]],["이 있으며, "],["제어",[["b"]]],[" 에는 시간차 제어인 "],["살사",[["c"]]],["가 있고, 살사의 한계를 극복하기 위한 오프폴리시 제어인 "],["큐러닝",[["c"]]],["이 있다."]]},"created_time":1676049753416,"last_edited_time":1676049753427,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"ec4a5ae9-11e2-4ab9-99f7-5675cd8700bb":{"role":"reader","value":{"id":"ec4a5ae9-11e2-4ab9-99f7-5675cd8700bb","version":5,"type":"text","created_time":1676049778090,"last_edited_time":1676049778092,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"3f564660-e60e-4e17-bdbb-779de3976eda":{"role":"reader","value":{"id":"3f564660-e60e-4e17-bdbb-779de3976eda","version":2,"type":"sub_sub_header","properties":{"title":[["사람의 학습 방법과 강화학습의 학습 방법",[["b"]]]]},"created_time":1676049753416,"last_edited_time":1676049753427,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"3257ea21-419d-41ab-ad1b-5400fa735d6a":{"role":"reader","value":{"id":"3257ea21-419d-41ab-ad1b-5400fa735d6a","version":2,"type":"text","properties":{"title":[["강화학습의 에이전트는 어떻게 모델 없이 "],["순차적 행동 결정 문제",[["b"]]],[" 를 풀 수 있을까? 강화학습의 "],["학습",[["b"]]],[" 은 어떻게 작동하는 것일까?🧐"]]},"created_time":1676049753416,"last_edited_time":1676049753427,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"057ad687-fe08-4d6b-94ad-cdb83274e285":{"role":"reader","value":{"id":"057ad687-fe08-4d6b-94ad-cdb83274e285","version":2,"type":"text","properties":{"title":[["다이내믹 프로그래밍을 한번 살펴보자. 다이내믹 프로그래밍은 상태나 차원이 증가할수록 계산 복잡도가 기하급수적으로 증가하는데, 이는 환경에 대한 정확한 정보를 가지고 "],["모든 상태에 대해 동시에 계산을 진행하기 때문이다.",[["b"]]],[" 마치 바둑을 둘때 모든 경우의 수를 고려하여 두는 형상이다. 사람도 그럴까? 사람도 바둑을 둘 때, 모든 경우의 수를 고려하여 두는 것일까? 아니다! 사람은 학습의 많은 부분을 그냥 바둑을 두면서 진행한다. 후에 복기를 하면서 어디서 잘못을 했고, 어떻게 고쳐야 할까를 고민한다. 여기에 강화학습이 학습을 어떻게 하는가에 대한 답이 있다."]]},"created_time":1676049753416,"last_edited_time":1676049753427,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"63a63cc1-5e04-4bf1-8b3e-62b3e9d240e1":{"role":"reader","value":{"id":"63a63cc1-5e04-4bf1-8b3e-62b3e9d240e1","version":5,"type":"text","created_time":1676049795761,"last_edited_time":1676049795763,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"870d2773-57ec-4ef2-8db3-1693a5b7f4ab":{"role":"reader","value":{"id":"870d2773-57ec-4ef2-8db3-1693a5b7f4ab","version":2,"type":"quote","properties":{"title":[["강화학습 은 일단 해보고, 자신을 평가한뒤, 평가한 대로 자신을 업데이트 하며,이 과정을 무수히 반복한다."]]},"created_time":1676049753416,"last_edited_time":1676049753427,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"49f95a69-4f60-453e-baa3-53035fa760fe":{"role":"reader","value":{"id":"49f95a69-4f60-453e-baa3-53035fa760fe","version":5,"type":"text","created_time":1676049797101,"last_edited_time":1676049797105,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"37c1adb3-9210-44a7-a2ec-7c791ea4786e":{"role":"reader","value":{"id":"37c1adb3-9210-44a7-a2ec-7c791ea4786e","version":2,"type":"sub_sub_header","properties":{"title":[["강화학습의 예측과 제어",[["b"]]]]},"created_time":1676049753416,"last_edited_time":1676049753427,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"6c8661e9-8e70-4b35-9c6c-35df00f15e9a":{"role":"reader","value":{"id":"6c8661e9-8e70-4b35-9c6c-35df00f15e9a","version":15,"type":"text","properties":{"title":[["MDP로 정의되는 문제를 풀 때 중요한 것은 "],["벨만 기대 방정식의 기댓값인 ",[["b"]]],["⁍",[["e","E_\\pi"],["b"]]],["를 어떻게 계산하는가",[["b"]]],[" 이다. 정책 이터레이션 대로 계산을 반복한다면 한 치의 오차도 없는 정확한 기댓값을 얻을 수 있으나, 이러한 다이내믹 프로그래밍을 적용할 수 있는 문제는 많지 않다."]]},"created_time":1676049753416,"last_edited_time":1676049812457,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"d52b7bbc-68a9-4f59-a540-c2d35c5e6c8b":{"role":"reader","value":{"id":"d52b7bbc-68a9-4f59-a540-c2d35c5e6c8b","version":2,"type":"text","properties":{"title":[["사람은 어떤것을 판단할 때, "],["항상 정확한 정보를 근거로 판단하지 않는다.",[["b"]]],[" 이터레이션보다 정확하진 않지만 "],["적당한 추론",[["b"]]],[" 을 통해 학습을 해나가는 것이 실제 세상에서는 더 "],["효율적",[["b"]]],[" 이다. 강화학습에서는 적당한 추론을 통해 원래 참 가치함수의 값을 "],["예측(prediction)",[["c"]]],["한다. 강화학습에서는 예측과 함께 정책을 발전시키는 것을 "],["제어(Control)",[["c"]]],["이라 부른다."]]},"created_time":1676049753416,"last_edited_time":1676049753427,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"1a87b1ac-3d94-42fa-be57-84eb4ca590ca":{"role":"reader","value":{"id":"1a87b1ac-3d94-42fa-be57-84eb4ca590ca","version":5,"type":"text","created_time":1676049799392,"last_edited_time":1676049799393,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"1481d45b-9bf2-4476-8f12-98c02c987b5c":{"role":"reader","value":{"id":"1481d45b-9bf2-4476-8f12-98c02c987b5c","version":2,"type":"sub_sub_header","properties":{"title":[["몬테카를로 근사의 예시",[["b"]]]]},"created_time":1676049753416,"last_edited_time":1676049753427,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"e568e7be-87c3-4910-a794-01cccba7db7c":{"role":"reader","value":{"id":"e568e7be-87c3-4910-a794-01cccba7db7c","version":7,"type":"text","properties":{"title":[["보통 원의 넓이를 계산하기 위해서는 원의 방정식("],["⁍",[["e","S = \\pi r^2"]]],[")을 찾아 그 방정식을 이용해 계산한다. 근데, 원의 방정식을 모른다면? 넓이를 어떻게 계산해야 할까."]]},"created_time":1676049753416,"last_edited_time":1676049825907,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"5a6b9e12-fb88-409e-972a-277f70906fb8":{"role":"reader","value":{"id":"5a6b9e12-fb88-409e-972a-277f70906fb8","version":2,"type":"text","properties":{"title":[["이때 사용할 수 있는 방법이 "],["몬테카를로 근사(Monte-Carlo Approximation)",[["c"]]],["이다. 몬테카를로라는 말은 "],["무작위로 무엇인가를 해본다",[["b"]]],["는 의미이며, 근사라는 것은 원래의 값은 모르지만 샘플을 통해 원래의 값이 이럴것이다라고 추정하는 것이다. 즉 "],["무작위로 무엇인가를 해서 원래의 넓이를 추정하는 것",[["b"]]],[" 이 몬테카를로 근사이다."]]},"created_time":1676049753416,"last_edited_time":1676049753427,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"2fb0a535-0408-4db7-a3c1-b85612ba93de":{"role":"reader","value":{"id":"2fb0a535-0408-4db7-a3c1-b85612ba93de","version":12,"type":"text","properties":{"title":[["몬테카를로 근사로 원의 넓이를 계산하는 예제를 한번 보자. 원(A)이 그려진 네모난 종이(B) 위에 점을 뿌린다고 생각해보자. 전체 뿌린 점들 중에서 A에 들어가 있는 점의 비율을 구하면 이미 알고 있는 "],["⁍",[["e","S(B)"]]],["를 통해서 "],["⁍",[["e","S(A)"]]],["의 값을 추정할 수 있다. "],["더 많은 샘플을 사용할수록 오차는 적어지며 무한히 반복하면 원래의 값과 동일해진다.",[["b"]]]]},"created_time":1676049753416,"last_edited_time":1676049877778,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"b0c05f52-b148-4e5f-83b2-54a3c639214e":{"role":"reader","value":{"id":"b0c05f52-b148-4e5f-83b2-54a3c639214e","version":13,"type":"equation","properties":{"title":[["if \\\\\\; n \\to \\infty, \\\\\\, then \\\\\\\\\n{1 \\over n} \\sum_{i=1}^n I(RedDot_i \\in A) = {S(A) \\over S(B)}"]],"language":[["Markdown"]]},"created_time":1676049902394,"last_edited_time":1676049908950,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"154e2566-3005-4c37-b6e2-db5f85097a42":{"role":"reader","value":{"id":"154e2566-3005-4c37-b6e2-db5f85097a42","version":8,"type":"image","properties":{"source":[["https://longshiine.github.io/2021/01/25/Reinforcement-Learning-Basic-5/montecarlo.png"]],"caption":[["무수히 많은 점으로 원의 넓이를 추정"]]},"format":{"block_width":432,"block_full_width":false,"block_page_width":false},"created_time":1676049753417,"last_edited_time":1676049837848,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"b4f5834a-45e0-48bb-b0ca-b82cd0843ba7":{"role":"reader","value":{"id":"b4f5834a-45e0-48bb-b0ca-b82cd0843ba7","version":2,"type":"text","properties":{"title":[["원의 넓이의 방정식을 알면 한번에 구할 수 있는데, 왜 이런 고생을 할까?😰 몬테카를로 근사의 장점은 "],["방정식을 몰라도 원래 값을 추정할 수 있다",[["b"]]],[" 는 것이다. 어떤 도형이든 상관없이 모든 도형의 넓이를 정확하진 않더라도 구할 수 있다. 아래와 같은 그림도 말이다!(아래의 그림에 대한 도형의 방정식은 존재하지 않는다). 이처럼 방정식을 몰라도 반복하기만 하면 답을 구할 수 있다는 장점은 강화학습에 그대로 이용된다."]]},"created_time":1676049753419,"last_edited_time":1676049753427,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"be02466e-0061-4c57-b1f5-817769bcd40b":{"role":"reader","value":{"id":"be02466e-0061-4c57-b1f5-817769bcd40b","version":6,"type":"image","properties":{"source":[["https://longshiine.github.io/2021/01/25/Reinforcement-Learning-Basic-5/drawing.png"]],"caption":[["도형의 모양에 상관없이 몬테카를로 근사로 구할 수 있다"]]},"format":{"block_width":384,"block_full_width":false,"block_page_width":false},"created_time":1676049753419,"last_edited_time":1676049846529,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"a8a2fec9-1d47-417e-a141-8e294462dd3e":{"role":"reader","value":{"id":"a8a2fec9-1d47-417e-a141-8e294462dd3e","version":4,"type":"text","created_time":1676049753419,"last_edited_time":1676049843957,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"bf24cb13-e900-4c2e-8cf9-7aef88ba7e23":{"role":"reader","value":{"id":"bf24cb13-e900-4c2e-8cf9-7aef88ba7e23","version":2,"type":"sub_sub_header","properties":{"title":[["몬테카를로 예측(Monte-Carlo Prediction)",[["b"]]]]},"created_time":1676049753419,"last_edited_time":1676049753427,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"82a5c096-d31b-4bc9-bd74-003132b7be9b":{"role":"reader","value":{"id":"82a5c096-d31b-4bc9-bd74-003132b7be9b","version":2,"type":"text","properties":{"title":[["원의 넓이를 추정하는 대신 가치함수를 추정해보자. 원의 넓이를 추정할 때는 점이 하나의 "],["샘플",[["b"]]],[" 이며 점을 찍는 것이 "],["샘플링(sampling)",[["b"]]],[" 이었다. 가치함수를 추정할 때는 에이전트가 "],["한 번 환경에서 에피소드를 진행하는 것",[["b"]]],[" 이 샘플링이다. 샘플의 평균으로 참 가치함수의 값을 추정할 것인데, 이때 몬테카를로 근사를 사용하는 것을 "],["몬테카를로 예측(Montecarlo Prediction)",[["c"]]],["이라고 한다."]]},"created_time":1676049753419,"last_edited_time":1676049753427,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"53cb0e1c-8a67-4eb3-907b-daa1162bdf64":{"role":"reader","value":{"id":"53cb0e1c-8a67-4eb3-907b-daa1162bdf64","version":2,"type":"text","properties":{"title":[["정책 이터레이션은 벨만 기대 방정식의 기댓값이란 녀석을 실제로 계산한 것이다. 샘플링을 통해 기댓값을 계산하지 않고 "],["샘플들의 평균으로 가치함수를 예측",[["b"]]],[" 하려면 어떻게 해야할까?"]]},"created_time":1676049753419,"last_edited_time":1676049753427,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"cf860417-620a-4778-bef0-bda6d7e8f2e7":{"role":"reader","value":{"id":"cf860417-620a-4778-bef0-bda6d7e8f2e7","version":12,"type":"text","properties":{"title":[["몬테 카를로 예측에서는 환경의 모델을 알아야 하는 "],["⁍",[["e","E_\\pi"]]],["를 계산하지 않는다. 환경의 모델을 몰라도 여러 에피소드를 통해 구한 "],["반환값의 평균",[["b"]]],[" 을 통해 "],["⁍",[["e","v_\\pi (s)"]]],["를 추정한다."]]},"created_time":1676049753419,"last_edited_time":1676049950326,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"7b100cdc-b634-4c95-ba2f-87fff5dd7f90":{"role":"reader","value":{"id":"7b100cdc-b634-4c95-ba2f-87fff5dd7f90","version":9,"type":"equation","properties":{"title":[["v_\\pi (s) \\sim {1 \\over N(s)}\\sum_{i=1}^{N(s)}G_i(s)"]]},"created_time":1676049960634,"last_edited_time":1676049964647,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"c665e1c6-a380-46a0-a7c4-1f51ec69ae09":{"role":"reader","value":{"id":"c665e1c6-a380-46a0-a7c4-1f51ec69ae09","version":2,"type":"text","properties":{"title":[["한 번의 에피소드로는 추정이 불가능하다. 그것은 마치 원의 넓이를 구할 때 점을 한번 찍는 것과 같으며 몬테카를로 예측을 하기 위해서는 각 상태에 대한 반환값들이 많이 모여야 한다. 각 상태에 모인 반환값들의 평균을 통해 참 가치함수의 값을 추정한다."]]},"created_time":1676049753419,"last_edited_time":1676049753427,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"f9b7092b-dcf6-470a-9758-2a19e3086a6c":{"role":"reader","value":{"id":"f9b7092b-dcf6-470a-9758-2a19e3086a6c","version":2,"type":"text","properties":{"title":[["현재 정책에 따라 무수히 많은 에피소드를 진행해 보면 현재 정책을 따랐을 때 지날 수 있는 모든 상태에 대해 "],["충분한 반환값",[["b"]]],[" 들을 모을 수 있다. 따라서 상당히 정확한 가치함수의 값을 얻을 수 있다."]]},"created_time":1676049753419,"last_edited_time":1676049753427,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"3082bb57-047f-46c5-8f6d-b679c96d90cc":{"role":"reader","value":{"id":"3082bb57-047f-46c5-8f6d-b679c96d90cc","version":7,"type":"text","properties":{"title":[["반환값들의 평균을 취하는 식을 조금 자세히 살펴보자. 편의상 상태에 대한 표현을 생략한다. n개의 반환값을 통해 평균을 취한 가치함수를 "],["⁍",[["e","V_{n+1}"]]],["이라고 하는데 여기서 대문자로 표현하는 이유는 오차가 내포되었다는 의미이다. 업데이트식을 정리해가는 과정은 다음과 같다."]]},"created_time":1676049753420,"last_edited_time":1676049980849,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"cc9e6d56-19a3-4720-8b8d-604013c0b7b2":{"role":"reader","value":{"id":"cc9e6d56-19a3-4720-8b8d-604013c0b7b2","version":13,"type":"equation","properties":{"title":[["V_{n+1} = {1 \\over n}\\sum_{i=1}^n G_i = {1 \\over n} \\biggl( G_n + \\sum_{i=1}^{n-1}G_i \\biggl) \\\\\\\\\n= {1 \\over n} \\biggl( G_n + (n-1)({1 \\over n-1})\\sum_{i=1}^{n-1}G_i \\biggl) \\\\\\\\\n= {1 \\over n} ( G_n + (n-1)V_n) \\\\\\\\\n= {1 \\over n} ( G_n + nV_n - V_n) \\\\\\\\\n= V_n + {1 \\over n}(G_n - V_n) \\\\\\\\"]],"language":[["Markdown"]]},"created_time":1676049988573,"last_edited_time":1676049994014,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"880882c8-38b4-409f-9b70-2e2495f585e0":{"role":"reader","value":{"id":"880882c8-38b4-409f-9b70-2e2495f585e0","version":12,"type":"text","properties":{"title":[["어떤 상태의 가치함수는 샘플링을 통해 에이전트가 그 상태를 방문할 때마다 업데이트하게 된다. 원래 가지고 있던 가치함수 값 "],["⁍",[["e","V(s)"]]],["에 "],["⁍",[["e","{1 \\over n}(G(s) - V(s))"]]],["를 더함으로써 업데이트 하는 것이다. 이렇게 "],["시간에 따라 평균을 업데이트 해나가는 것",[["b"]]],[" 을 "],["이동평균",[["c"]]],["이라고 하며, 아래는 가치함수의 업데이트 식이다."]]},"created_time":1676049753420,"last_edited_time":1676050020294,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"f711c4ac-ce3e-410e-8c50-2b812e061d95":{"role":"reader","value":{"id":"f711c4ac-ce3e-410e-8c50-2b812e061d95","version":9,"type":"equation","properties":{"title":[["V(s) \\leftarrow V(s) + {1 \\over n}(G(s) - V(s))"]]},"created_time":1676050030498,"last_edited_time":1676050033815,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"a2657e3c-b3c2-4c0f-854b-fdc5c578b2ee":{"role":"reader","value":{"id":"a2657e3c-b3c2-4c0f-854b-fdc5c578b2ee","version":17,"type":"text","properties":{"title":[["위의 수식에서 "],["⁍",[["e","G(s) - V(s)"]]],["를 오차라고 하며 "],["⁍",[["e","{1 \\over n}"]]],["을 "],["스텝사이즈(Step size)",[["c"]]],["로서 업데이트할 때 오차의 얼마를 가지고 업데이트 할지를 결정한다. 일반적으로 "],["스텝사이즈",[["b"]]],[" 는 "],["⁍",[["e","\\alpha"]]],["라고 표현하며, 위의 식을 일반적인 형태로 나타내면 아래와 같다. 스텝사이즈가 클수록 과거에 얻은 반환값을 지수적으로 감소시킨다. (환경이 지속적으로 변화한다면 1/n로 평균을 구하는 것보다 일정한 숫자로 고정하는 것이 좋다(?))"]]},"created_time":1676049753420,"last_edited_time":1676050083213,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"af257003-48a2-4117-91db-77289eb490b0":{"role":"reader","value":{"id":"af257003-48a2-4117-91db-77289eb490b0","version":9,"type":"equation","properties":{"title":[["V(s) \\leftarrow V(s) + \\alpha (G(s) - V(s))"]]},"created_time":1676050085796,"last_edited_time":1676050099016,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"79c6c47b-fe1e-44b5-b678-8069e7596f6d":{"role":"reader","value":{"id":"79c6c47b-fe1e-44b5-b678-8069e7596f6d","version":2,"type":"text","properties":{"title":[["몬테카를로 예측에서 에이전트는 이 업데이트 식을 통해 에피소드 동안 경험했던 모든 상태에 대해 가치함수를 업데이트한다. 어떠한 상태의 가치함수가 업데이트 될수록 가치함수는 "],["현재 정책에 대한 참 가치함수에 수렴해간다.",[["b"]]],[" 이후의 모든 강화학습 방법에서 가치함수를 업데이트하는 것은 위의 수식의 변형일 뿐이다. 따라서 이 식을 정확하게 이해하는 것이 중요하다."]]},"created_time":1676049753420,"last_edited_time":1676049753427,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"a88e7bcb-1771-4d97-a211-9a8557f79f2a":{"role":"reader","value":{"id":"a88e7bcb-1771-4d97-a211-9a8557f79f2a","version":8,"type":"image","properties":{"source":[["https://longshiine.github.io/2021/01/25/Reinforcement-Learning-Basic-5/gridworld.png"]],"caption":[["그리드월드에서 몬테카를로 예측"]]},"format":{"block_width":384,"block_full_width":false,"block_page_width":false},"created_time":1676049753420,"last_edited_time":1676050112987,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"bef0e018-9d43-46de-a508-99c352a1ca32":{"role":"reader","value":{"id":"bef0e018-9d43-46de-a508-99c352a1ca32","version":2,"type":"text","properties":{"title":[["한 에피소드는 빨간색 선과 같으며, 에이전트는 마침 상태에 갈 때까지 아무것도 하지 않는다. 마침 상태(파란색 동그라미)에 도착하면 에이전트는 지나온 "],["모든 상태의 가치함수를 업데이트",[["b"]]],[" 한다. 에피소드 동안 방문했던 모든상태의 가치함수를 업데이트하면 에이전트는 다시 시작부터 새로운 에피소드를 진행하며, 이러한 과정을 계속 반복하는 것이 "],["몬테카를로 예측",[["b"]]],[" 이다."]]},"created_time":1676049753420,"last_edited_time":1676049753427,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"f7f6c8cd-4ffd-461c-ad11-8e74f755713a":{"role":"reader","value":{"id":"f7f6c8cd-4ffd-461c-ad11-8e74f755713a","version":5,"type":"text","created_time":1676050114590,"last_edited_time":1676050114592,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"fc9745a4-495f-47b9-879e-0cab780cfb22":{"role":"reader","value":{"id":"fc9745a4-495f-47b9-879e-0cab780cfb22","version":2,"type":"sub_sub_header","properties":{"title":[["시간차 예측(Temporal-Difference Prediction)",[["b"]]]]},"created_time":1676049753420,"last_edited_time":1676049753427,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"fb203ff3-7f8b-436c-82a1-e4dc2f6db4df":{"role":"reader","value":{"id":"fb203ff3-7f8b-436c-82a1-e4dc2f6db4df","version":2,"type":"text","properties":{"title":[["몬테카를로 예측의 단점은 "],["실시간이 아니라는 점",[["b"]]],[" 이다. 가치함수를 업데이트하기 위해 에피소드가 끝날 때까지 기다려야 한다. 또한 "],["에피소드의 끝이 없거나, 에피소드의 길이가 긴 경우",[["b"]]],[" 에는 몬테카를로 예측이 적합하지 않다."]]},"created_time":1676049753420,"last_edited_time":1676049753427,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"c7122690-3076-4ef6-99cb-050eeebe2831":{"role":"reader","value":{"id":"c7122690-3076-4ef6-99cb-050eeebe2831","version":2,"type":"text","properties":{"title":[["시간차 예측(Temporal-Difference Prediction)",[["c"]]],["은 몬테카를로 예측과는 다르게 타임스텝마다 가치함수를 업데이트 한다. 사람이 다음 순간을 지속적으로 예측하고 바로 학습하는 것처럼, 에이전트를 실시간으로 예측과 ground-truth 간의 차이로 학습시키기 위한 기법이다."]]},"created_time":1676049753420,"last_edited_time":1676049753427,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"acbf267c-e754-4278-a91e-e70fc4eb6d32":{"role":"reader","value":{"id":"acbf267c-e754-4278-a91e-e70fc4eb6d32","version":22,"type":"text","properties":{"title":[["몬테카를로 예측에서 "],["⁍",[["e","G_t"]]],["는 에피소드가 끝나야 그 값을 알 수 있었다. 시간차 예측에서는 비슷하지만"],["⁍",[["e"," G_t"]]],["를 "],["⁍",[["e","R_{t+1} + \\gamma v_\\pi(s')"]]],["으로 나타낸 가치함수의 정의인 아래의 식을 이용한다. 다이내믹 프로그래밍처럼 기댓값을 계산하지는 않고 "],["⁍",[["e","R_{t+1} + \\gamma v_\\pi(s')"]]],["값을 샘플링해서 그 값으로 현재의 가치함수를 업데이트 한다."]]},"created_time":1676049753420,"last_edited_time":1676050179563,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"9bdb0649-0900-4e6c-8726-4e640d887f15":{"role":"reader","value":{"id":"9bdb0649-0900-4e6c-8726-4e640d887f15","version":9,"type":"equation","properties":{"title":[["v_\\pi (s) = E_\\pi [R_{t+1}+ \\gamma v_\\pi (S_{t+1})|S_t = s]"]]},"created_time":1676050196406,"last_edited_time":1676050200683,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"afeed508-d072-4e55-91b0-b408d6d68a1e":{"role":"reader","value":{"id":"afeed508-d072-4e55-91b0-b408d6d68a1e","version":17,"type":"text","properties":{"title":[["가치함수의 업데이트는 실시간으로 이뤄지며, 몬테카를로 예측과는 달리 "],["한번에 하나의 가치함수만 업데이트",[["b"]]],[" 한다. 에이전트는 현재 가지고 있는 가치함수 리스트에서 다음 상태에 해당하는 가치함수 "],["⁍",[["e","V(S_{t+1})"]]],["을 가져올 수 있을 것이다. 그러면 바로 "],["⁍",[["e","R_{t+1} + \\gamma v_\\pi(s')"]]],["를 계산할 수 있고, 계산된 값은 "],["⁍",[["e","S_t"]]],["의 가치함수 업데이트의 목표가 된다."]]},"created_time":1676049753421,"last_edited_time":1676050244895,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"8a482389-a9a2-4218-aa75-da497882dcd1":{"role":"reader","value":{"id":"8a482389-a9a2-4218-aa75-da497882dcd1","version":9,"type":"equation","properties":{"title":[["V(S_t) \\leftarrow V(S_t) + \\alpha (R_{t+1} + \\gamma V(S_{t+1}) - V(S_t))"]]},"created_time":1676050252829,"last_edited_time":1676050256451,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"c54ed555-a70e-4c87-8739-a28ea0b7da67":{"role":"reader","value":{"id":"c54ed555-a70e-4c87-8739-a28ea0b7da67","version":17,"type":"text","properties":{"title":[["⁍",[["e","R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)"]]],["는 "],["시간차 에러(Temporal-Difference Error)",[["c"]]],["라고 하며 시간차 예측에서 업데이트의 목표는 "],["반환값",[["b"]]],[" 과는 달리 "],["실제의 값은 아니다.",[["b"]]],[" "],["⁍",[["e","V(S_{t+1})"]]],["은 현재 에이전트가 가지고 있는 값이고, 에이전트는 이 값을 "],["⁍",[["e","S_{t+1}"]]],["의 가치함수일 것이라고 "],["예측",[["b"]]],[" 할 뿐이다. 다른 상태의 가치함수 예측값을 통해 지금 상태의 가치함수를 예측하는 이러한 방식을 "],["부트스트랩(Bootstrap)",[["c"]]],["이라고 한다. 즉, "],["업데이트 목표도 정확하지 않은 상황에서 가치함수를 업데이트 하는 것이다.",[["b"]]]]},"created_time":1676049753421,"last_edited_time":1676050296465,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"9bdd2130-f6cd-4091-816d-52f00e35fabb":{"role":"reader","value":{"id":"9bdd2130-f6cd-4091-816d-52f00e35fabb","version":2,"type":"text","properties":{"title":[["시간차 예측은 에피소드가 끝날 때까지 기다릴 필요없이 가치함수를 바로 업데이트 할 수 있다. 하지만 이렇게 업데이트를 해도 몬테카를로 예측과 같이 "],["원래 가치함수 값에 수렴할까?",[["b"]]]]},"created_time":1676049753421,"last_edited_time":1676049753427,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"0eb11c27-15ee-4df9-8531-364a97806edc":{"role":"reader","value":{"id":"0eb11c27-15ee-4df9-8531-364a97806edc","version":2,"type":"text","properties":{"title":[["충분히 많은 샘플링을 통해 업데이트",[["b"]]],[" 하면 참 가치함수에 수렴하며 많은 경우 몬테카를로 예측보다 "],["더 효율적으로 빠른 시간안에 참 가치함수에 근접한다",[["b"]]],[" 고 한다. (이 부분에 대한 디테일한 설명은 나와있지 않다..😭) 하지만 시간차 예측은 몬테 카를로 예측보다 초기 가치함수 값에 따라 "],["예측 정확도가 많이 달라진다는 단점",[["b"]]],[" 이 존재한다."]]},"created_time":1676049753421,"last_edited_time":1676049753427,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"a85ed14f-ba55-41f7-a4f4-c823e1b06299":{"role":"reader","value":{"id":"a85ed14f-ba55-41f7-a4f4-c823e1b06299","version":2,"type":"text","properties":{"title":[["다음 포스트에서는 강화학습 알고리즘인 "],["살사(SARSA)",[["c"]]],["와 "],["큐러닝(Q-learning)",[["c"]]],["을 살펴볼 것이다!"]]},"created_time":1676049753421,"last_edited_time":1676049753427,"parent_id":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"3d335e41-f00a-45df-8432-34a266c7566a":{"role":"reader","value":{"id":"3d335e41-f00a-45df-8432-34a266c7566a","version":480,"type":"page","properties":{"title":[["About"]]},"content":["e1067486-5fb5-4dc3-9415-54abc63fc3a5","ab253a7b-1dec-4620-8c4e-e99f94e1f6c9","460c887f-cc3d-4351-8671-7cfbc6d3bc1d","c62b8a9d-e8cb-4c5b-be5a-48412e956c95","abe79af7-1342-438a-9a59-2d0b1ce59007","88d551b8-6b87-4540-89b9-3619a8fe4059","ae31ce74-317a-45ae-9dd3-16c14e764361","78f83c2a-05c2-4033-bcd8-945d700b2952","fd280029-9a4c-47fe-8e3c-ce8d3f146671","6a0d0c32-f1bc-4277-a128-33da8613ede4","2bcccf09-dccf-4896-8819-255cf75aabaf","d84b4bd5-3b6b-45dd-a807-5fea62c5213b","8b7fb4d3-0195-4f61-a0eb-330792f84053","e9394173-e197-4fad-a1db-ac74fdba3738","cb29948c-7258-4b05-9bfa-25bf19ad0946","a4e4243c-25aa-480d-a9ee-0af2230c7d76","172e2e25-e319-4766-ad38-02fc168994cd","6c428956-d84e-4855-a998-7a79f1fc8ea5","90f1cecf-6258-4b54-b0ee-26ee4d950872","3eec82e9-0211-4271-8c30-1dbc86ab3be1","8ce5f430-8c4c-4e8f-b6a4-23396731b27d","aeb9d85e-fffa-4c34-b253-cb0499f047a2","3d37f4c7-8064-4792-a863-234d8b4d05a6","77daa708-adbc-4092-b7f7-9e3c8a988049","26e3aca8-3e09-4a37-8c92-3c95cf579bbb"],"format":{"page_icon":"https://s3-us-west-2.amazonaws.com/secure.notion-static.com/fc853118-df50-43ed-96d9-0711493d5e25/jang_inspiration_logo.png","page_cover":"/images/page-cover/nasa_reduced_gravity_walking_simulator.jpg","copied_from_pointer":{"id":"f1199d37-579b-41cb-abfc-0b5174f4256a","table":"block","spaceId":"fde5ac74-eea3-4527-8f00-4482710e1af3"},"page_cover_position":0.5},"created_time":1675998770866,"last_edited_time":1677390731360,"parent_id":"d83b5165-627c-41b3-82f0-f1cbf904e176","parent_table":"block","alive":true,"copied_from":"f1199d37-579b-41cb-abfc-0b5174f4256a","file_ids":["c8028bee-f4c7-4736-8c36-fffcab5d977e","9407e769-d877-4de9-bbaa-9e5626d971ed","07d5a5b4-de2a-4322-bfcc-c4ade3a63b86","fc853118-df50-43ed-96d9-0711493d5e25"],"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"d83b5165-627c-41b3-82f0-f1cbf904e176":{"role":"reader","value":{"id":"d83b5165-627c-41b3-82f0-f1cbf904e176","version":17,"type":"column","content":["e1fe2c9c-6eb8-412c-9874-a8ac2fce9ed8","3d335e41-f00a-45df-8432-34a266c7566a"],"format":{"column_ratio":0.25},"created_time":1676020529237,"last_edited_time":1676091451534,"parent_id":"1109cf3f-d8f9-4532-a13e-1af177ad4fdd","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"1109cf3f-d8f9-4532-a13e-1af177ad4fdd":{"role":"reader","value":{"id":"1109cf3f-d8f9-4532-a13e-1af177ad4fdd","version":13,"type":"column_list","content":["91ac285d-9a69-4ec4-96f6-9b046a15647c","028efbb4-417e-4742-8474-dd7a2ddeb8ee","d63c036c-b99a-4aa9-a068-87abc40d37a6","d83b5165-627c-41b3-82f0-f1cbf904e176"],"format":{"block_width":720,"block_full_width":false,"block_page_width":true},"created_time":1676020365844,"last_edited_time":1676091451534,"parent_id":"6246082f-4014-4d06-98ab-59e9840b298a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"e1067486-5fb5-4dc3-9415-54abc63fc3a5":{"role":"reader","value":{"id":"e1067486-5fb5-4dc3-9415-54abc63fc3a5","version":86,"type":"text","properties":{"title":[["Instagram",[["h","blue_background"],["a","https://www.instagram.com/jang.inspiration/"]]],[" • "],["GitHub",[["h","teal_background"],["a","https://github.com/longshiine"]]],[" • "],["LinkedIn",[["h","pink_background"],["a","https://www.linkedin.com/in/jangyeong-kim-b7924422a/"]]]]},"format":{"copied_from_pointer":{"id":"0ab4ae84-463a-4b31-9430-f4ed513c5fc2","table":"block","spaceId":"fde5ac74-eea3-4527-8f00-4482710e1af3"}},"created_time":1675998770872,"last_edited_time":1676960330564,"parent_id":"3d335e41-f00a-45df-8432-34a266c7566a","parent_table":"block","alive":true,"copied_from":"0ab4ae84-463a-4b31-9430-f4ed513c5fc2","created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}}},"collection":{"ba8460cf-4781-486e-8976-01358ef4659d":{"role":"reader","value":{"id":"ba8460cf-4781-486e-8976-01358ef4659d","version":117,"schema":{";KhU":{"name":"Last Updated","type":"last_edited_time"},"==~K":{"name":"Public","type":"checkbox"},"=bhc":{"name":"Curated","type":"checkbox"},"AfoN":{"name":"Category","type":"select","options":[{"id":"20579d4b-5ad0-469e-8946-2560e458bb81","color":"orange","value":"강화학습"},{"id":"b78b3694-0089-4efb-802c-c288e6190037","color":"green","value":"알고리즘"},{"id":"34c6aef1-b3b3-490e-9369-b5a385f7da4e","color":"blue","value":"딥러닝"},{"id":"67465999-8b72-4fe4-92ce-db5a812b880a","color":"brown","value":"공학수학"},{"id":"8b385c1c-8e95-4a02-b009-6215a718ebef","color":"pink","value":"글쓰기"},{"id":"435e1850-8e40-4b73-8496-0a3f694b6aeb","color":"purple","value":"스타트업"},{"id":"2b34c718-9490-4d2c-8c1d-949ea1a5010c","color":"gray","value":"독서"}]},"BN]P":{"name":"Tags","type":"multi_select","options":[{"id":"210cfb45-7eae-44e5-83dc-dba99aa3a853","color":"green","value":"Node.js"},{"id":"02b16a55-92ee-455d-9449-5e5e0a67cd04","color":"brown","value":"Computer Science"},{"id":"7993ec69-9767-4b84-adb9-5f1c907a6c77","color":"blue","value":"React.js"},{"id":"264c4a74-71d6-4015-bc91-5742970abd89","color":"yellow","value":"OSS"},{"id":"da38c5e1-f969-4abe-b28f-389b37aa22e5","color":"pink","value":"Startups"},{"id":"ceb7f269-10b7-49a9-bd4e-ba99533dee7b","color":"red","value":"Career"},{"id":"46b1d846-537f-43b5-a18c-298386278e63","color":"default","value":"Video"},{"id":"e485cc4c-e0de-4363-b442-22a0f6f588a9","color":"orange","value":"Saasify"},{"id":"5b6ee85e-ba0a-4c12-8e27-00e5b868939e","color":"gray","value":"SaaS"},{"id":"01c8627e-ae07-4f7b-b248-f877f88c8c4f","color":"purple","value":"Web Dev"},{"id":"136170f8-fe43-466b-b790-087508e8bc27","color":"blue","value":"Software Development"},{"id":"77f16ef0-4622-4f47-8f39-f5913a96421d","color":"pink","value":"Projects"},{"id":"aba9d5d2-c4f2-40ad-9f37-db6211536f92","color":"blue","value":"App Dev"},{"id":"84f76e57-8d2d-4078-bb7d-cf85520ba75c","color":"orange","value":"Lifestyle"},{"id":"38b5979d-877b-4cf4-920b-8b5da5ce9ba1","color":"yellow","value":"Thought Experiments"},{"id":"452e3943-dbcc-4d48-95fc-20d5d6339ddd","color":"orange","value":"Research"},{"id":"534e3ab2-fe89-487b-9784-71d76d5396a1","color":"purple","value":"Passion Economy"},{"id":"aa48c5a8-cfe3-4195-bfaa-25a72353299a","color":"yellow","value":"Tech"},{"id":"ff6898c4-f3f1-4024-b6a9-1cb871133744","color":"blue","value":"Creator Economy"},{"id":"ec59ba49-f2d4-4317-9a0e-3c7fb0bc60b8","color":"green","value":"Crypto"},{"id":"3fc961e4-4408-4d65-b750-7c136977ce61","color":"gray","value":"Deep Learning"},{"id":"a1419788-3341-407e-8df0-cd3d1f4ff636","color":"brown","value":"Gradient Vanishing"},{"id":"50ebe91f-e808-4056-b8ec-3b0bae45d464","color":"yellow","value":"Convolution Layer"},{"id":"ae8b3ff9-00e0-4d25-8d8d-e7389fc7fc8c","color":"orange","value":"Dot Product"},{"id":"8f6c2d82-17e0-4fb1-82eb-a6047ec75d02","color":"orange","value":"Vector"},{"id":"96ed8c32-a637-4b4f-ba99-3b397d04435a","color":"red","value":"Auction Theory"},{"id":"d4090af3-0dfb-480d-9503-0d0328774c16","color":"pink","value":"Reinforcement Learning"},{"id":"6fbb9d5c-c0f4-4791-8397-6e89c63e0c82","color":"yellow","value":"MDP"},{"id":"92885d4f-2302-43ac-87b0-e9e1c7a8cd8b","color":"yellow","value":"Introduction"},{"id":"f5ee4f74-bef9-4701-9d7d-1b9e7068d702","color":"blue","value":"Value Function"},{"id":"a575fa70-0999-44a3-b96c-262d57984b63","color":"green","value":"bellman equation"},{"id":"807f640e-1abc-476a-8300-1fe0ec0a15a3","color":"gray","value":"Grid World"},{"id":"e959edcb-aeb9-4a66-91e9-7677c68a85a8","color":"purple","value":"Dynamic Programming"},{"id":"3c5ba2f8-7ec5-4d9c-89b8-2ae975f4740a","color":"default","value":"Policy Iteration"},{"id":"b79852f3-5c3c-4de7-8d6d-731673caae40","color":"green","value":"Value Iteration"},{"id":"c2bd765b-351d-4a6d-a75d-94e8c476a180","color":"red","value":"Policy Evaluation"},{"id":"4a911c23-ca00-4f29-ad2f-463eb166f8f2","color":"brown","value":"SARSA"},{"id":"563d1a3e-095d-4ad8-bb7e-cc8c8195a9c1","color":"green","value":"Q-Learning"},{"id":"9e1bb3b2-a545-4e9d-b1eb-c47d2db8f4a8","color":"gray","value":"Writing"},{"id":"fcaf8148-917c-44bd-8ca1-fce87ba28f55","color":"blue","value":"Nepal"},{"id":"d93da022-471d-4d13-8dbc-051aa3e4639f","color":"orange","value":"Travel"},{"id":"f20c5e6f-58fe-4208-baa3-20704f665d21","color":"purple","value":"Algorithm"},{"id":"447d9963-8cfc-4c4c-9adb-88f0dc85f249","color":"red","value":"Python"},{"id":"633d7256-ffbf-4e5b-9912-906130d85250","color":"yellow","value":"Big-O"},{"id":"1a1af600-6b9b-4b8e-9483-c038712cda7c","color":"default","value":"String"},{"id":"b24d85c4-b7af-411c-905f-c9ae68eab612","color":"pink","value":"Array"},{"id":"87516a34-7662-4a77-a219-149777b960dc","color":"red","value":"LinkedList"},{"id":"d63330f6-ba17-4b9c-b370-579f4c99358a","color":"blue","value":"Stack"},{"id":"04eff27e-18a0-4883-8915-6ba26059106d","color":"yellow","value":"Queue"},{"id":"328cf039-a0b4-4b5c-90f7-be587808a1dc","color":"orange","value":"Deque"},{"id":"3364095f-3a21-47c6-8460-16ddc7f02c13","color":"brown","value":"HashTable"},{"id":"64841489-db5b-45ba-972a-7af2f53f41c7","color":"yellow","value":"Graph"},{"id":"b6dea43b-7633-4290-91f0-9761023838b0","color":"purple","value":"Tree"},{"id":"9dc575d0-d0d5-4282-8430-aae294857404","color":"gray","value":"Heap"},{"id":"edc331ab-bf6d-4985-9ecc-6884de83e8fc","color":"brown","value":"Trie"},{"id":"f2fdcca6-c69d-4b53-ab23-665ca7a223b9","color":"yellow","value":"Sort"},{"id":"5e02ca54-b743-4501-9776-62e030442114","color":"default","value":"BinarySearch"},{"id":"9a317b05-c2df-41ff-bf3f-32c39b5c451b","color":"red","value":"Greedy"},{"id":"536f3cec-7932-4444-b8c7-154672ad5fe6","color":"pink","value":"DivideAndConquer"},{"id":"1fe05376-d59a-4e4a-8596-fcd70f194621","color":"orange","value":"Basic"},{"id":"6fdde3f1-6554-4e54-ba5b-af1fc95b0ce4","color":"green","value":"Linear"},{"id":"28fc1aec-d1dd-44bb-a3ab-78ebc6f74837","color":"yellow","value":"NonLinear"},{"id":"e74e9521-68b1-4c11-ab42-a034a44dfd24","color":"blue","value":"알고리즘"},{"id":"42f167a6-7e2e-4d8e-854d-426689132f08","color":"brown","value":"Amazon"},{"id":"8aefb99d-7b6b-453b-941f-e114e32c6438","color":"blue","value":"book"},{"id":"d5083966-d1d3-4116-a615-5107104890c2","color":"pink","value":"Business"},{"id":"df607bdb-4379-4273-a45e-e52d436aef0d","color":"red","value":"Diffusion Model"},{"id":"da21253b-a1e7-4fe8-9ed3-6073a347c457","color":"orange","value":"Paper Review"}]},"NVm^":{"name":"Slug","type":"text"},"QSi`":{"name":"Series","type":"checkbox"},"a\u003cql":{"name":"Published","type":"date"},"jhf;":{"name":"Tweet","type":"text"},"nAX{":{"name":"Created","type":"created_time"},"}nqi":{"name":"Author","type":"text"},"~]S\u003c":{"name":"Description","type":"text"},"title":{"name":"Name","type":"title"}},"format":{"copied_from_pointer":{"id":"e5fdcb8e-6e29-4bc9-828d-263749307808","table":"collection","spaceId":"fde5ac74-eea3-4527-8f00-4482710e1af3"},"property_visibility":[{"property":"AfoN","visibility":"hide_if_empty"},{"property":"BN]P","visibility":"hide_if_empty"},{"property":"a\u003cql","visibility":"hide_if_empty"},{"property":"}nqi","visibility":"hide_if_empty"},{"property":"~]S\u003c","visibility":"hide"},{"property":"==~K","visibility":"hide"},{"property":"jhf;","visibility":"hide"},{"property":"=bhc","visibility":"hide"},{"property":"NVm^","visibility":"hide"},{"property":"nAX{","visibility":"hide"},{"property":";KhU","visibility":"hide"},{"property":"QSi`","visibility":"hide"}],"collection_page_properties":[{"visible":false,"property":"AfoN"},{"visible":true,"property":"BN]P"},{"visible":true,"property":"a\u003cql"},{"visible":false,"property":"}nqi"},{"visible":true,"property":"~]S\u003c"},{"visible":true,"property":"==~K"},{"visible":true,"property":"jhf;"},{"visible":false,"property":"=bhc"},{"visible":false,"property":"NVm^"},{"visible":true,"property":"nAX{"},{"visible":false,"property":";KhU"}]},"parent_id":"4f20ede8-ccf7-4ae1-82e5-819e100dd032","parent_table":"block","alive":true,"copied_from":"e5fdcb8e-6e29-4bc9-828d-263749307808","template_pages":["cacb6437-50cd-4a3d-9d43-58962f75e40b"],"migrated":true,"space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6","deleted_schema":{":nQy":{"name":"Curating","type":"text"}}}}},"collection_view":{"d91647c5-6a81-48b1-a1ff-a04529d0ddba":{"role":"reader","value":{"id":"d91647c5-6a81-48b1-a1ff-a04529d0ddba","version":33,"type":"gallery","name":"Gallery view","format":{"gallery_cover":{"type":"page_cover"},"gallery_cover_size":"medium","gallery_properties":[{"visible":false,"property":"AfoN"},{"visible":false,"property":"jhf;"},{"visible":false,"property":"NVm^"},{"visible":false,"property":"==~K"},{"visible":false,"property":";KhU"},{"visible":false,"property":"=bhc"},{"visible":false,"property":"}nqi"},{"visible":false,"property":"nAX{"},{"visible":false,"property":"BN]P"},{"visible":true,"property":"title"},{"visible":true,"property":"~]S\u003c"},{"visible":true,"property":"a\u003cql"}],"gallery_cover_aspect":"cover","hide_linked_collection_name":true,"inline_collection_first_load_limit":{"type":"load_limit","limit":10}},"parent_id":"4f20ede8-ccf7-4ae1-82e5-819e100dd032","parent_table":"block","alive":true,"page_sort":["e3df5b79-eb34-4bec-a82f-628699f43852","7a8e9c32-39af-4bca-a26e-3f8f1c9a1762","7864e0d8-d646-4df5-a4ae-057371b7559d","36a57bd6-d63a-40f0-a0a2-6ba247885fab","c2994f3d-696a-4015-b7d1-2b71883562ad","58fc3d75-92a2-44e9-ab0d-84cb61086232","30ac6075-b07c-41fa-b684-15c16f1134ba","4c804cac-3119-4cc3-a621-a82040d9a0db","6ce6c0ba-0d6f-40b3-b7d4-afe8ca715c73","64158711-b8f3-4c3d-ba2f-3999059c5581","0c679f04-0865-4142-a65b-28668aa9daea","54fa8c94-ead7-4b14-b068-174936adb37f","ab2e2675-7daf-464f-a2c4-563136630232","ef52bf7e-8726-4804-a0d5-e9b5f7919284","460f888e-b262-4f14-a39f-93ae0090bb9d","eff2e07b-ee44-45e0-b46a-622deb611d0b","c94f3f08-d65f-4f7d-a409-72c353f9786d","daac7673-f388-475e-8f43-a345dc350bef","6ff46924-9cc3-4445-badc-28e7daa5fa5a","c114dc9f-b09a-4e30-9c0d-5ace92e6a8e3","51b78f8b-69ff-4a46-9ec6-74f0d5400d3e","06adcb1d-3acf-4266-a8d9-6cb0d3c439ce","22c49483-380f-4b94-be90-b437b7cbab52","45a3d677-b3df-4cc1-ac40-7e0ab214aeef","7a177274-38ce-4f03-b3f6-06e20b0ba4f8","15aa9135-7cca-40c7-b656-ba4457524924","599373d0-99e2-4c28-89de-d273b43aca5c","48756e2f-e604-4421-9b34-be2c90e20589","b94b3e79-f161-45a8-bf8e-5315f85dca99","3e71fe2e-c827-4fc6-b610-7d71147ae4a7","298f8e1a-a9f2-4274-b5f1-7279f7bc4e5b","c7f629b2-6f4e-43fe-8720-4a3a10d4e651","b1fded7b-ceea-46d0-89e2-32010807d35c","bd99ffd5-ce6d-4418-be0e-6db2cd8ae070","5e852682-71e3-45b9-8056-d40e972563fd","1db23f4a-8d7e-4445-b241-a8e4be5bd02a","70df362d-5c3a-4d2c-a3da-ef27e1f207e3","dae737f3-5216-4c50-b708-2bf7c2662020","a13c9d7e-5e2f-44c3-bd8c-17fd50ca4892","f3236b1d-73c7-45d5-920e-f7cac8c19573","c181e327-c3bf-42f8-b8aa-21a367ce64f2","f5613c52-6b68-4073-b593-03d26f51e710"],"query2":{"sort":[{"property":"=bhc","direction":"descending"},{"property":"a\u003cql","direction":"descending"}],"filter":{"filters":[{"filters":[{"filter":{"value":{"type":"exact","value":true},"operator":"checkbox_is"},"property":"==~K"},{"filter":{"value":{"type":"exact","value":true},"operator":"checkbox_is"},"property":"=bhc"},{"filter":{"value":{"type":"exact","value":true},"operator":"checkbox_is"},"property":"QSi`"}],"operator":"and"}],"operator":"and"},"aggregations":[{"aggregator":"count"}]},"space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"27bc73a3-5779-44e0-b617-2f0d26f5aa2f":{"role":"reader","value":{"id":"27bc73a3-5779-44e0-b617-2f0d26f5aa2f","version":1,"type":"table","name":"","format":{"table_properties":[{"width":293,"visible":true,"property":"title"},{"width":398,"visible":true,"property":"~]S\u003c"},{"width":81,"visible":true,"property":"==~K"},{"width":105,"visible":true,"property":"=bhc"},{"width":146,"visible":true,"property":"a\u003cql"},{"width":200,"visible":true,"property":"BN]P"},{"width":122,"visible":true,"property":"}nqi"},{"width":156,"visible":true,"property":"jhf;"},{"width":146,"visible":true,"property":"NVm^"},{"width":200,"visible":true,"property":";KhU"},{"width":200,"visible":false,"property":"nAX{"}]},"parent_id":"4f20ede8-ccf7-4ae1-82e5-819e100dd032","parent_table":"block","alive":true,"page_sort":["f3d44d4c-975d-4b11-8396-c68b35bfdb26","70df362d-5c3a-4d2c-a3da-ef27e1f207e3","f3236b1d-73c7-45d5-920e-f7cac8c19573","4117e62e-18ec-4503-a32a-6c31806a5e2a","6b9736df-63cb-4e7c-ba8e-d6e54f26d6c9","c2f261a2-c616-4198-b1a9-2caf6be162a0","a082287f-d4c9-4cfb-b3b2-72b8bbf043c6","4801bc76-a763-46a8-981d-79dc38c5d85a","e3df5b79-eb34-4bec-a82f-628699f43852","36a57bd6-d63a-40f0-a0a2-6ba247885fab","c2994f3d-696a-4015-b7d1-2b71883562ad","58fc3d75-92a2-44e9-ab0d-84cb61086232","30ac6075-b07c-41fa-b684-15c16f1134ba","4c804cac-3119-4cc3-a621-a82040d9a0db","6ce6c0ba-0d6f-40b3-b7d4-afe8ca715c73","64158711-b8f3-4c3d-ba2f-3999059c5581","0c679f04-0865-4142-a65b-28668aa9daea","54fa8c94-ead7-4b14-b068-174936adb37f","ab2e2675-7daf-464f-a2c4-563136630232","ef52bf7e-8726-4804-a0d5-e9b5f7919284","460f888e-b262-4f14-a39f-93ae0090bb9d","eff2e07b-ee44-45e0-b46a-622deb611d0b","c94f3f08-d65f-4f7d-a409-72c353f9786d","daac7673-f388-475e-8f43-a345dc350bef","6ff46924-9cc3-4445-badc-28e7daa5fa5a","c114dc9f-b09a-4e30-9c0d-5ace92e6a8e3","51b78f8b-69ff-4a46-9ec6-74f0d5400d3e","06adcb1d-3acf-4266-a8d9-6cb0d3c439ce","22c49483-380f-4b94-be90-b437b7cbab52","1db23f4a-8d7e-4445-b241-a8e4be5bd02a","dae737f3-5216-4c50-b708-2bf7c2662020","a13c9d7e-5e2f-44c3-bd8c-17fd50ca4892","7864e0d8-d646-4df5-a4ae-057371b7559d","7a8e9c32-39af-4bca-a26e-3f8f1c9a1762","0f58fdf7-da3e-4793-93f8-b2503443020d","ca43f24a-f6c2-4611-a92a-8051ec80fb0f","36b681a4-e13e-4a78-a2d6-f56b2e3657a1","8cfc0d5a-59ba-4ea0-9609-4720753d5fba","88891af2-1639-4eff-a2c9-f3ba6668b2ea","92db0a94-6e1e-44a6-8df5-882e60ff5b3f","0d958b63-df7a-44a4-b80c-5408c78f59e4","7a177274-38ce-4f03-b3f6-06e20b0ba4f8","ae0b3ee0-531c-4f81-8d4b-cc6dd040fea1","bf9953ee-2589-4969-948c-0d31106a9deb","fdb7d90f-a355-4e7d-8f9a-3d07a58a457e","561e9779-b56c-4310-8913-d54675e02c74","6951e99b-10cc-4833-a27f-f0e08f8941d0","76a4bb7d-be00-45f3-bbcd-cba28c38588b","18c92268-31e5-4509-8644-9c6ad9e44c10","6463af62-efa9-47dc-91ce-99df728f66e0"],"query2":{"sort":[{"property":"=bhc","direction":"descending"},{"property":"a\u003cql","direction":"descending"}],"aggregations":[{"property":"title","aggregator":"count"}]},"space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}}},"notion_user":{},"collection_query":{},"signed_urls":{},"preview_images":{"images.unsplash.com/photo-1563209259-b2fa97148ce1":{"originalWidth":4509,"originalHeight":3433,"dataURIBase64":"data:image/webp;base64,UklGRkQAAABXRUJQVlA4IDgAAAAQAgCdASoQAAwABUB8JZQCdAEN4Z4/0jgAAP7nI6g5q7g7NuXgHKw945x+RhNJbTW2vaRJ2E2wAA=="},"notion.so/image/notion.so%2Fimages%2Fpage-cover%2Fnasa_reduced_gravity_walking_simulator.jpg":{"originalWidth":2000,"originalHeight":1597,"dataURIBase64":"data:image/webp;base64,UklGRjwAAABXRUJQVlA4IDAAAADQAQCdASoQAA0ABUB8JaQAAuUwaXLIHAD+3O9cAAKxBQKBBYBng9tx6GTwHd24mAA="},"notion.so/image/s3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fb29e9b03-c79c-4e52-a45e-7228163ba524%2Fcompass-circular-tool_(3).png":{"originalWidth":2000,"originalHeight":2000,"dataURIBase64":"data:image/webp;base64,UklGRv4AAABXRUJQVlA4WAoAAAAQAAAADwAADwAAQUxQSFUAAAARL6CmkRQ4eF5eaio0EBGB5+2AUWxbbR5t/yWkKiDsUwVQnn8xoViI6P8EAOK9AiCF/BQQSXKHsKrOmo21PddC4xsKsbafNSc1jfQLgBSSGsD0HBoAAFZQOCCCAAAAsAIAnQEqEAAQAAVAfCWkAA+DSIAfdDvk/UudpKAAAPYcuQdG3VZft4V447Ryb0FWml/BLapzhboSZ6SLFp2WFSojeqJNq2VAANIrwpK47u1M/geoV8ECX9+TEWBDeQC4c2g3HcSldGi6FylcOf6PWeqJaegO6jdPFlsvX04FC2gAAA=="},"notion.so/image/longshiine.github.io%2F2021%2F01%2F25%2FReinforcement-Learning-Basic-5%2Fmontecarlo.png":{"originalWidth":2000,"originalHeight":1939,"dataURIBase64":"data:image/webp;base64,UklGRoYAAABXRUJQVlA4WAoAAAAQAAAADwAADgAAQUxQSCMAAAARHyAUQGDyX2EFkUZExBjMtG2T8UeycJzyD0FE/+OqAf5XBQBWUDggPAAAAPABAJ0BKhAADwAFQHwlsAJ0AREVRVvkAAD+6xrGNEtPAxc/5uKzDNkPKO+BJyEgzI4i+ZZHoAlqLuU0AA=="},"notion.so/image/longshiine.github.io%2F2021%2F01%2F25%2FReinforcement-Learning-Basic-5%2Fdrawing.png":{"originalWidth":2000,"originalHeight":2014,"dataURIBase64":"data:image/webp;base64,UklGRkYAAABXRUJQVlA4IDoAAADwAQCdASoQABAABUB8JaQAAqpKCYT89NoA/vHgxZsPW1HIBDakr2BHox2PQ7+NXT3R76FpCAdwAAAA"},"notion.so/image/longshiine.github.io%2F2021%2F01%2F25%2FReinforcement-Learning-Basic-5%2Fgridworld.png":{"originalWidth":2000,"originalHeight":2127,"dataURIBase64":"data:image/webp;base64,UklGRkIAAABXRUJQVlA4IDYAAADQAQCdASoPABAABUB8JQAAYv8fVMFxoAD+4eEaZZEm9wbxGTLwUDnxn50cZJNUAsyNsxBAAAA="},"notion.so/image/s3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Ffc853118-df50-43ed-96d9-0711493d5e25%2Fjang_inspiration_logo.png":{"originalWidth":2000,"originalHeight":2000,"dataURIBase64":"data:image/webp;base64,UklGRv4AAABXRUJQVlA4WAoAAAAQAAAADwAADwAAQUxQSFUAAAARL6CmkRQ4eF5eaio0EBGB5+2AUWxbbR5t/yWkKiDsUwVQnn8xoViI6P8EAOK9AiCF/BQQSXKHsKrOmo21PddC4xsKsbafNSc1jfQLgBSSGsD0HBoAAFZQOCCCAAAAsAIAnQEqEAAQAAVAfCWkAA+DSIAfdDvk/UudpKAAAPYcuQdG3VZft4V447Ryb0FWml/BLapzhboSZ6SLFp2WFSojeqJNq2VAANIrwpK47u1M/geoV8ECX9+TEWBDeQC4c2g3HcSldGi6FylcOf6PWeqJaegO6jdPFlsvX04FC2gAAA=="}}},"pageId":"dba7311e-93e6-4f18-8cd1-41a08bd33b19","rawPageId":"reinforcement-learning-6"},"__N_SSG":true},"page":"/[pageId]","query":{"pageId":"reinforcement-learning-6"},"buildId":"I-ojLq_lc7g-_5LzXbOVe","isFallback":false,"dynamicIds":[635,7274],"gsp":true,"scriptLoader":[]}</script></body></html>
<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta name="robots" content="index,follow"/><meta property="og:type" content="website"/><meta property="og:site_name" content="Jang. Inspiration"/><meta property="twitter:domain" content="jang-inspiration.com"/><meta name="description" content="앞 장에서 문제를 MDP로 정의하는 방식에 대해 살펴보았다. 이제 본격적으로 가치함수와 큐함수, 벨만 기대 방정식과 벨만 최적 방정식에 대해 톺아보자. "/><meta property="og:description" content="앞 장에서 문제를 MDP로 정의하는 방식에 대해 살펴보았다. 이제 본격적으로 가치함수와 큐함수, 벨만 기대 방정식과 벨만 최적 방정식에 대해 톺아보자. "/><meta name="twitter:description" content="앞 장에서 문제를 MDP로 정의하는 방식에 대해 살펴보았다. 이제 본격적으로 가치함수와 큐함수, 벨만 기대 방정식과 벨만 최적 방정식에 대해 톺아보자. "/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:image" content="https://jang-inspiration.com/api/social-image?id=c6190e2c-1011-4990-9ce3-d219dfd1c0b4"/><meta property="og:image" content="https://jang-inspiration.com/api/social-image?id=c6190e2c-1011-4990-9ce3-d219dfd1c0b4"/><link rel="canonical" href="https://jang-inspiration.com/bellman-equation"/><meta property="og:url" content="https://jang-inspiration.com/bellman-equation"/><meta property="twitter:url" content="https://jang-inspiration.com/bellman-equation"/><link rel="alternate" type="application/rss+xml" href="https://jang-inspiration.com/feed" title="Jang. Inspiration"/><meta property="og:title" content="&lt;파이썬과 케라스로 배우는 강화학습&gt; - (3) 가치함수와 벨만방정식"/><meta name="twitter:title" content="&lt;파이썬과 케라스로 배우는 강화학습&gt; - (3) 가치함수와 벨만방정식"/><title>&lt;파이썬과 케라스로 배우는 강화학습&gt; - (3) 가치함수와 벨만방정식</title><meta name="naver-site-verification" content="3942485b5f7254d146b71f1249d907d89048a4d6"/><link rel="preload" as="image" href="https://images.unsplash.com/photo-1563209259-b2fa97148ce1?ixlib=rb-4.0.3&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb"/><meta name="next-head-count" content="22"/><link rel="shortcut icon" href="/favicon.ico"/><link rel="icon" type="image/png" sizes="32x32" href="favicon.png"/><link rel="manifest" href="/manifest.json"/><link rel="preload" href="/_next/static/css/d2e6a1cb5181bdcf.css" as="style"/><link rel="stylesheet" href="/_next/static/css/d2e6a1cb5181bdcf.css" data-n-g=""/><link rel="preload" href="/_next/static/css/4e32f0fa5eadbe4b.css" as="style"/><link rel="stylesheet" href="/_next/static/css/4e32f0fa5eadbe4b.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script defer="" src="/_next/static/chunks/3607272e.d338bf53926ee7c2.js"></script><script defer="" src="/_next/static/chunks/853.526a4df21aef109c.js"></script><script defer="" src="/_next/static/chunks/175675d1.a2f4b19cd9daa73f.js"></script><script defer="" src="/_next/static/chunks/274.59c48af6aaac8ebd.js"></script><script defer="" src="/_next/static/chunks/358.0027340467b29549.js"></script><script src="/_next/static/chunks/webpack-bcbf8f4abc46b243.js" defer=""></script><script src="/_next/static/chunks/framework-4556c45dd113b893.js" defer=""></script><script src="/_next/static/chunks/main-f08b69bdcc7bbb61.js" defer=""></script><script src="/_next/static/chunks/pages/_app-27630c741ae01a08.js" defer=""></script><script src="/_next/static/chunks/780-3413afc2700f261b.js" defer=""></script><script src="/_next/static/chunks/634-6b5e1cdbf7ca12e3.js" defer=""></script><script src="/_next/static/chunks/pages/%5BpageId%5D-35899cbd792aff57.js" defer=""></script><script src="/_next/static/a6SgCHTQj8ioz2ipgYdLM/_buildManifest.js" defer=""></script><script src="/_next/static/a6SgCHTQj8ioz2ipgYdLM/_ssgManifest.js" defer=""></script></head><body><script>
/** Inlined version of noflash.js from use-dark-mode */
;(function () {
  var storageKey = 'darkMode'
  var classNameDark = 'dark-mode'
  var classNameLight = 'light-mode'
  function setClassOnDocumentBody(darkMode) {
    document.body.classList.add(darkMode ? classNameDark : classNameLight)
    document.body.classList.remove(darkMode ? classNameLight : classNameDark)
  }
  var preferDarkQuery = '(prefers-color-scheme: dark)'
  var mql = window.matchMedia(preferDarkQuery)
  var supportsColorSchemeQuery = mql.media === preferDarkQuery
  var localStorageTheme = null
  try {
    localStorageTheme = localStorage.getItem(storageKey)
  } catch (err) {}
  var localStorageExists = localStorageTheme !== null
  if (localStorageExists) {
    localStorageTheme = JSON.parse(localStorageTheme)
  }
  // Determine the source of truth
  if (localStorageExists) {
    // source of truth from localStorage
    setClassOnDocumentBody(localStorageTheme)
  } else if (supportsColorSchemeQuery) {
    // source of truth from system
    setClassOnDocumentBody(mql.matches)
    localStorage.setItem(storageKey, mql.matches)
  } else {
    // source of truth from document.body
    var isDarkMode = document.body.classList.contains(classNameDark)
    localStorage.setItem(storageKey, JSON.stringify(isDarkMode))
  }
})();
</script><div id="__next"><div class="notion notion-app light-mode notion-block-c6190e2c101149909ce3d219dfd1c0b4"><div class="notion-viewport"></div><div class="notion-frame"><header class="notion-header"><div class="notion-nav-header"><div class="breadcrumbs"><a class="breadcrumb" href="/"><div class="notion-page-icon-inline notion-page-icon-image"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%272000%27%20height=%272000%27/%3e"/></span><img alt="Jang. Inspiration" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" class="icon notion-page-icon" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/webp;base64,UklGRv4AAABXRUJQVlA4WAoAAAAQAAAADwAADwAAQUxQSFUAAAARL6CmkRQ4eF5eaio0EBGB5+2AUWxbbR5t/yWkKiDsUwVQnn8xoViI6P8EAOK9AiCF/BQQSXKHsKrOmo21PddC4xsKsbafNSc1jfQLgBSSGsD0HBoAAFZQOCCCAAAAsAIAnQEqEAAQAAVAfCWkAA+DSIAfdDvk/UudpKAAAPYcuQdG3VZft4V447Ryb0FWml/BLapzhboSZ6SLFp2WFSojeqJNq2VAANIrwpK47u1M/geoV8ECX9+TEWBDeQC4c2g3HcSldGi6FylcOf6PWeqJaegO6jdPFlsvX04FC2gAAA==&quot;)"/><noscript><img alt="Jang. Inspiration" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fb29e9b03-c79c-4e52-a45e-7228163ba524%2Fcompass-circular-tool_(3).png?table=block&amp;id=6246082f-4014-4d06-98ab-59e9840b298a&amp;cache=v2" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="icon notion-page-icon" loading="lazy"/></noscript></span></div><span class="title">Jang. Inspiration</span></a></div><div class="notion-nav-header-rhs breadcrumbs"><a href="/about" class="breadcrumb button">About</a><div class="breadcrumb button styles_hidden__7gYve"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="32" d="M256 48v48m0 320v48m147.08-355.08l-33.94 33.94M142.86 369.14l-33.94 33.94M464 256h-48m-320 0H48m355.08 147.08l-33.94-33.94M142.86 142.86l-33.94-33.94"></path><circle cx="256" cy="256" r="80" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="32"></circle></svg></div><div role="button" class="breadcrumb button notion-search-button"><svg class="notion-icon searchIcon" viewBox="0 0 17 17"><path d="M6.78027 13.6729C8.24805 13.6729 9.60156 13.1982 10.709 12.4072L14.875 16.5732C15.0684 16.7666 15.3232 16.8633 15.5957 16.8633C16.167 16.8633 16.5713 16.4238 16.5713 15.8613C16.5713 15.5977 16.4834 15.3516 16.29 15.1582L12.1504 11.0098C13.0205 9.86719 13.5391 8.45215 13.5391 6.91406C13.5391 3.19629 10.498 0.155273 6.78027 0.155273C3.0625 0.155273 0.0214844 3.19629 0.0214844 6.91406C0.0214844 10.6318 3.0625 13.6729 6.78027 13.6729ZM6.78027 12.2139C3.87988 12.2139 1.48047 9.81445 1.48047 6.91406C1.48047 4.01367 3.87988 1.61426 6.78027 1.61426C9.68066 1.61426 12.0801 4.01367 12.0801 6.91406C12.0801 9.81445 9.68066 12.2139 6.78027 12.2139Z"></path></svg></div></div></div></header><div class="notion-page-scroller"><div class="notion-page-cover-wrapper"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%274509%27%20height=%273433%27/%3e"/></span><img alt="&lt;파이썬과 케라스로 배우는 강화학습&gt; - (3) 가치함수와 벨만방정식" src="https://images.unsplash.com/photo-1563209259-b2fa97148ce1?ixlib=rb-4.0.3&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb" decoding="async" data-nimg="intrinsic" class="notion-page-cover" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-position:center 50%;background-size:cover;background-position:center 50%;filter:blur(20px);background-image:url(&quot;data:image/webp;base64,UklGRkQAAABXRUJQVlA4IDgAAAAQAgCdASoQAAwABUB8JZQCdAEN4Z4/0jgAAP7nI6g5q7g7NuXgHKw945x+RhNJbTW2vaRJ2E2wAA==&quot;)"/><noscript><img alt="&lt;파이썬과 케라스로 배우는 강화학습&gt; - (3) 가치함수와 벨만방정식" src="https://images.unsplash.com/photo-1563209259-b2fa97148ce1?ixlib=rb-4.0.3&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-position:center 50%" class="notion-page-cover"/></noscript></span></div><main class="notion-page notion-page-has-cover notion-page-has-icon notion-page-has-text-icon notion-full-page"><div class="notion-page-icon-hero notion-page-icon-span"><span class="notion-page-icon" role="img" aria-label="🎮">🎮</span></div><h1 class="notion-title">&lt;파이썬과 케라스로 배우는 강화학습&gt; - (3) <b>가치함수와 벨만방정식</b></h1><div class="notion-collection-page-properties"><div class="notion-collection-row"><div class="notion-collection-row-body"><div class="notion-collection-row-property"><div class="notion-collection-column-title"><svg viewBox="0 0 14 14" class="notion-collection-column-title-icon"><path d="M7 13A6 6 0 107 1a6 6 0 000 12zM3.751 5.323A.2.2 0 013.909 5h6.182a.2.2 0 01.158.323L7.158 9.297a.2.2 0 01-.316 0L3.751 5.323z"></path></svg><div class="notion-collection-column-title-body">Category</div></div><div class="notion-collection-row-value"><span class="notion-property notion-property-select"><div class="notion-property-select-item notion-item-orange">강화학습</div></span></div></div><div class="notion-collection-row-property"><div class="notion-collection-column-title"><svg viewBox="0 0 14 14" class="notion-collection-column-title-icon"><path d="M4 3a1 1 0 011-1h7a1 1 0 110 2H5a1 1 0 01-1-1zm0 4a1 1 0 011-1h7a1 1 0 110 2H5a1 1 0 01-1-1zm0 4a1 1 0 011-1h7a1 1 0 110 2H5a1 1 0 01-1-1zM2 4a1 1 0 110-2 1 1 0 010 2zm0 4a1 1 0 110-2 1 1 0 010 2zm0 4a1 1 0 110-2 1 1 0 010 2z"></path></svg><div class="notion-collection-column-title-body">Tags</div></div><div class="notion-collection-row-value"><span class="notion-property notion-property-multi_select"><div class="notion-property-multi_select-item notion-item-pink">Reinforcement Learning</div><div class="notion-property-multi_select-item notion-item-blue">Value Function</div><div class="notion-property-multi_select-item notion-item-green">bellman equation</div></span></div></div><div class="notion-collection-row-property"><div class="notion-collection-column-title"><svg viewBox="0 0 14 14" class="notion-collection-column-title-icon"><path d="M10.889 5.5H3.11v1.556h7.778V5.5zm1.555-4.444h-.777V0H10.11v1.056H3.89V0H2.333v1.056h-.777c-.864 0-1.548.7-1.548 1.555L0 12.5c0 .856.692 1.5 1.556 1.5h10.888C13.3 14 14 13.356 14 12.5V2.611c0-.855-.7-1.555-1.556-1.555zm0 11.444H1.556V3.944h10.888V12.5zM8.556 8.611H3.11v1.556h5.445V8.61z"></path></svg><div class="notion-collection-column-title-body">Published</div></div><div class="notion-collection-row-value"><span class="notion-property notion-property-date">January 22, 2021</span></div></div><div class="notion-collection-row-property"><div class="notion-collection-column-title"><svg viewBox="0 0 14 14" class="notion-collection-column-title-icon"><path d="M7 4.568a.5.5 0 00-.5-.5h-6a.5.5 0 00-.5.5v1.046a.5.5 0 00.5.5h6a.5.5 0 00.5-.5V4.568zM.5 1a.5.5 0 00-.5.5v1.045a.5.5 0 00.5.5h12a.5.5 0 00.5-.5V1.5a.5.5 0 00-.5-.5H.5zM0 8.682a.5.5 0 00.5.5h11a.5.5 0 00.5-.5V7.636a.5.5 0 00-.5-.5H.5a.5.5 0 00-.5.5v1.046zm0 3.068a.5.5 0 00.5.5h9a.5.5 0 00.5-.5v-1.045a.5.5 0 00-.5-.5h-9a.5.5 0 00-.5.5v1.045z"></path></svg><div class="notion-collection-column-title-body">Author</div></div><div class="notion-collection-row-value"><span class="notion-property notion-property-text"><b>Jay</b></span></div></div></div></div></div><div class="notion-page-content notion-page-content-has-aside"><article class="notion-page-content-inner"><blockquote class="notion-quote notion-block-7caccd5251ba4591ae1f6f47e82c07a5"><div>본 포스트는 “파이썬과 케라스로 배우는 강화학습” 도서의 세번째 리뷰 포스트입니다.</div></blockquote><div class="notion-text notion-block-3a27b945debc4971918c529d7c869ee2"><a target="_blank" rel="noopener noreferrer" class="notion-link" href="http://www.yes24.com/Product/Goods/44136413">http://www.yes24.com/Product/Goods/44136413</a></div><div class="notion-blank notion-block-27fa4255a0584c799e8d36321d739557"> </div><div class="notion-table-of-contents notion-gray notion-block-b60dff2917a04513b652c49f2df5eb67"><a href="#261e0e7282cd43418d4b2f714d68d988" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:0">2장 강화학습 기초 1: 가치함수와 벨만 방정식</span></a><a href="#b752bd0c61614559aa38dc156652dbca" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">가치함수</span></a><a href="#80382a0f853c4d44af89368e6e6162f3" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">큐함수</span></a><a href="#a66b49323fd84d1a864b77b3b10d3fa3" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">벨만 기대 방정식</span></a><a href="#91186934e1c04f8ba89eb993a8342934" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">벨만 최적 방정식</span></a><a href="#d734ec80b47e455492e9b312b5689892" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">정리</span></a><a href="#417f55c09f2f42a88309f07303d0e406" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">2장 한줄평</span></a></div><div class="notion-blank notion-block-92ee542b609f4ad0b91ab3da6adc1296"> </div><h3 class="notion-h notion-h2 notion-h-indent-0 notion-block-261e0e7282cd43418d4b2f714d68d988" data-id="261e0e7282cd43418d4b2f714d68d988"><span><div id="261e0e7282cd43418d4b2f714d68d988" class="notion-header-anchor"></div><a class="notion-hash-link" href="#261e0e7282cd43418d4b2f714d68d988" title="2장 강화학습 기초 1: 가치함수와 벨만 방정식"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>2장 강화학습 기초 1: 가치함수와 벨만 방정식</b></span></span></h3><hr class="notion-hr notion-block-996a4e3c3f484d3bb9d63cb3621ac969"/><div class="notion-text notion-block-f2c9362977bd48918dac45814add6e17">이제 에이전트가 학습할 수 있도록 문제를 <code class="notion-inline-code">MDP</code>로 정의 했다. 에이전트는 <code class="notion-inline-code">MDP</code>를 통해 <b>최적 정책</b> 을 찾으면 된다. 이제 에이전트가 어떻게 최적 정책을 찾을 수 있을지 보다 구체적으로 알아보도록 하자!<b>(❗️수식 주의)</b></div><div class="notion-blank notion-block-a67603c15bec440abc436308b596dac9"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-b752bd0c61614559aa38dc156652dbca" data-id="b752bd0c61614559aa38dc156652dbca"><span><div id="b752bd0c61614559aa38dc156652dbca" class="notion-header-anchor"></div><a class="notion-hash-link" href="#b752bd0c61614559aa38dc156652dbca" title="가치함수"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>가치함수</b></span></span></h4><div class="notion-text notion-block-fb10810bfcb8499cb7d807f01806aefc">에이전트의 입장에서 어떤 행동을 하는 것이 좋은지는 어떻게 알 수 있을까? 앞으로 받을 보상들을 고려해서 선택해야 좋은 선택을 할텐데, 아직 받지 않는 보상들을 어떻게 고려한단 말일까?🧐 <b>앞으로 받을 보상에 관련된 개념</b> 이 바로 <code class="notion-inline-code">가치 함수</code>이다.</div><div class="notion-text notion-block-6815777ecee9481f8a48b891931d65d4">현재 시간 t로부터 에이전트가 행동을 하면서 받을 보상들을 단순히 합한다는 생각을 해볼 수 있다. 그렇게 되면 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>와 같은 꼴일 것이다. 그런데 시간에 따른 보상을 이렇게 단순하게 더한다면 세가지 문제가 생긴다.</div><ol start="1" class="notion-list notion-list-numbered notion-block-9f1d89f7511146f8bf28cd89d514969e"><li><b>지금 받은 보상이나 미래에 받는 보상이나 똑같이 취급한다.</b></li></ol><ol start="2" class="notion-list notion-list-numbered notion-block-4fc910a10ef047c3a23330bc8b5e8144"><li><b>100이라는 보상을 1번 받는 것과 20이라는 보상을 5번 받는 것을 구분할 방법이 없다.</b></li></ol><ol start="3" class="notion-list notion-list-numbered notion-block-13f62577181e45f28ba69dff90328cc6"><li><b>시간이 무한대라고 하면 보상을 시간마다 0.1씩 받아도 합이 무한대이고 1씩 받아도 합이 무한대이다.</b></li></ol><div class="notion-text notion-block-998e359fa4554c92abc0c53996e758af">이러한 문제 때문에 에이전트는 단순한 보상의 합으로는 <b>시간 t에 있었던 상태가 어떤 가치를 가지는지</b> 판단하기가 어렵다. 따라서 좀 더 정확하게 상태의 가치를 판단하기 위해 이전 포스트에서 설명한 <code class="notion-inline-code">할인율</code>이라는 개념을 사용한다. 할인율을 적용하여 시간 t 이후의 <b>시간 t시점에서의 보상</b> 을 모두 더한 것을 <code class="notion-inline-code">반환값</code>이라고 하며 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>로 표현한다. 반환값을 수식으로 표현하면 아래와 같다.</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-e180fbfc56814db38958667cc9e483c0"><span></span></span><div class="notion-text notion-block-41bb65293d8346a48800799bdfe3aa62">반환값이라는 것은 에이전트가 실제로 환경을 탐험하며 받은 보상의 합으로, 이 책에서는 에이전트와 환경이 <b>유한한 시간동안 상호작용</b> 하는 경우만 다룬다. 이렇게 <b>유한한 에이전트와 환경의 상호작용</b> 을 <code class="notion-inline-code">에피소드</code>라고 부른다. 에피소드 에서는 에피소드를 끝낼 수 있는 마지막 상태가 있는데, 체스의 경우를 생각한다면 킹을 잃는 순간이 마지막 상태가 된다.</div><div class="notion-text notion-block-0757549dc77a491dafef8cc93a167e69">에이전트가 에피소드가 끝난 후에 <b>‘그때로부터 얼마의 보상을 받았지?’</b> 라며 보상을 정산하는 것이 반환값이다. 만일 에피소드를 t=1 부터 5까지 진행했다면 에피소드가 끝난 후에 방문했던 상태들에 대한 5개의 반환값이 생길 것이다.</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-80ccb2ce5c6244ba924b11d513a1d0f7"><span></span></span><div class="notion-text notion-block-4b0b4a92f4d94cb685fbd27cb5e3bcbe">MDP로 정의 되는 세계에서 반환값은 에피소드마다 다를 수 있다. 때문에 에이전트는 특정 상태의 가치를 <b>반환값에 대한 기댓값</b> 으로 판단해야 한다. 이것이 바로 <code class="notion-inline-code">가치함수</code>의 개념이다. 따라서 가치함수는 아래와 같이 표현된다.</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-e7f3dff4ca8a43528a2dba67bdc6175c"><span></span></span><div class="notion-text notion-block-b7b6cedec22e4274876d303bf9c30a9d">각 타임스텝마다 받는 보상이 모두 확률적이고 반환값이 그 보상들의 합이므로 반환값은 <b>확률변수</b> 이다. 하지만 가치함수는 확률변수가 아니라 특정 양을 나타내는 값이므로 <b>소문자</b>로 표현한다. 가치함수의 식에 위에서 정의한 반환값의 수식을 대입하고 정리해보면 아래와 같다.</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-30485274d2534b0fa0816f6ad4564e1e"><span></span></span><div class="notion-text notion-block-6bce39cc623e4680a3e81eff7dd29b2f">사실 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>부분을 반환값의 형태로 표현하긴 했지만 사실 에이전트가 실제로 받은 보상이 아니다. 이 보상은 <b>앞으로 받을 것이라 예상하는 보상</b> 으로써 이 부분 또한 가치함수로 표현할 수 있다.</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-a13eb7e42975475db80639687500b2e8"><span></span></span><div class="notion-text notion-block-62cdb445bb8847b0b4709406ecf3eb66">여기까지는 가치함수를 정의할 때 정책을 고려하지 않았다. 하지만 에이전트가 앞으로 받을 보상에 대해 생각할 때 정책을 고려하지 않으면 안된다. 왜냐하면 상태에서 상태로 넘어갈 때 에이전트는 무조건 행동을 해야 하고 <b>각 상태에서의 행동을 결정하는 것</b> 이 에이전트의 정책이기 때문이다.</div><div class="notion-text notion-block-10ae23f4f8ea4efebc69981e3c4c1f09">보상은 어떤 상태에서 어떤 행동을 하는지에 따라 환경에서 에이전트에게 주어진다. 이말은 곧 <code class="notion-inline-code">MDP</code>로 정의 되는 문제에서의 가치함수가 행동을 결정하는 <code class="notion-inline-code">정책</code>에 의존한다는 것이다. 따라서 아래의 수식처럼 <b>가치함수에 아래 첨자로 정책을 쓰면</b> 더 명확한 수식이 된다.</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-bfc4cc97ec2f42149f29f32ab73b5ac3"><span></span></span><div class="notion-text notion-block-6b40aabbe7814c029dd12211c05e3d76">이 수식이 바로 강화학습에서 상당히 중요한 <code class="notion-inline-code">벨만 기대 방정식(Bellman Expectation Equation)</code>이다.벨만 기대 방정식은 <b>현재 상태의 가치함수 </b><b><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span></b><b>와 다음상태의 가치함수 </b><b><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span></b><b>사이의 관계</b> 를 말해주는 방정식이다.</div><blockquote class="notion-quote notion-block-5252a9234a014c8290018031951b9122"><div>강화학습은 벨만 방정식을 어떻게 풀어가느냐의 스토리이다 🤧</div></blockquote><div class="notion-blank notion-block-bdc7a4f17661418c93afc369e752570e"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-80382a0f853c4d44af89368e6e6162f3" data-id="80382a0f853c4d44af89368e6e6162f3"><span><div id="80382a0f853c4d44af89368e6e6162f3" class="notion-header-anchor"></div><a class="notion-hash-link" href="#80382a0f853c4d44af89368e6e6162f3" title="큐함수"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>큐함수</b></span></span></h4><div class="notion-text notion-block-e668f5023b55470bbbc7c2c59e0c254a">가치함수는 말 그대로 <b>“함수”</b> 이다. 따라서 입력이 무엇이고 출력이 무엇인지 알 필요가 있다. 지금까지 설명한 가치함수는 <code class="notion-inline-code">상태 가치함수(state value-function)</code>으로써 <b>상태</b> 가 입력으로 들어오면 그 상태에서 <b>앞으로 받을 보상의 합</b> 을 출력하는 함수이다. 따라서 에이전트는 가치함수를 통해 어떤 상태에 있는 것이 얼마나 좋은지를 알 수 있다.</div><div class="notion-text notion-block-294e7331565c49d89c2ebff18e1378b1">상태 가치함수가 각 상태에 대해 가치를 알려주는 것처럼 각 행동에 대해 가치를 알려주는 함수가 있다면 어떨까? 아마 에이전트는 그 함수의 값만 보고 바로 행동을 선택할 수 있을 것이다. <b>어떤 상태에서 어떤 행동이 얼마나 좋은지</b> 알려주는 함수를 <code class="notion-inline-code">큐함수(Q Function)</code>, 다른말로 행동 가치함수 라고 한다.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-d9681d8d237647bda82b415bebde7de2"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:240px;max-width:100%;flex-direction:column"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%272000%27%20height=%272348%27/%3e"/></span><img alt="큐함수" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/webp;base64,UklGRkIAAABXRUJQVlA4IDYAAADwAQCdASoOABAABUB8JaQAAuddOGI6OAAA/vHapZeURANzT1qe3DO+M+KvbCrGHsNYXURsAAA=&quot;)"/><noscript><img alt="큐함수" src="https://www.notion.so/image/https%3A%2F%2Flongshiine.github.io%2F2021%2F01%2F22%2FReinforcement-Learning-Basic-2%2Fqfunction.jpeg?table=block&amp;id=d9681d8d-2376-47bd-a82b-415bebde7de2&amp;cache=v2" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></span><figcaption class="notion-asset-caption">큐함수</figcaption></div></figure><div class="notion-text notion-block-bf439e4ee19045f8abf7bcc0957a9fb1">큐함수는 상태, 행동이라는 두가지 변수를 가지며 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>라고 나타낸다. 또한 가치함수와 큐함수 사이의 관계식은 다음과 같이 표현할 수 있다.</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-66b81c50134e4393bab7784732a1e705"><span></span></span><ol start="1" class="notion-list notion-list-numbered notion-block-ac802ea3f658419d852294d79fe2acfb"><li>각 행동을 했을 때 앞으로 받을 보상인 큐함수 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>를 정책 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>에 곱한다</li></ol><ol start="2" class="notion-list notion-list-numbered notion-block-1ca45c2d98a542cfaae2cda6aa9a5b7d"><li>모든 행동에 대해 큐함수와 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>를 곱한 값을 더하면 가치함수가 된다.</li></ol><div class="notion-text notion-block-751d7baa4d7e45719fe889a5d8612d31">큐함수는 강화학습에서 중요한 역할을 한다. <b>강화학습에서 에이전트가 행동을 선택하는 기준</b> 으로 가치 함수 보다는 보통 <b>큐함수</b> 를 사용한다. 그 이유는 뒤에서 등장한다! 큐함수 또한 벨만 기대 방정식의 형태로 나타 낼수 있으며 조건문에 행동이 들어간다는 점에서 가치함수의 식과 다르다.</div><div class="notion-text notion-block-6ec173325ce54a13b79024904ed656c5"><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span></div><div class="notion-blank notion-block-d531b2532f1d4af1bc6a9b5dcbc61cf2"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-a66b49323fd84d1a864b77b3b10d3fa3" data-id="a66b49323fd84d1a864b77b3b10d3fa3"><span><div id="a66b49323fd84d1a864b77b3b10d3fa3" class="notion-header-anchor"></div><a class="notion-hash-link" href="#a66b49323fd84d1a864b77b3b10d3fa3" title="벨만 기대 방정식"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>벨만 기대 방정식</b></span></span></h4><div class="notion-text notion-block-004967b4b28940e7bac0c68cf94d0c94">이제 2장의 메인 디쉬인 <code class="notion-inline-code">벨만 기대 방정식</code>을 찬찬히 살펴보자(👂🏻집중). 살짝 정리를 해보면 벨만 <b>기대</b> 방정식이라고 하는 이유는 식에 <b>기댓값의 개념</b> 이 들어가기 때문이고, 이 벨만 방정식은 <b>현재 상태의 가치함수와 다음 상태의 가치함수 사이의 관계</b> 를 식으로 나타낸 것이었다.</div><div class="notion-text notion-block-59068fdbb6ed4c208dd363f6936f3e75">벨만 방정식은 강화학습에서 상당히 중요한 부분을 차지한다. 벨만 방정식이 강화학습에서 왜 그렇게 중요한 위치를 차지하고 있는 걸까?🤔 앞에서 정의했던 가치함수의 정의를 다시 한번 살펴보자.</div><div class="notion-text notion-block-aa30ebe680c049328fe541b1d8c4a479">이 수식으로 부터 기댓값을 알아내려면 앞으로 받을 보상에 대해 고려해야하고, 이는 정의상으로는 가능하지만 <b>상태가 많아질수록 상당히 비효율적인 방법</b> 이다. 따라서 컴퓨터가 이 기댓값을 계산하기 위해 다른 조치가 필요하다.</div><div class="notion-text notion-block-721c84d5d26c4ce98e58efd6cfe82752">예를 들어 1을 100번 더해야하는 문제가 있다고 해보자. 식 하나로 풀어내는 방법은 아래와 같다</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-83cbd1c0a4d54af7bdd914f66106eea2"><span></span></span><div class="notion-text notion-block-c5f6ef91a5334b48bc4df04a6e083566">하지만 다른 방법으로 접근해볼 수도 있다. x라는 변수를 지정해 그 값에 1을 계속 더해나가는 것이다.</div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python">X = 0
for i in range(100):
  X = X + 1</code></pre><div class="notion-text notion-block-6c87999ffd1c4e4e913cc54f1e9f8829">벨만 방정식으로 가치함수를 계산하는 것은 두 번째 방식과 같은 것이다. 한 번에 모든 것을 계산하는 것이 아니라 값을 변수에 저장하고, 루프를 도는 계산을 통해 참 값을 알아나가는 것이다. 즉, <b>현재 가치함수 값을 업데이트</b> 해 나가는 것이다. 하지만 업데이트 하려면 기댓값을 계산해야 하는데 기댓값은 어떻게 계산할 수 있을까?</div><div class="notion-text notion-block-ef0191f16d6d42769197d8a66c03b9f0">기댓값에는 어떠한 행동을 할 확률(<b>정책 </b><b><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span></b>)과 그 행동을 했을 때 어떤 상태로 가게 되는 확률(<b>상태 변환 확률 </b><b><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span></b>)이 포함되어 있다. 따라서 정책과 상태 변환 확률을 포함해서 계산하면 된다.</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-92a4b20a474e45799ea92d8cbcabf74b"><span></span></span><div class="notion-text notion-block-4c9d1637363f4f84aea9d1b19874463f">예시를 한번 살펴보는 것이 매우 도움이 될 것이다.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-913e2530875d4cd3bcd3fa6631fb8497"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:100%;max-width:100%;flex-direction:column"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%272000%27%20height=%27753%27/%3e"/></span><img alt="벨만 기대 방정식의 예시" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/webp;base64,UklGRjIAAABXRUJQVlA4ICYAAAAwAQCdASoQAAYABUB8JaQAA3AA/u/25CyNHHTnULWZhZDJYSoAAA==&quot;)"/><noscript><img alt="벨만 기대 방정식의 예시" src="https://www.notion.so/image/https%3A%2F%2Flongshiine.github.io%2F2021%2F01%2F22%2FReinforcement-Learning-Basic-2%2Fbellman.png?table=block&amp;id=913e2530-875d-4cd3-bcd3-fa6631fb8497&amp;cache=v2" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" loading="lazy"/></noscript></span><figcaption class="notion-asset-caption">벨만 기대 방정식의 예시</figcaption></div></figure><div class="notion-blank notion-block-d6c5b8e5acfa4f9490f2816252fa1c7f"> </div><div class="notion-text notion-block-7daa452fb0a94bb0b824f428e74367a8">설명을 용이하게 하기 위해,</div><ol start="1" class="notion-list notion-list-numbered notion-block-09dbf85aea754437bf46831aa94d0565"><li><code class="notion-inline-code">상태 변환 확률</code>(<span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>)은 모두 1이라고 생각하고(왼쪽으로 가기로 결정했다면 1의 확률로 왼쪽으로 간다고 하자)</li></ol><ol start="2" class="notion-list notion-list-numbered notion-block-9a464cb341a946a8b8eaee68cd919b7a"><li><code class="notion-inline-code">정책</code>(<span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>)은 무작위로 행동하는 것으로서 각 행동이 25%의 확률로 선택이 되며,</li></ol><ol start="3" class="notion-list notion-list-numbered notion-block-a421c8fd1086451bb2e26a2992953e6d"><li><code class="notion-inline-code">할인율</code>(<span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>)은 0.9라고 생각하고,</li></ol><ol start="4" class="notion-list notion-list-numbered notion-block-689164f5f93443628c52973e25631227"><li><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>은 모든 상태일 수 있으나, 에이전트의 행동으로 도착하게 될 상태라고 해보자.</li></ol><ol start="5" class="notion-list notion-list-numbered notion-block-da29e5888c924505ab19a2c5c5b86f13"><li>회색 별은 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>에 해당하는 보상 값이다.</li></ol><div class="notion-text notion-block-8b643dce01f84efdabb6179f96d7582c">위의 가정을 적용하여 식을 바꾸어 본다면,</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-505cefb7627e4ae38af7da75d76472d7"><span></span></span><div class="notion-text notion-block-aa0fdf6da6364727b43809d6f080c7e3">와 같이 바꿀 수 있고, 위의 표와 같이 계산을 할 수 있게 된다.</div><div class="notion-blank notion-block-fd2a2fbe282b44949f962ca995ed9313"> </div><blockquote class="notion-quote notion-block-00b88caef2a14aa9a5e070b8adb22df7"><div>벨만 기대 방정식을 이용해 현재의 가치함수를 계속 업데이트 하다 보면,에이전트가 얻을 실제 보상에 대한 참 기댓값을 얻을 수 있다.</div></blockquote><div class="notion-blank notion-block-875dcde0a3414b95a64f28c85a73e574"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-91186934e1c04f8ba89eb993a8342934" data-id="91186934e1c04f8ba89eb993a8342934"><span><div id="91186934e1c04f8ba89eb993a8342934" class="notion-header-anchor"></div><a class="notion-hash-link" href="#91186934e1c04f8ba89eb993a8342934" title="벨만 최적 방정식"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>벨만 최적 방정식</b></span></span></h4><div class="notion-text notion-block-eacf08c0a632445c80d51bb23a2f0b47">처음의 가치함수의 값들은 의미가 없는 값으로 초기화 된다. 초기값으로 부터 시작해서 <code class="notion-inline-code">벨만 기대 방정식</code>으로 반복적으로 계산한다고 가정해보자. 이 계산을 반복하다보면 방정식의 우항과 좌항이 같아진다(무한히 반복한다는 가정하에). 즉, <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>값이 수렴하는 것이다. 그렇다면 현재의 정책 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>에 대한 <b>참 가치함수</b> 를 구한것이다.</div><div class="notion-text notion-block-9423b856e96a484c9f374dddd024a855">벨만 기대 방정식을 기댓값을 계산하기 위해 살짝 변형하면, 현재 정책에 대한 <b>참 가치함수</b> 를 구할 수 있다. 위의 식은 뒤에서 배울 dynamic programming에서 자세히 다루도록 한다.</div><div class="notion-text notion-block-ec71d70431944664b9bddd09ff499acd">그러나 <b>참 가치함수</b> 와 <b>최적 가치함수(Optimal Value Function)</b> 은 다르다⭐️. <code class="notion-inline-code">참 가치함수</code>는 <b>어떤 정책을 따라서 움직였을 경우에 받게되는 보상에 대한 참값</b> 이고, <code class="notion-inline-code">최적의 가치함수</code>는 <b>수많은 정책 중에서 가장 높은 보상을 얻게 되는 정책을 따랐을 때의 가치함수</b> 이다.</div><div class="notion-text notion-block-348fdd7ef50f4c368ad5e501987f667e">그렇다. 우리는 단순히 현재 정책에 대한 가치함수를 구하고 싶은게 아닌, <b>최적 정책</b>을 찾고 싶은 것이다. 단순히 현 정책에 대한 가치함수를 찾는 것이 아닌 더 좋은 정책으로 현재의 정책을 업데이트 해 나가야 한다. 이쯤에서 이런 질문이 들어야 한다.</div><ul class="notion-list notion-list-disc notion-block-f1c069f422904336a33d432d0cbfd0ce"><li>더 좋은 정책이라는 것의 정의는 무엇인가?</li></ul><ul class="notion-list notion-list-disc notion-block-9d775ffcc0ed447db2acea5c13c78f67"><li>어떤 정책이 더 좋은 정책이라고 판단할 수 있을까?</li></ul><div class="notion-text notion-block-4670c8787f3b49569eb008e41ba21a30">더 좋은 정책이란 정책을 따라갔을 때 받을 보상들의 합이 더 큰 경우라고 말할 수 있을 것이다. 그리고 그것은 <b>가치함수</b> 를 통해 판단할 수 있었다. 결국 가치함수가 정책이 얼마나 좋은지를 말해주는 것이다. 따라서 <b>모든 정책 중, 가장 큰 가치함수를 갖는 정책이 최적 정책이다.</b> 최적 큐함수 또한 같은 방식으로 생각한다.</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-53a2ffd5844e404d940bb94bad2e0654"><span></span></span><div class="notion-text notion-block-c93fa6c5398248f1a1784f6c5eba839e">가장 높은 가치함수(큐함수)를 에이전트가 찾았다고 가정해보자. 이때 <b>최적 정책</b> 은 각 상태 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>에서의 최적의 큐함수에 대해 가장 큰 값을 가진 행동을 하는 것이다. 즉, 선택 상황에서 판단 기준은 큐함수이며, 최적 정책은 최적 큐함수 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>만 안다면 아래와 같이 구할 수 있다.</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-cd44bbfe05b2473cb4221dc02c96ecfb"><span></span></span><div class="notion-text notion-block-2d3824c67a2d4506adaa77c6611ccb5e">그렇다면 최적의 큐함수는 어떻게 구할 수 있을까?<!-- -->그것을 구하는 것이 <b>순차적 행동 결정 문제(MDP)</b> 를 푸는 것이다. 어떻게 최적의 가치함수를 구하는지에 대해서는 다음 장에서 다룬다. 여기서는 최적의 가치함수 끼리 관계가 어떻게 되는지를 살펴보자.</div><div class="notion-text notion-block-81c7d476748b4c2397a302eb5ff64419">현재 상태의 가치함수가 최적이라고 가정해보자. 현재상태의 가치함수가 최적이라는 것은 에이전트가 가장 좋은 행동을 선택한다는 것이다. 이때 선택의 기준이 되는 큐함수는 최적의 큐함수 이어야 하고, 따라서 다음이 최적의 가치함수의 식이 된다.</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-d653ec9264ef4ec18bdab99192294df5"><span></span></span><div class="notion-text notion-block-07502acf278845ff80b18dac6863547e">여기서 큐함수를 가치함수로 고쳐서 표현하면 아래와 같다.</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-6c834085f83741b8aefacd934aede06c"><span></span></span><div class="notion-text notion-block-eb386ca4154d433ba5508d8fdee92646">바로 이 식을 <code class="notion-inline-code">벨만 최적 방정식(Bellman Optimality Equation)</code> 이라 부르며, 이식은 최적의 가치함수에 대한 것이다. 큐함수에 대해서도 벨만 최적 방정식을 표현할 수 있는데, 아래와 같이 표현한다.</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-5357aa08c45f4f42a7eb5ca390eb9729"><span></span></span><div class="notion-text notion-block-cdfd57bd92f34e8ca00be69615886692">기댓값인 이유는 다음 상태가 상태 변환 확률에 따라 달라지기 때문이다. 벨만 기대 방정식과 벨만 최적 방정식을 이용해 MDP로 정의되는 문제를 <b>계산</b> 으로 푸는 방법이 바로 다음장에서 다룰 <code class="notion-inline-code">다이내믹 프로그래밍(Dynamic programming)</code>이다.</div><div class="notion-blank notion-block-b624f66f51274f0da1332140a36b4169"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-d734ec80b47e455492e9b312b5689892" data-id="d734ec80b47e455492e9b312b5689892"><span><div id="d734ec80b47e455492e9b312b5689892" class="notion-header-anchor"></div><a class="notion-hash-link" href="#d734ec80b47e455492e9b312b5689892" title="정리"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>정리</b></span></span></h4><ol start="1" class="notion-list notion-list-numbered notion-block-3e283b6a1df64772ad10d4f6dde5076a"><li><code class="notion-inline-code">MDP</code>: 순차적 행동 결정 문제를 수학적으로 정의하는 것. 상태(<span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>), 행동(<span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>), 보상함수(<span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>), 상태 변환 확률(<span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>), 할인율(<span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>)로 구성.</li><ol class="notion-list notion-list-numbered notion-block-3e283b6a1df64772ad10d4f6dde5076a"><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-a10b90312bde42aa90f744f00c05206e"><span></span></span></ol></ol><ol start="2" class="notion-list notion-list-numbered notion-block-45a5da3d129c4d578508a3b9993977f1"><li><code class="notion-inline-code">가치함수</code>: 현재 상태로부터 정책을 따라갔을 때 받을 것이라 예상되는 보상의 합.</li><ol class="notion-list notion-list-numbered notion-block-45a5da3d129c4d578508a3b9993977f1"></ol></ol><ol start="3" class="notion-list notion-list-numbered notion-block-39b23947fdad459189449b1c600a6c59"><li><code class="notion-inline-code">큐함수</code>: 각 행동에 대해 가치를 알려주는 함수, 정책 업데이트 시에 사용</li></ol><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-8810b4c237374ea5a00bb92e5953072a"><span></span></span><ol start="1" class="notion-list notion-list-numbered notion-block-e3a3f05bfd2046688b1ac4ab369d85e1"><li><code class="notion-inline-code">벨만 기대 방정식</code>: 현재 상태의 가치함수와 다음 상태 가치함수의 관계식</li><ol class="notion-list notion-list-numbered notion-block-e3a3f05bfd2046688b1ac4ab369d85e1"><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-d6e68191e68644c0b9ba3dcfda2f5a3c"><span></span></span></ol></ol><ol start="2" class="notion-list notion-list-numbered notion-block-7dcbb1c566944240aa750d867c360590"><li><code class="notion-inline-code">벨만 최적 방정식</code>: 최적의 정책을 따르는 가치함수와 다음 상태 가치함수의 관계식</li><ol class="notion-list notion-list-numbered notion-block-7dcbb1c566944240aa750d867c360590"><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-958d8ba1193f46b9b050473997d80235"><span></span></span><div class="notion-blank notion-block-7f7b990c044648c99db3e76fe4569f5c"> </div></ol></ol><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-417f55c09f2f42a88309f07303d0e406" data-id="417f55c09f2f42a88309f07303d0e406"><span><div id="417f55c09f2f42a88309f07303d0e406" class="notion-header-anchor"></div><a class="notion-hash-link" href="#417f55c09f2f42a88309f07303d0e406" title="2장 한줄평"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>2장 한줄평</b></span></span></h4><blockquote class="notion-quote notion-block-bfd5656f5900455fba71171c864a947e"><div>최적 방정식 부분을 좀 더 명확하게 다시 정리해 봐야겠다. 🤥</div></blockquote></article><aside class="notion-aside"></aside></div></main></div></div></div><div style="width:100%;background-color:#ffffff;color:#373534;padding:20px"><div id="disqus_recommendations"></div><div id="disqus_thread"></div><footer class="styles_footer__RBpyk"><div class="styles_copyright__nhL_k">Copyright 2023 <!-- -->Jang Yeong</div><div class="styles_settings__GyEhi"></div><div class="styles_social__ptL3p"><a class="styles_github__0JN7a" href="https://github.com/longshiine" title="GitHub @longshiine" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a class="styles_linkedin__bgwDi" href="https://www.linkedin.com/in/jangyeong-kim-b7924422a" title="LinkedIn Jang Yeong" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a><a class="styles_instagram__BY5Hj" href="https://instagram.com/jang.inspiration" title="Instagram Jang Yeong" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><g><path fill="none" d="M0 0h24v24H0z"></path><path fill-rule="nonzero" d="M12 2c2.717 0 3.056.01 4.122.06 1.065.05 1.79.217 2.428.465.66.254 1.216.598 1.772 1.153a4.908 4.908 0 0 1 1.153 1.772c.247.637.415 1.363.465 2.428.047 1.066.06 1.405.06 4.122 0 2.717-.01 3.056-.06 4.122-.05 1.065-.218 1.79-.465 2.428a4.883 4.883 0 0 1-1.153 1.772 4.915 4.915 0 0 1-1.772 1.153c-.637.247-1.363.415-2.428.465-1.066.047-1.405.06-4.122.06-2.717 0-3.056-.01-4.122-.06-1.065-.05-1.79-.218-2.428-.465a4.89 4.89 0 0 1-1.772-1.153 4.904 4.904 0 0 1-1.153-1.772c-.248-.637-.415-1.363-.465-2.428C2.013 15.056 2 14.717 2 12c0-2.717.01-3.056.06-4.122.05-1.066.217-1.79.465-2.428a4.88 4.88 0 0 1 1.153-1.772A4.897 4.897 0 0 1 5.45 2.525c.638-.248 1.362-.415 2.428-.465C8.944 2.013 9.283 2 12 2zm0 5a5 5 0 1 0 0 10 5 5 0 0 0 0-10zm6.5-.25a1.25 1.25 0 0 0-2.5 0 1.25 1.25 0 0 0 2.5 0zM12 9a3 3 0 1 1 0 6 3 3 0 0 1 0-6z"></path></g></svg></a></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"site":{"domain":"jang-inspiration.com","name":"Jang. Inspiration","rootNotionPageId":"6246082f40144d0698ab59e9840b298a","rootNotionSpaceId":null,"description":"장영의 영감노트"},"recordMap":{"block":{"c6190e2c-1011-4990-9ce3-d219dfd1c0b4":{"role":"reader","value":{"id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","version":1422,"type":"page","properties":{"==~K":[["Yes"]],"AfoN":[["강화학습"]],"BN]P":[["Reinforcement Learning,Value Function,bellman equation"]],"NVm^":[["bellman-equation"]],"a\u003cql":[["‣",[["d",{"type":"date","start_date":"2021-01-22"}]]]],"}nqi":[["Jay"]],"~]S\u003c":[["앞 장에서 문제를 MDP로 정의하는 방식에 대해 살펴보았다. 이제 본격적으로 가치함수와 큐함수, 벨만 기대 방정식과 벨만 최적 방정식에 대해 톺아보자. "]],"title":[["\u003c파이썬과 케라스로 배우는 강화학습\u003e - (3) "],["가치함수와 벨만방정식",[["b"]]]]},"content":["7caccd52-51ba-4591-ae1f-6f47e82c07a5","3a27b945-debc-4971-918c-529d7c869ee2","27fa4255-a058-4c79-9e8d-36321d739557","b60dff29-17a0-4513-b652-c49f2df5eb67","92ee542b-609f-4ad0-b91a-b3da6adc1296","261e0e72-82cd-4341-8d4b-2f714d68d988","996a4e3c-3f48-4d3b-b9d6-3cb3621ac969","f2c93629-77bd-4891-8dac-45814add6e17","a67603c1-5bec-440a-bc43-6308b596dac9","b752bd0c-6161-4559-aa38-dc156652dbca","fb10810b-fcb8-499c-b7d8-07f01806aefc","6815777e-cee9-481f-8a48-b891931d65d4","9f1d89f7-5111-46f8-bf28-cd89d514969e","4fc910a1-0ef0-47c3-a233-30bc8b5e8144","13f62577-181e-45f2-8ba6-9dff90328cc6","998e359f-a455-4c92-abc0-c53996e758af","e180fbfc-5681-4db3-8958-667cc9e483c0","41bb6529-3d83-46a4-8800-799bdfe3aa62","0757549d-c77a-491d-afef-8cc93a167e69","80ccb2ce-5c62-44ba-924b-11d513a1d0f7","4b0b4a92-f4d9-4cb6-85fb-d27cb5e3bcbe","e7f3dff4-ca8a-4352-8a2d-ba67bdc6175c","b7b6cede-c22e-4274-876d-303bf9c30a9d","30485274-d253-4b0f-a081-6f6ad4564e1e","6bce39cc-623e-4680-a3e8-1eff7dd29b2f","a13eb7e4-2975-475d-b806-39687500b2e8","62cdb445-bb88-47b0-b470-9406ecf3eb66","10ae23f4-f8ea-4efe-bc69-981e3c4c1f09","bfc4cc97-ec2f-4214-9f29-f32ab73b5ac3","6b40aabb-e781-4c02-9dd1-2211c05e3d76","5252a923-4a01-4c82-9001-8031951b9122","bdc7a4f1-7661-418c-93af-c369e752570e","80382a0f-853c-4d44-af89-368e6e6162f3","e668f502-3b55-470b-bbc7-c2c59e0c254a","294e7331-565c-49d8-9c2e-bff18e1378b1","d9681d8d-2376-47bd-a82b-415bebde7de2","bf439e4e-e190-45f8-abf7-bcc0957a9fb1","66b81c50-134e-4393-bab7-784732a1e705","ac802ea3-f658-419d-8522-94d79fe2acfb","1ca45c2d-98a5-42cf-aae2-cda6aa9a5b7d","751d7baa-4d7e-4571-9fe8-89a5d8612d31","6ec17332-5ce5-4a13-b790-24904ed656c5","d531b253-2f1d-4af1-bc6a-9b5dcbc61cf2","a66b4932-3fd8-4d1a-864b-77b3b10d3fa3","004967b4-b289-40e7-bac0-c68cf94d0c94","59068fdb-b6ed-4c20-8dd3-63f6936f3e75","70667186-25a6-4080-8a03-60ed2568fec3","aa30ebe6-80c0-4932-8fe5-41b1d8c4a479","721c84d5-d26c-4ce9-8e58-efd6cfe82752","83cbd1c0-a4d5-4af7-bdd9-14f66106eea2","c5f6ef91-a533-4b48-bc4d-f04a6e083566","626dcb65-e91f-4fa9-b74c-934875790c49","6c87999f-fd1c-4e4e-913c-c54f1e9f8829","ef0191f1-6d6d-4276-9197-d8a66c03b9f0","92a4b20a-474e-4579-9ea9-2d8cbcabf74b","4c9d1637-363f-4f84-aea9-d1b19874463f","913e2530-875d-4cd3-bcd3-fa6631fb8497","d6c5b8e5-acfa-4f94-90f2-816252fa1c7f","7daa452f-b0a9-4bb0-b824-f428e74367a8","09dbf85a-ea75-4437-bf46-831aa94d0565","9a464cb3-41a9-46a8-b8ea-ee68cd919b7a","a421c8fd-1086-451b-b2e2-6a2992953e6d","689164f5-f934-4362-8c52-973e25631227","da29e588-8c92-4505-ab19-a2c5c5b86f13","8b643dce-01f8-4efd-abb6-179f96d7582c","505cefb7-627e-4ae3-8af7-da75d76472d7","aa0fdf6d-a636-4727-b438-09d6f080c7e3","fd2a2fbe-282b-4494-9f96-2ca995ed9313","00b88cae-f2a1-4aa9-a5e0-70b8adb22df7","875dcde0-a341-4b95-a64f-28c85a73e574","91186934-e1c0-4f8b-a89e-b993a8342934","eacf08c0-a632-445c-80d5-1bb23a2f0b47","04dbd6df-c6f8-40c2-979e-09c2b4aa9e88","9423b856-e96a-484c-9f37-4dddd024a855","ec71d704-3194-4664-b9bd-dd09ff499acd","348fdd7e-f50f-4c36-8ad5-e501987f667e","f1c069f4-2290-4336-a33d-432d0cbfd0ce","9d775ffc-c0ed-447d-b2ac-ea5c13c78f67","4670c878-7f3b-4956-9eb0-08e41ba21a30","53a2ffd5-844e-404d-940b-b94bad2e0654","c93fa6c5-3982-48f1-a178-4f6c5eba839e","cd44bbfe-05b2-473c-b422-1dc02c96ecfb","2d3824c6-7a2d-4506-adaa-77c6611ccb5e","81c7d476-748b-4c23-97a3-02eb5ff64419","d653ec92-64ef-4ec1-8bda-b99192294df5","07502acf-2788-45ff-80b1-8dac6863547e","6c834085-f837-41b8-aefa-cd934aede06c","eb386ca4-154d-433b-a550-8d8fdee92646","5357aa08-c45f-4f42-a7eb-5ca390eb9729","cdfd57bd-92f3-4e8c-a00b-e69615886692","b624f66f-5127-4f0d-a133-2140a36b4169","d734ec80-b47e-4554-92e9-b312b5689892","3e283b6a-1df6-4772-ad10-d4f6dde5076a","45a5da3d-129c-4d57-8508-a3b9993977f1","39b23947-fdad-4591-8944-9b1c600a6c59","8810b4c2-3737-4ea5-a00b-b92e5953072a","e3a3f05b-fd20-4668-8b1a-c4ab369d85e1","7dcbb1c5-6694-4240-aa75-0d867c360590","417f55c0-9f2f-42a8-8309-f07303d0e406","bfd5656f-5900-455f-ba71-171c864a947e"],"format":{"page_icon":"🎮","page_cover":"https://images.unsplash.com/photo-1563209259-b2fa97148ce1?ixlib=rb-4.0.3\u0026q=80\u0026fm=jpg\u0026crop=entropy\u0026cs=tinysrgb","page_cover_position":0.5},"created_time":1676045925196,"last_edited_time":1677651622693,"parent_id":"ba8460cf-4781-486e-8976-01358ef4659d","parent_table":"collection","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"4f20ede8-ccf7-4ae1-82e5-819e100dd032":{"role":"reader","value":{"id":"4f20ede8-ccf7-4ae1-82e5-819e100dd032","version":129,"type":"collection_view","view_ids":["d91647c5-6a81-48b1-a1ff-a04529d0ddba","27bc73a3-5779-44e0-b617-2f0d26f5aa2f"],"collection_id":"ba8460cf-4781-486e-8976-01358ef4659d","format":{"collection_pointer":{"id":"ba8460cf-4781-486e-8976-01358ef4659d","table":"collection","spaceId":"4af10338-3e65-4b50-af9f-798d59d5c8f6"},"copied_from_pointer":{"id":"3e3073e9-7aee-481c-b831-765e112ec7b5","table":"block","spaceId":"fde5ac74-eea3-4527-8f00-4482710e1af3"}},"created_time":1675998770865,"last_edited_time":1677136438825,"parent_id":"6246082f-4014-4d06-98ab-59e9840b298a","parent_table":"block","alive":true,"copied_from":"3e3073e9-7aee-481c-b831-765e112ec7b5","created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"6246082f-4014-4d06-98ab-59e9840b298a":{"role":"reader","value":{"id":"6246082f-4014-4d06-98ab-59e9840b298a","version":609,"type":"page","properties":{"title":[["Jang. Inspiration"]]},"content":["651dcdf3-689d-42d2-8497-64f8509d3504","0bd6df0e-6679-499e-b382-c9dc2c597776","f4c89bb7-a90e-41d7-a678-588b3deff765","d5557a1e-de5e-4085-896d-362a19928b69","4f20ede8-ccf7-4ae1-82e5-819e100dd032","ce518f27-4e46-4e98-ac75-13c467c1370c","dfee9c57-3be6-41db-8113-a54eeef675a7","1109cf3f-d8f9-4532-a13e-1af177ad4fdd","5855fe5e-17e2-4f14-8b66-702838bcc734"],"format":{"page_icon":"https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b29e9b03-c79c-4e52-a45e-7228163ba524/compass-circular-tool_(3).png","page_cover":"/images/page-cover/nasa_reduced_gravity_walking_simulator.jpg","copied_from_pointer":{"id":"78754261-97cf-4616-9880-9def95960ebf","table":"block","spaceId":"fde5ac74-eea3-4527-8f00-4482710e1af3"},"page_cover_position":0.5},"permissions":[{"role":"editor","type":"user_permission","user_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb"},{"role":"reader","type":"public_permission","added_timestamp":1675998834095}],"created_time":1675998770872,"last_edited_time":1677392850666,"parent_id":"3a2d56c9-1da9-4a62-b698-f0ad3576c8c1","parent_table":"block","alive":true,"copied_from":"78754261-97cf-4616-9880-9def95960ebf","file_ids":["f70d3dc6-ce97-4be2-9cde-b86606147b41","a2bd3317-78e4-48bc-8d27-9b733175a416","7fae9664-8795-4723-844e-0adecdea62dc","4235c094-2110-4aa6-b058-6b5fe220dbb7","c8194a03-81d0-482d-a7de-f491a6e85f54","7eb95609-c81b-48c1-969e-5ef2f220bc5a","160057d8-120e-4f9f-8c1f-6bcf31a50f15","0cb9278b-708b-4da1-929a-6696aa8cdfa3","3eb9471e-9b71-4bd3-a13d-c33158a442be","b29e9b03-c79c-4e52-a45e-7228163ba524"],"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"7caccd52-51ba-4591-ae1f-6f47e82c07a5":{"role":"reader","value":{"id":"7caccd52-51ba-4591-ae1f-6f47e82c07a5","version":13,"type":"quote","properties":{"title":[["본 포스트는 “파이썬과 케라스로 배우는 강화학습” 도서의 세번째 리뷰 포스트입니다."]]},"created_time":1676046122559,"last_edited_time":1676055502631,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"3a27b945-debc-4971-918c-529d7c869ee2":{"role":"reader","value":{"id":"3a27b945-debc-4971-918c-529d7c869ee2","version":7,"type":"text","properties":{"title":[["http://www.yes24.com/Product/Goods/44136413",[["a","http://www.yes24.com/Product/Goods/44136413"]]]]},"created_time":1676055503341,"last_edited_time":1676055503771,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"27fa4255-a058-4c79-9e8d-36321d739557":{"role":"reader","value":{"id":"27fa4255-a058-4c79-9e8d-36321d739557","version":6,"type":"text","created_time":1676049237236,"last_edited_time":1676049239447,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"b60dff29-17a0-4513-b652-c49f2df5eb67":{"role":"reader","value":{"id":"b60dff29-17a0-4513-b652-c49f2df5eb67","version":4,"type":"table_of_contents","format":{"block_color":"gray"},"created_time":1676049244867,"last_edited_time":1676049244868,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"92ee542b-609f-4ad0-b91a-b3da6adc1296":{"role":"reader","value":{"id":"92ee542b-609f-4ad0-b91a-b3da6adc1296","version":6,"type":"text","created_time":1676049237469,"last_edited_time":1676049237469,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"261e0e72-82cd-4341-8d4b-2f714d68d988":{"role":"reader","value":{"id":"261e0e72-82cd-4341-8d4b-2f714d68d988","version":4,"type":"sub_header","properties":{"title":[["2장 강화학습 기초 1: 가치함수와 벨만 방정식",[["b"]]]]},"created_time":1676046122559,"last_edited_time":1676049237469,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"996a4e3c-3f48-4d3b-b9d6-3cb3621ac969":{"role":"reader","value":{"id":"996a4e3c-3f48-4d3b-b9d6-3cb3621ac969","version":2,"type":"divider","created_time":1676046122559,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"f2c93629-77bd-4891-8dac-45814add6e17":{"role":"reader","value":{"id":"f2c93629-77bd-4891-8dac-45814add6e17","version":2,"type":"text","properties":{"title":[["이제 에이전트가 학습할 수 있도록 문제를 "],["MDP",[["c"]]],["로 정의 했다. 에이전트는 "],["MDP",[["c"]]],["를 통해 "],["최적 정책",[["b"]]],[" 을 찾으면 된다. 이제 에이전트가 어떻게 최적 정책을 찾을 수 있을지 보다 구체적으로 알아보도록 하자!"],["(❗️수식 주의)",[["b"]]]]},"created_time":1676046122559,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"a67603c1-5bec-440a-bc43-6308b596dac9":{"role":"reader","value":{"id":"a67603c1-5bec-440a-bc43-6308b596dac9","version":5,"type":"text","created_time":1676046138268,"last_edited_time":1676046138273,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"b752bd0c-6161-4559-aa38-dc156652dbca":{"role":"reader","value":{"id":"b752bd0c-6161-4559-aa38-dc156652dbca","version":2,"type":"sub_sub_header","properties":{"title":[["가치함수",[["b"]]]]},"created_time":1676046122559,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"fb10810b-fcb8-499c-b7d8-07f01806aefc":{"role":"reader","value":{"id":"fb10810b-fcb8-499c-b7d8-07f01806aefc","version":2,"type":"text","properties":{"title":[["에이전트의 입장에서 어떤 행동을 하는 것이 좋은지는 어떻게 알 수 있을까? 앞으로 받을 보상들을 고려해서 선택해야 좋은 선택을 할텐데, 아직 받지 않는 보상들을 어떻게 고려한단 말일까?🧐 "],["앞으로 받을 보상에 관련된 개념",[["b"]]],[" 이 바로 "],["가치 함수",[["c"]]],["이다."]]},"created_time":1676046122559,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"6815777e-cee9-481f-8a48-b891931d65d4":{"role":"reader","value":{"id":"6815777e-cee9-481f-8a48-b891931d65d4","version":25,"type":"text","properties":{"title":[["현재 시간 t로부터 에이전트가 행동을 하면서 받을 보상들을 단순히 합한다는 생각을 해볼 수 있다. 그렇게 되면 "],["⁍",[["e","R_{t+1}+R_{t+2}+R_{t+3}+R_{t+4}+R_{t+5}+…"]]],["와 같은 꼴일 것이다. 그런데 시간에 따른 보상을 이렇게 단순하게 더한다면 세가지 문제가 생긴다."]]},"created_time":1676046122560,"last_edited_time":1676046383712,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"9f1d89f7-5111-46f8-bf28-cd89d514969e":{"role":"reader","value":{"id":"9f1d89f7-5111-46f8-bf28-cd89d514969e","version":2,"type":"numbered_list","properties":{"title":[["지금 받은 보상이나 미래에 받는 보상이나 똑같이 취급한다.",[["b"]]]]},"created_time":1676046122560,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"4fc910a1-0ef0-47c3-a233-30bc8b5e8144":{"role":"reader","value":{"id":"4fc910a1-0ef0-47c3-a233-30bc8b5e8144","version":2,"type":"numbered_list","properties":{"title":[["100이라는 보상을 1번 받는 것과 20이라는 보상을 5번 받는 것을 구분할 방법이 없다.",[["b"]]]]},"created_time":1676046122560,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"13f62577-181e-45f2-8ba6-9dff90328cc6":{"role":"reader","value":{"id":"13f62577-181e-45f2-8ba6-9dff90328cc6","version":2,"type":"numbered_list","properties":{"title":[["시간이 무한대라고 하면 보상을 시간마다 0.1씩 받아도 합이 무한대이고 1씩 받아도 합이 무한대이다.",[["b"]]]]},"created_time":1676046122560,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"998e359f-a455-4c92-abc0-c53996e758af":{"role":"reader","value":{"id":"998e359f-a455-4c92-abc0-c53996e758af","version":11,"type":"text","properties":{"title":[["이러한 문제 때문에 에이전트는 단순한 보상의 합으로는 "],["시간 t에 있었던 상태가 어떤 가치를 가지는지",[["b"]]],[" 판단하기가 어렵다. 따라서 좀 더 정확하게 상태의 가치를 판단하기 위해 이전 포스트에서 설명한 "],["할인율",[["c"]]],["이라는 개념을 사용한다. 할인율을 적용하여 시간 t 이후의 "],["시간 t시점에서의 보상",[["b"]]],[" 을 모두 더한 것을 "],["반환값",[["c"]]],["이라고 하며 "],["⁍",[["e","G_t"]]],["로 표현한다. 반환값을 수식으로 표현하면 아래와 같다."]]},"created_time":1676046122560,"last_edited_time":1676046394492,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"e180fbfc-5681-4db3-8958-667cc9e483c0":{"role":"reader","value":{"id":"e180fbfc-5681-4db3-8958-667cc9e483c0","version":10,"type":"equation","properties":{"title":[["G_t = R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+\\gamma^3 R_{t+4}+\\gamma^4 R_{t+5}…"]]},"created_time":1676046122560,"last_edited_time":1676046410386,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"41bb6529-3d83-46a4-8800-799bdfe3aa62":{"role":"reader","value":{"id":"41bb6529-3d83-46a4-8800-799bdfe3aa62","version":2,"type":"text","properties":{"title":[["반환값이라는 것은 에이전트가 실제로 환경을 탐험하며 받은 보상의 합으로, 이 책에서는 에이전트와 환경이 "],["유한한 시간동안 상호작용",[["b"]]],[" 하는 경우만 다룬다. 이렇게 "],["유한한 에이전트와 환경의 상호작용",[["b"]]],[" 을 "],["에피소드",[["c"]]],["라고 부른다. 에피소드 에서는 에피소드를 끝낼 수 있는 마지막 상태가 있는데, 체스의 경우를 생각한다면 킹을 잃는 순간이 마지막 상태가 된다."]]},"created_time":1676046122561,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"0757549d-c77a-491d-afef-8cc93a167e69":{"role":"reader","value":{"id":"0757549d-c77a-491d-afef-8cc93a167e69","version":2,"type":"text","properties":{"title":[["에이전트가 에피소드가 끝난 후에 "],["‘그때로부터 얼마의 보상을 받았지?’",[["b"]]],[" 라며 보상을 정산하는 것이 반환값이다. 만일 에피소드를 t=1 부터 5까지 진행했다면 에피소드가 끝난 후에 방문했던 상태들에 대한 5개의 반환값이 생길 것이다."]]},"created_time":1676046122561,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"80ccb2ce-5c62-44ba-924b-11d513a1d0f7":{"role":"reader","value":{"id":"80ccb2ce-5c62-44ba-924b-11d513a1d0f7","version":12,"type":"equation","properties":{"title":[["G_1 = R_{2}+\\gamma R_{3}+\\gamma^2 R_{4}+\\gamma^3 R_{5}+\\gamma^4 R_{6}\\\\\\\\\nG_2 = R_{3}+\\gamma R_{4}+\\gamma^2 R_{5}+\\gamma^3 R_{6}\\\\\\\\\nG_3 = R_{4}+\\gamma R_{5}+\\gamma^2 R_{6}\\\\\\\\\nG_4 = R_{5}+\\gamma R_{6}\\\\\\\\\nG_5 = R_{6}"]],"language":[["Markdown"]]},"created_time":1676046122562,"last_edited_time":1676046427972,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"4b0b4a92-f4d9-4cb6-85fb-d27cb5e3bcbe":{"role":"reader","value":{"id":"4b0b4a92-f4d9-4cb6-85fb-d27cb5e3bcbe","version":2,"type":"text","properties":{"title":[["MDP로 정의 되는 세계에서 반환값은 에피소드마다 다를 수 있다. 때문에 에이전트는 특정 상태의 가치를 "],["반환값에 대한 기댓값",[["b"]]],[" 으로 판단해야 한다. 이것이 바로 "],["가치함수",[["c"]]],["의 개념이다. 따라서 가치함수는 아래와 같이 표현된다."]]},"created_time":1676046122562,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"e7f3dff4-ca8a-4352-8a2d-ba67bdc6175c":{"role":"reader","value":{"id":"e7f3dff4-ca8a-4352-8a2d-ba67bdc6175c","version":7,"type":"equation","properties":{"title":[["v(s) = E[G_t|S_t=s]"]]},"created_time":1676046122562,"last_edited_time":1676046442672,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"b7b6cede-c22e-4274-876d-303bf9c30a9d":{"role":"reader","value":{"id":"b7b6cede-c22e-4274-876d-303bf9c30a9d","version":2,"type":"text","properties":{"title":[["각 타임스텝마다 받는 보상이 모두 확률적이고 반환값이 그 보상들의 합이므로 반환값은 "],["확률변수",[["b"]]],[" 이다. 하지만 가치함수는 확률변수가 아니라 특정 양을 나타내는 값이므로 "],["소문자",[["b"]]],["로 표현한다. 가치함수의 식에 위에서 정의한 반환값의 수식을 대입하고 정리해보면 아래와 같다."]]},"created_time":1676046122562,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"30485274-d253-4b0f-a081-6f6ad4564e1e":{"role":"reader","value":{"id":"30485274-d253-4b0f-a081-6f6ad4564e1e","version":12,"type":"equation","properties":{"title":[["v(s) = E[R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}...|S_t=s]\\\\\\\\\n = E[R_{t+1}+\\gamma(R_{t+2}+\\gamma R_{t+3}...)|S_t=s]\\\\\\\\\n = E[R_{t+1}+\\gamma G_{t+1}|S_t=s]"]],"language":[["Markdown"]]},"created_time":1676046122563,"last_edited_time":1676046459005,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"6bce39cc-623e-4680-a3e8-1eff7dd29b2f":{"role":"reader","value":{"id":"6bce39cc-623e-4680-a3e8-1eff7dd29b2f","version":11,"type":"text","properties":{"title":[["사실 "],["⁍",[["e","R_{t+2}+\\gamma R_{t+3} … "]]],["부분을 반환값의 형태로 표현하긴 했지만 사실 에이전트가 실제로 받은 보상이 아니다. 이 보상은 "],["앞으로 받을 것이라 예상하는 보상",[["b"]]],[" 으로써 이 부분 또한 가치함수로 표현할 수 있다."]]},"created_time":1676046122564,"last_edited_time":1676046486898,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"a13eb7e4-2975-475d-b806-39687500b2e8":{"role":"reader","value":{"id":"a13eb7e4-2975-475d-b806-39687500b2e8","version":7,"type":"equation","properties":{"title":[["v(s) = E[R_{t+1}+\\gamma v(S_{t+1})|S_t=s]"]]},"created_time":1676046122564,"last_edited_time":1676046471672,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"62cdb445-bb88-47b0-b470-9406ecf3eb66":{"role":"reader","value":{"id":"62cdb445-bb88-47b0-b470-9406ecf3eb66","version":2,"type":"text","properties":{"title":[["여기까지는 가치함수를 정의할 때 정책을 고려하지 않았다. 하지만 에이전트가 앞으로 받을 보상에 대해 생각할 때 정책을 고려하지 않으면 안된다. 왜냐하면 상태에서 상태로 넘어갈 때 에이전트는 무조건 행동을 해야 하고 "],["각 상태에서의 행동을 결정하는 것",[["b"]]],[" 이 에이전트의 정책이기 때문이다."]]},"created_time":1676046122564,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"10ae23f4-f8ea-4efe-bc69-981e3c4c1f09":{"role":"reader","value":{"id":"10ae23f4-f8ea-4efe-bc69-981e3c4c1f09","version":2,"type":"text","properties":{"title":[["보상은 어떤 상태에서 어떤 행동을 하는지에 따라 환경에서 에이전트에게 주어진다. 이말은 곧 "],["MDP",[["c"]]],["로 정의 되는 문제에서의 가치함수가 행동을 결정하는 "],["정책",[["c"]]],["에 의존한다는 것이다. 따라서 아래의 수식처럼 "],["가치함수에 아래 첨자로 정책을 쓰면",[["b"]]],[" 더 명확한 수식이 된다."]]},"created_time":1676046122564,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"bfc4cc97-ec2f-4214-9f29-f32ab73b5ac3":{"role":"reader","value":{"id":"bfc4cc97-ec2f-4214-9f29-f32ab73b5ac3","version":7,"type":"equation","properties":{"title":[["v_\\pi (s) = E_\\pi [R_{t+1}+ \\gamma v_\\pi (S_{t+1})|S_t = s]"]]},"created_time":1676046122565,"last_edited_time":1676046506573,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"6b40aabb-e781-4c02-9dd1-2211c05e3d76":{"role":"reader","value":{"id":"6b40aabb-e781-4c02-9dd1-2211c05e3d76","version":14,"type":"text","properties":{"title":[["이 수식이 바로 강화학습에서 상당히 중요한 "],["벨만 기대 방정식(Bellman Expectation Equation)",[["c"]]],["이다.벨만 기대 방정식은 "],["현재 상태의 가치함수 ",[["b"]]],["⁍",[["e","v_\\pi (s)"],["b"]]],["와 다음상태의 가치함수 ",[["b"]]],["⁍",[["e","v_\\pi (S_{t+1})"],["b"]]],["사이의 관계",[["b"]]],[" 를 말해주는 방정식이다."]]},"created_time":1676046122565,"last_edited_time":1676046554452,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"5252a923-4a01-4c82-9001-8031951b9122":{"role":"reader","value":{"id":"5252a923-4a01-4c82-9001-8031951b9122","version":2,"type":"quote","properties":{"title":[["강화학습은 벨만 방정식을 어떻게 풀어가느냐의 스토리이다 🤧"]]},"created_time":1676046122565,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"bdc7a4f1-7661-418c-93af-c369e752570e":{"role":"reader","value":{"id":"bdc7a4f1-7661-418c-93af-c369e752570e","version":5,"type":"text","created_time":1676046575612,"last_edited_time":1676046575617,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"80382a0f-853c-4d44-af89-368e6e6162f3":{"role":"reader","value":{"id":"80382a0f-853c-4d44-af89-368e6e6162f3","version":2,"type":"sub_sub_header","properties":{"title":[["큐함수",[["b"]]]]},"created_time":1676046122565,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"e668f502-3b55-470b-bbc7-c2c59e0c254a":{"role":"reader","value":{"id":"e668f502-3b55-470b-bbc7-c2c59e0c254a","version":2,"type":"text","properties":{"title":[["가치함수는 말 그대로 "],["“함수”",[["b"]]],[" 이다. 따라서 입력이 무엇이고 출력이 무엇인지 알 필요가 있다. 지금까지 설명한 가치함수는 "],["상태 가치함수(state value-function)",[["c"]]],["으로써 "],["상태",[["b"]]],[" 가 입력으로 들어오면 그 상태에서 "],["앞으로 받을 보상의 합",[["b"]]],[" 을 출력하는 함수이다. 따라서 에이전트는 가치함수를 통해 어떤 상태에 있는 것이 얼마나 좋은지를 알 수 있다."]]},"created_time":1676046122565,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"294e7331-565c-49d8-9c2e-bff18e1378b1":{"role":"reader","value":{"id":"294e7331-565c-49d8-9c2e-bff18e1378b1","version":2,"type":"text","properties":{"title":[["상태 가치함수가 각 상태에 대해 가치를 알려주는 것처럼 각 행동에 대해 가치를 알려주는 함수가 있다면 어떨까? 아마 에이전트는 그 함수의 값만 보고 바로 행동을 선택할 수 있을 것이다. "],["어떤 상태에서 어떤 행동이 얼마나 좋은지",[["b"]]],[" 알려주는 함수를 "],["큐함수(Q Function)",[["c"]]],[", 다른말로 행동 가치함수 라고 한다."]]},"created_time":1676046122565,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"d9681d8d-2376-47bd-a82b-415bebde7de2":{"role":"reader","value":{"id":"d9681d8d-2376-47bd-a82b-415bebde7de2","version":8,"type":"image","properties":{"source":[["https://longshiine.github.io/2021/01/22/Reinforcement-Learning-Basic-2/qfunction.jpeg"]],"caption":[["큐함수"]]},"format":{"block_width":240,"block_full_width":false,"block_page_width":false},"created_time":1676046122565,"last_edited_time":1676046585383,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"bf439e4e-e190-45f8-abf7-bcc0957a9fb1":{"role":"reader","value":{"id":"bf439e4e-e190-45f8-abf7-bcc0957a9fb1","version":8,"type":"text","properties":{"title":[["큐함수는 상태, 행동이라는 두가지 변수를 가지며 "],["⁍",[["e","q_\\pi (s,a)"]]],["라고 나타낸다. 또한 가치함수와 큐함수 사이의 관계식은 다음과 같이 표현할 수 있다."]]},"created_time":1676046122572,"last_edited_time":1676046607318,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"66b81c50-134e-4393-bab7-784732a1e705":{"role":"reader","value":{"id":"66b81c50-134e-4393-bab7-784732a1e705","version":7,"type":"equation","properties":{"title":[["v_\\pi (s) = \\sum_{a \\in A} \\pi (a|s) q_\\pi (s,a)"]]},"created_time":1676046122572,"last_edited_time":1676046618776,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"ac802ea3-f658-419d-8522-94d79fe2acfb":{"role":"reader","value":{"id":"ac802ea3-f658-419d-8522-94d79fe2acfb","version":22,"type":"numbered_list","properties":{"title":[["각 행동을 했을 때 앞으로 받을 보상인 큐함수 "],["⁍",[["e","q_\\pi (s,a)"]]],["를 정책 "],["⁍",[["e","\\pi (a|s)"]]],["에 곱한다"]]},"created_time":1676046122572,"last_edited_time":1676046644291,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"1ca45c2d-98a5-42cf-aae2-cda6aa9a5b7d":{"role":"reader","value":{"id":"1ca45c2d-98a5-42cf-aae2-cda6aa9a5b7d","version":9,"type":"numbered_list","properties":{"title":[["모든 행동에 대해 큐함수와 "],["⁍",[["e","π(a|s)"]]],["를 곱한 값을 더하면 가치함수가 된다."]]},"created_time":1676046122572,"last_edited_time":1676046652065,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"751d7baa-4d7e-4571-9fe8-89a5d8612d31":{"role":"reader","value":{"id":"751d7baa-4d7e-4571-9fe8-89a5d8612d31","version":2,"type":"text","properties":{"title":[["큐함수는 강화학습에서 중요한 역할을 한다. "],["강화학습에서 에이전트가 행동을 선택하는 기준",[["b"]]],[" 으로 가치 함수 보다는 보통 "],["큐함수",[["b"]]],[" 를 사용한다. 그 이유는 뒤에서 등장한다! 큐함수 또한 벨만 기대 방정식의 형태로 나타 낼수 있으며 조건문에 행동이 들어간다는 점에서 가치함수의 식과 다르다."]]},"created_time":1676046122573,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"6ec17332-5ce5-4a13-b790-24904ed656c5":{"role":"reader","value":{"id":"6ec17332-5ce5-4a13-b790-24904ed656c5","version":8,"type":"text","properties":{"title":[["⁍",[["e","q_\\pi (s,a) = E_\\pi [R_{t+1}+ \\gamma q_\\pi (S_{t+1}, A_{t+1})|S_t = s, A_t=a]"]]]]},"created_time":1676046122573,"last_edited_time":1676046663963,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"d531b253-2f1d-4af1-bc6a-9b5dcbc61cf2":{"role":"reader","value":{"id":"d531b253-2f1d-4af1-bc6a-9b5dcbc61cf2","version":5,"type":"text","created_time":1676046665693,"last_edited_time":1676046665697,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"a66b4932-3fd8-4d1a-864b-77b3b10d3fa3":{"role":"reader","value":{"id":"a66b4932-3fd8-4d1a-864b-77b3b10d3fa3","version":2,"type":"sub_sub_header","properties":{"title":[["벨만 기대 방정식",[["b"]]]]},"created_time":1676046122573,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"004967b4-b289-40e7-bac0-c68cf94d0c94":{"role":"reader","value":{"id":"004967b4-b289-40e7-bac0-c68cf94d0c94","version":2,"type":"text","properties":{"title":[["이제 2장의 메인 디쉬인 "],["벨만 기대 방정식",[["c"]]],["을 찬찬히 살펴보자(👂🏻집중). 살짝 정리를 해보면 벨만 "],["기대",[["b"]]],[" 방정식이라고 하는 이유는 식에 "],["기댓값의 개념",[["b"]]],[" 이 들어가기 때문이고, 이 벨만 방정식은 "],["현재 상태의 가치함수와 다음 상태의 가치함수 사이의 관계",[["b"]]],[" 를 식으로 나타낸 것이었다."]]},"created_time":1676046122573,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"59068fdb-b6ed-4c20-8dd3-63f6936f3e75":{"role":"reader","value":{"id":"59068fdb-b6ed-4c20-8dd3-63f6936f3e75","version":2,"type":"text","properties":{"title":[["벨만 방정식은 강화학습에서 상당히 중요한 부분을 차지한다. 벨만 방정식이 강화학습에서 왜 그렇게 중요한 위치를 차지하고 있는 걸까?🤔 앞에서 정의했던 가치함수의 정의를 다시 한번 살펴보자."]]},"created_time":1676046122573,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"70667186-25a6-4080-8a03-60ed2568fec3":{"role":"reader","value":{"id":"70667186-25a6-4080-8a03-60ed2568fec3","version":11,"type":"equation","properties":{"title":[["⁍",[["e","v_\\pi (s) = E[R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}...|S_t=s]"]]]]},"created_time":1676046122574,"last_edited_time":1676046680310,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"aa30ebe6-80c0-4932-8fe5-41b1d8c4a479":{"role":"reader","value":{"id":"aa30ebe6-80c0-4932-8fe5-41b1d8c4a479","version":2,"type":"text","properties":{"title":[["이 수식으로 부터 기댓값을 알아내려면 앞으로 받을 보상에 대해 고려해야하고, 이는 정의상으로는 가능하지만 "],["상태가 많아질수록 상당히 비효율적인 방법",[["b"]]],[" 이다. 따라서 컴퓨터가 이 기댓값을 계산하기 위해 다른 조치가 필요하다."]]},"created_time":1676046122574,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"721c84d5-d26c-4ce9-8e58-efd6cfe82752":{"role":"reader","value":{"id":"721c84d5-d26c-4ce9-8e58-efd6cfe82752","version":2,"type":"text","properties":{"title":[["예를 들어 1을 100번 더해야하는 문제가 있다고 해보자. 식 하나로 풀어내는 방법은 아래와 같다"]]},"created_time":1676046122574,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"83cbd1c0-a4d5-4af7-bdd9-14f66106eea2":{"role":"reader","value":{"id":"83cbd1c0-a4d5-4af7-bdd9-14f66106eea2","version":33,"type":"equation","properties":{"title":[["1+1+1+…+1=100"]]},"created_time":1676046122574,"last_edited_time":1676046704869,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"c5f6ef91-a533-4b48-bc4d-f04a6e083566":{"role":"reader","value":{"id":"c5f6ef91-a533-4b48-bc4d-f04a6e083566","version":5,"type":"text","properties":{"title":[["하지만 다른 방법으로 접근해볼 수도 있다. x라는 변수를 지정해 그 값에 1을 계속 더해나가는 것이다."]]},"created_time":1676046691543,"last_edited_time":1676046691550,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"626dcb65-e91f-4fa9-b74c-934875790c49":{"role":"reader","value":{"id":"626dcb65-e91f-4fa9-b74c-934875790c49","version":21,"type":"code","properties":{"title":[["X = 0\nfor i in range(100):\n  X = X + 1"]],"language":[["Python"]]},"created_time":1676046724835,"last_edited_time":1676046736538,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"6c87999f-fd1c-4e4e-913c-c54f1e9f8829":{"role":"reader","value":{"id":"6c87999f-fd1c-4e4e-913c-c54f1e9f8829","version":2,"type":"text","properties":{"title":[["벨만 방정식으로 가치함수를 계산하는 것은 두 번째 방식과 같은 것이다. 한 번에 모든 것을 계산하는 것이 아니라 값을 변수에 저장하고, 루프를 도는 계산을 통해 참 값을 알아나가는 것이다. 즉, "],["현재 가치함수 값을 업데이트",[["b"]]],[" 해 나가는 것이다. 하지만 업데이트 하려면 기댓값을 계산해야 하는데 기댓값은 어떻게 계산할 수 있을까?"]]},"created_time":1676046122575,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"ef0191f1-6d6d-4276-9197-d8a66c03b9f0":{"role":"reader","value":{"id":"ef0191f1-6d6d-4276-9197-d8a66c03b9f0","version":14,"type":"text","properties":{"title":[["기댓값에는 어떠한 행동을 할 확률("],["정책 ",[["b"]]],["⁍",[["e","\\pi (a|s)"],["b"]]],[")과 그 행동을 했을 때 어떤 상태로 가게 되는 확률("],["상태 변환 확률 ",[["b"]]],["⁍",[["e","P_{ss'}^a"],["b"]]],[")이 포함되어 있다. 따라서 정책과 상태 변환 확률을 포함해서 계산하면 된다."]]},"created_time":1676046122576,"last_edited_time":1676046770342,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"92a4b20a-474e-4579-9ea9-2d8cbcabf74b":{"role":"reader","value":{"id":"92a4b20a-474e-4579-9ea9-2d8cbcabf74b","version":7,"type":"equation","properties":{"title":[["v_\\pi (s) = \\sum_{a \\in A} \\pi (a|s) (r_{(s,a)} + \\gamma \\sum_{s' \\in S} P_{ss'}^a v_\\pi (s') )"]]},"created_time":1676046122576,"last_edited_time":1676046785044,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"4c9d1637-363f-4f84-aea9-d1b19874463f":{"role":"reader","value":{"id":"4c9d1637-363f-4f84-aea9-d1b19874463f","version":2,"type":"text","properties":{"title":[["예시를 한번 살펴보는 것이 매우 도움이 될 것이다."]]},"created_time":1676046122576,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"913e2530-875d-4cd3-bcd3-fa6631fb8497":{"role":"reader","value":{"id":"913e2530-875d-4cd3-bcd3-fa6631fb8497","version":4,"type":"image","properties":{"source":[["https://longshiine.github.io/2021/01/22/Reinforcement-Learning-Basic-2/bellman.png"]],"caption":[["벨만 기대 방정식의 예시"]]},"created_time":1676046122576,"last_edited_time":1676046791013,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"d6c5b8e5-acfa-4f94-90f2-816252fa1c7f":{"role":"reader","value":{"id":"d6c5b8e5-acfa-4f94-90f2-816252fa1c7f","version":9,"type":"text","created_time":1676046791448,"last_edited_time":1676046850869,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"7daa452f-b0a9-4bb0-b824-f428e74367a8":{"role":"reader","value":{"id":"7daa452f-b0a9-4bb0-b824-f428e74367a8","version":4,"type":"text","properties":{"title":[["설명을 용이하게 하기 위해,"]]},"created_time":1676046791448,"last_edited_time":1676046850869,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"09dbf85a-ea75-4437-bf46-831aa94d0565":{"role":"reader","value":{"id":"09dbf85a-ea75-4437-bf46-831aa94d0565","version":12,"type":"numbered_list","properties":{"title":[["상태 변환 확률",[["c"]]],["("],["⁍",[["e","P_{ss'}^a"]]],[")은 모두 1이라고 생각하고(왼쪽으로 가기로 결정했다면 1의 확률로 왼쪽으로 간다고 하자)"]]},"created_time":1676046122576,"last_edited_time":1676046820613,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"9a464cb3-41a9-46a8-b8ea-ee68cd919b7a":{"role":"reader","value":{"id":"9a464cb3-41a9-46a8-b8ea-ee68cd919b7a","version":9,"type":"numbered_list","properties":{"title":[["정책",[["c"]]],["("],["⁍",[["e","π(a|s)"]]],[")은 무작위로 행동하는 것으로서 각 행동이 25%의 확률로 선택이 되며,"]]},"created_time":1676046122577,"last_edited_time":1676046824611,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"a421c8fd-1086-451b-b2e2-6a2992953e6d":{"role":"reader","value":{"id":"a421c8fd-1086-451b-b2e2-6a2992953e6d","version":9,"type":"numbered_list","properties":{"title":[["할인율",[["c"]]],["("],["⁍",[["e","γ"]]],[")은 0.9라고 생각하고,"]]},"created_time":1676046122577,"last_edited_time":1676046830644,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"689164f5-f934-4362-8c52-973e25631227":{"role":"reader","value":{"id":"689164f5-f934-4362-8c52-973e25631227","version":9,"type":"numbered_list","properties":{"title":[["⁍",[["e","s′"]]],["은 모든 상태일 수 있으나, 에이전트의 행동으로 도착하게 될 상태라고 해보자."]]},"created_time":1676046122577,"last_edited_time":1676046836161,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"da29e588-8c92-4505-ab19-a2c5c5b86f13":{"role":"reader","value":{"id":"da29e588-8c92-4505-ab19-a2c5c5b86f13","version":9,"type":"numbered_list","properties":{"title":[["회색 별은 "],["⁍",[["e","r(s,a)"]]],["에 해당하는 보상 값이다."]]},"created_time":1676046122577,"last_edited_time":1676046852187,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"8b643dce-01f8-4efd-abb6-179f96d7582c":{"role":"reader","value":{"id":"8b643dce-01f8-4efd-abb6-179f96d7582c","version":2,"type":"text","properties":{"title":[["위의 가정을 적용하여 식을 바꾸어 본다면,"]]},"created_time":1676046122577,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"505cefb7-627e-4ae3-8af7-da75d76472d7":{"role":"reader","value":{"id":"505cefb7-627e-4ae3-8af7-da75d76472d7","version":14,"type":"equation","properties":{"title":[["v*\\pi (s) = \\sum*{a \\in A} 0.25 (r*{(s,a)} + (0.9)v*\\pi (s') )"]]},"created_time":1676046122577,"last_edited_time":1676046865480,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"aa0fdf6d-a636-4727-b438-09d6f080c7e3":{"role":"reader","value":{"id":"aa0fdf6d-a636-4727-b438-09d6f080c7e3","version":2,"type":"text","properties":{"title":[["와 같이 바꿀 수 있고, 위의 표와 같이 계산을 할 수 있게 된다."]]},"created_time":1676046122577,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"fd2a2fbe-282b-4494-9f96-2ca995ed9313":{"role":"reader","value":{"id":"fd2a2fbe-282b-4494-9f96-2ca995ed9313","version":5,"type":"text","created_time":1676046871397,"last_edited_time":1676046871399,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"00b88cae-f2a1-4aa9-a5e0-70b8adb22df7":{"role":"reader","value":{"id":"00b88cae-f2a1-4aa9-a5e0-70b8adb22df7","version":2,"type":"quote","properties":{"title":[["벨만 기대 방정식을 이용해 현재의 가치함수를 계속 업데이트 하다 보면,에이전트가 얻을 실제 보상에 대한 참 기댓값을 얻을 수 있다."]]},"created_time":1676046122577,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"875dcde0-a341-4b95-a64f-28c85a73e574":{"role":"reader","value":{"id":"875dcde0-a341-4b95-a64f-28c85a73e574","version":5,"type":"text","created_time":1676046868993,"last_edited_time":1676046868997,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"91186934-e1c0-4f8b-a89e-b993a8342934":{"role":"reader","value":{"id":"91186934-e1c0-4f8b-a89e-b993a8342934","version":2,"type":"sub_sub_header","properties":{"title":[["벨만 최적 방정식",[["b"]]]]},"created_time":1676046122577,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"eacf08c0-a632-445c-80d5-1bb23a2f0b47":{"role":"reader","value":{"id":"eacf08c0-a632-445c-80d5-1bb23a2f0b47","version":16,"type":"text","properties":{"title":[["처음의 가치함수의 값들은 의미가 없는 값으로 초기화 된다. 초기값으로 부터 시작해서 "],["벨만 기대 방정식",[["c"]]],["으로 반복적으로 계산한다고 가정해보자. 이 계산을 반복하다보면 방정식의 우항과 좌항이 같아진다(무한히 반복한다는 가정하에). 즉, "],["⁍",[["e","v_\\pi (s)"]]],["값이 수렴하는 것이다. 그렇다면 현재의 정책 "],["⁍",[["e","π"]]],["에 대한 "],["참 가치함수",[["b"]]],[" 를 구한것이다."]]},"created_time":1676046122577,"last_edited_time":1676046898246,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"04dbd6df-c6f8-40c2-979e-09c2b4aa9e88":{"role":"reader","value":{"id":"04dbd6df-c6f8-40c2-979e-09c2b4aa9e88","version":8,"type":"equation","properties":{"title":[["⁍",[["e","v_{k+1}(s) \\leftarrow \\sum_{a \\in A} \\pi (a|s)(r_{(s,a)}+ \\gamma v_k(s'))"]]]]},"created_time":1676046122577,"last_edited_time":1676046913215,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"9423b856-e96a-484c-9f37-4dddd024a855":{"role":"reader","value":{"id":"9423b856-e96a-484c-9f37-4dddd024a855","version":2,"type":"text","properties":{"title":[["벨만 기대 방정식을 기댓값을 계산하기 위해 살짝 변형하면, 현재 정책에 대한 "],["참 가치함수",[["b"]]],[" 를 구할 수 있다. 위의 식은 뒤에서 배울 dynamic programming에서 자세히 다루도록 한다."]]},"created_time":1676046122578,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"ec71d704-3194-4664-b9bd-dd09ff499acd":{"role":"reader","value":{"id":"ec71d704-3194-4664-b9bd-dd09ff499acd","version":2,"type":"text","properties":{"title":[["그러나 "],["참 가치함수",[["b"]]],[" 와 "],["최적 가치함수(Optimal Value Function)",[["b"]]],[" 은 다르다⭐️. "],["참 가치함수",[["c"]]],["는 "],["어떤 정책을 따라서 움직였을 경우에 받게되는 보상에 대한 참값",[["b"]]],[" 이고, "],["최적의 가치함수",[["c"]]],["는 "],["수많은 정책 중에서 가장 높은 보상을 얻게 되는 정책을 따랐을 때의 가치함수",[["b"]]],[" 이다."]]},"created_time":1676046122578,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"348fdd7e-f50f-4c36-8ad5-e501987f667e":{"role":"reader","value":{"id":"348fdd7e-f50f-4c36-8ad5-e501987f667e","version":2,"type":"text","properties":{"title":[["그렇다. 우리는 단순히 현재 정책에 대한 가치함수를 구하고 싶은게 아닌, "],["최적 정책",[["b"]]],["을 찾고 싶은 것이다. 단순히 현 정책에 대한 가치함수를 찾는 것이 아닌 더 좋은 정책으로 현재의 정책을 업데이트 해 나가야 한다. 이쯤에서 이런 질문이 들어야 한다."]]},"created_time":1676046122578,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"f1c069f4-2290-4336-a33d-432d0cbfd0ce":{"role":"reader","value":{"id":"f1c069f4-2290-4336-a33d-432d0cbfd0ce","version":2,"type":"bulleted_list","properties":{"title":[["더 좋은 정책이라는 것의 정의는 무엇인가?"]]},"created_time":1676046122578,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"9d775ffc-c0ed-447d-b2ac-ea5c13c78f67":{"role":"reader","value":{"id":"9d775ffc-c0ed-447d-b2ac-ea5c13c78f67","version":2,"type":"bulleted_list","properties":{"title":[["어떤 정책이 더 좋은 정책이라고 판단할 수 있을까?"]]},"created_time":1676046122578,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"4670c878-7f3b-4956-9eb0-08e41ba21a30":{"role":"reader","value":{"id":"4670c878-7f3b-4956-9eb0-08e41ba21a30","version":4,"type":"text","properties":{"title":[["더 좋은 정책이란 정책을 따라갔을 때 받을 보상들의 합이 더 큰 경우라고 말할 수 있을 것이다. 그리고 그것은 "],["가치함수",[["b"]]],[" 를 통해 판단할 수 있었다. 결국 가치함수가 정책이 얼마나 좋은지를 말해주는 것이다. 따라서 "],["모든 정책 중, 가장 큰 가치함수를 갖는 정책이 최적 정책이다.",[["b"]]],[" 최적 큐함수 또한 같은 방식으로 생각한다."]]},"created_time":1676046122578,"last_edited_time":1676046938486,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"53a2ffd5-844e-404d-940b-b94bad2e0654":{"role":"reader","value":{"id":"53a2ffd5-844e-404d-940b-b94bad2e0654","version":14,"type":"equation","properties":{"title":[["v^*(s) = max_{\\pi}[v_\\pi (s)]\\\\\\\\\nq^*(s,a) = max_{\\pi}[q_\\pi (s,a)]"]],"language":[["Markdown"]]},"created_time":1676046122578,"last_edited_time":1676046937047,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"c93fa6c5-3982-48f1-a178-4f6c5eba839e":{"role":"reader","value":{"id":"c93fa6c5-3982-48f1-a178-4f6c5eba839e","version":21,"type":"text","properties":{"title":[["가장 높은 가치함수(큐함수)를 에이전트가 찾았다고 가정해보자. 이때 "],["최적 정책",[["b"]]],[" 은 각 상태 "],["⁍",[["e","s"]]],["에서의 최적의 큐함수에 대해 가장 큰 값을 가진 행동을 하는 것이다. 즉, 선택 상황에서 판단 기준은 큐함수이며, 최적 정책은 최적 큐함수 "],["⁍",[["e","q^∗"]]],["만 안다면 아래와 같이 구할 수 있다."]]},"created_time":1676046122578,"last_edited_time":1676046970580,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"cd44bbfe-05b2-473c-b422-1dc02c96ecfb":{"role":"reader","value":{"id":"cd44bbfe-05b2-473c-b422-1dc02c96ecfb","version":13,"type":"equation","properties":{"title":[["\\pi^*(s,a) = \\begin{cases}\n  \\displaystyle 1, \\\\\\;if \\\\\\; a = argmax_{a \\in A} \\\\\\;q^*(s,a) \\\\\\\\\n  \\displaystyle 0, \\\\\\;otherwise\n\\end{cases}"]],"language":[["Markdown"]]},"created_time":1676046945286,"last_edited_time":1676046952947,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"2d3824c6-7a2d-4506-adaa-77c6611ccb5e":{"role":"reader","value":{"id":"2d3824c6-7a2d-4506-adaa-77c6611ccb5e","version":2,"type":"text","properties":{"title":[["그렇다면 최적의 큐함수는 어떻게 구할 수 있을까?"],["그것을 구하는 것이 "],["순차적 행동 결정 문제(MDP)",[["b"]]],[" 를 푸는 것이다. 어떻게 최적의 가치함수를 구하는지에 대해서는 다음 장에서 다룬다. 여기서는 최적의 가치함수 끼리 관계가 어떻게 되는지를 살펴보자."]]},"created_time":1676046122579,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"81c7d476-748b-4c23-97a3-02eb5ff64419":{"role":"reader","value":{"id":"81c7d476-748b-4c23-97a3-02eb5ff64419","version":4,"type":"text","properties":{"title":[["현재 상태의 가치함수가 최적이라고 가정해보자. 현재상태의 가치함수가 최적이라는 것은 에이전트가 가장 좋은 행동을 선택한다는 것이다. 이때 선택의 기준이 되는 큐함수는 최적의 큐함수 이어야 하고, 따라서 다음이 최적의 가치함수의 식이 된다."]]},"created_time":1676046122579,"last_edited_time":1676046992128,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"d653ec92-64ef-4ec1-8bda-b99192294df5":{"role":"reader","value":{"id":"d653ec92-64ef-4ec1-8bda-b99192294df5","version":9,"type":"equation","properties":{"title":[["v^"],["(s) = max_{a}[q^",[["i"]]],["(s,a) | S_t=s, A_t=a]"]]},"created_time":1676046992366,"last_edited_time":1676046997082,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"07502acf-2788-45ff-80b1-8dac6863547e":{"role":"reader","value":{"id":"07502acf-2788-45ff-80b1-8dac6863547e","version":2,"type":"text","properties":{"title":[["여기서 큐함수를 가치함수로 고쳐서 표현하면 아래와 같다."]]},"created_time":1676046122579,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"6c834085-f837-41b8-aefa-cd934aede06c":{"role":"reader","value":{"id":"6c834085-f837-41b8-aefa-cd934aede06c","version":8,"type":"equation","properties":{"title":[["v^"],["(s) = max_{a}E[R_{t+1} + \\gamma v^",[["i"]]],["(S_{t+1}) | S_t=s, A_t=a]"]]},"created_time":1676046122579,"last_edited_time":1676047008747,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"eb386ca4-154d-433b-a550-8d8fdee92646":{"role":"reader","value":{"id":"eb386ca4-154d-433b-a550-8d8fdee92646","version":2,"type":"text","properties":{"title":[["바로 이 식을 "],["벨만 최적 방정식(Bellman Optimality Equation)",[["c"]]],[" 이라 부르며, 이식은 최적의 가치함수에 대한 것이다. 큐함수에 대해서도 벨만 최적 방정식을 표현할 수 있는데, 아래와 같이 표현한다."]]},"created_time":1676046122579,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"5357aa08-c45f-4f42-a7eb-5ca390eb9729":{"role":"reader","value":{"id":"5357aa08-c45f-4f42-a7eb-5ca390eb9729","version":7,"type":"equation","properties":{"title":[["q^"],["(s,a) = E[R_{t+1} + \\gamma max_{a'}q^",[["i"]]],["(S_{t+1}, a') | S_t=s, A_t=a]"]]},"created_time":1676046122580,"last_edited_time":1676047020420,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"cdfd57bd-92f3-4e8c-a00b-e69615886692":{"role":"reader","value":{"id":"cdfd57bd-92f3-4e8c-a00b-e69615886692","version":2,"type":"text","properties":{"title":[["기댓값인 이유는 다음 상태가 상태 변환 확률에 따라 달라지기 때문이다. 벨만 기대 방정식과 벨만 최적 방정식을 이용해 MDP로 정의되는 문제를 "],["계산",[["b"]]],[" 으로 푸는 방법이 바로 다음장에서 다룰 "],["다이내믹 프로그래밍(Dynamic programming)",[["c"]]],["이다."]]},"created_time":1676046122580,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"b624f66f-5127-4f0d-a133-2140a36b4169":{"role":"reader","value":{"id":"b624f66f-5127-4f0d-a133-2140a36b4169","version":5,"type":"text","created_time":1676047024839,"last_edited_time":1676047024845,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"d734ec80-b47e-4554-92e9-b312b5689892":{"role":"reader","value":{"id":"d734ec80-b47e-4554-92e9-b312b5689892","version":2,"type":"sub_sub_header","properties":{"title":[["정리",[["b"]]]]},"created_time":1676046122580,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"3e283b6a-1df6-4772-ad10-d4f6dde5076a":{"role":"reader","value":{"id":"3e283b6a-1df6-4772-ad10-d4f6dde5076a","version":34,"type":"numbered_list","properties":{"title":[["MDP",[["c"]]],[": 순차적 행동 결정 문제를 수학적으로 정의하는 것. 상태("],["⁍",[["e","S_t"]]],["), 행동("],["⁍",[["e","A_t"]]],["), 보상함수("],["⁍",[["e","r(s,a)"]]],["), 상태 변환 확률("],["⁍",[["e","P^a_{ss′}"]]],["), 할인율("],["⁍",[["e","γ"]]],[")로 구성."]]},"content":["a10b9031-2bde-42aa-90f7-44f00c05206e"],"created_time":1676046122580,"last_edited_time":1676047095058,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"a10b9031-2bde-42aa-90f7-44f00c05206e":{"role":"reader","value":{"id":"a10b9031-2bde-42aa-90f7-44f00c05206e","version":8,"type":"equation","properties":{"title":[["v_\\pi (s) = E_\\pi [R_{t+1}+ \\gamma v_\\pi (S_{t+1})|S_t = s]"]]},"created_time":1676046122580,"last_edited_time":1676047099924,"parent_id":"3e283b6a-1df6-4772-ad10-d4f6dde5076a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"45a5da3d-129c-4d57-8508-a3b9993977f1":{"role":"reader","value":{"id":"45a5da3d-129c-4d57-8508-a3b9993977f1","version":4,"type":"numbered_list","properties":{"title":[["가치함수",[["c"]]],[": 현재 상태로부터 정책을 따라갔을 때 받을 것이라 예상되는 보상의 합."]]},"content":["5ed05359-ee95-47b1-9ab2-a70f4abcb9d2"],"created_time":1676046122581,"last_edited_time":1676047117223,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"5ed05359-ee95-47b1-9ab2-a70f4abcb9d2":{"role":"reader","value":{"id":"5ed05359-ee95-47b1-9ab2-a70f4abcb9d2","version":7,"type":"equation","properties":{"title":[["⁍",[["e","q_\\pi (s,a) = E_\\pi [R_{t+1}+ \\gamma q_\\pi (S_{t+1}, A_{t+1})|S_t = s, A_t=a]"]]]]},"created_time":1676046122581,"last_edited_time":1676047123384,"parent_id":"45a5da3d-129c-4d57-8508-a3b9993977f1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"39b23947-fdad-4591-8944-9b1c600a6c59":{"role":"reader","value":{"id":"39b23947-fdad-4591-8944-9b1c600a6c59","version":10,"type":"numbered_list","properties":{"title":[["큐함수",[["c"]]],[": 각 행동에 대해 가치를 알려주는 함수, 정책 업데이트 시에 사용"]]},"created_time":1676046122581,"last_edited_time":1676047139147,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"8810b4c2-3737-4ea5-a00b-b92e5953072a":{"role":"reader","value":{"id":"8810b4c2-3737-4ea5-a00b-b92e5953072a","version":7,"type":"equation","properties":{"title":[["q_\\pi (s,a) = E_\\pi [R_{t+1}+ \\gamma q_\\pi (S_{t+1}, A_{t+1})|S_t = s, A_t=a]"]]},"created_time":1676047139143,"last_edited_time":1676047143909,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"e3a3f05b-fd20-4668-8b1a-c4ab369d85e1":{"role":"reader","value":{"id":"e3a3f05b-fd20-4668-8b1a-c4ab369d85e1","version":24,"type":"numbered_list","properties":{"title":[["벨만 기대 방정식",[["c"]]],[": 현재 상태의 가치함수와 다음 상태 가치함수의 관계식"]]},"content":["d6e68191-e686-44c0-b9ba-3dcfda2f5a3c"],"format":{"list_start_index":4},"created_time":1676046122581,"last_edited_time":1676047168403,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"d6e68191-e686-44c0-b9ba-3dcfda2f5a3c":{"role":"reader","value":{"id":"d6e68191-e686-44c0-b9ba-3dcfda2f5a3c","version":14,"type":"equation","properties":{"title":[["v_\\pi (s) = E_\\pi [R_{t+1}+ \\gamma v_\\pi (S_{t+1})|S_t = s]"]]},"created_time":1676047149220,"last_edited_time":1676047162417,"parent_id":"e3a3f05b-fd20-4668-8b1a-c4ab369d85e1","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"7dcbb1c5-6694-4240-aa75-0d867c360590":{"role":"reader","value":{"id":"7dcbb1c5-6694-4240-aa75-0d867c360590","version":19,"type":"numbered_list","properties":{"title":[["벨만 최적 방정식",[["c"]]],[": 최적의 정책을 따르는 가치함수와 다음 상태 가치함수의 관계식"]]},"content":["958d8ba1-193f-46b9-b050-473997d80235","7f7b990c-0446-48c9-9db3-e76fe4569f5c"],"created_time":1676046122582,"last_edited_time":1676047190240,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"417f55c0-9f2f-42a8-8309-f07303d0e406":{"role":"reader","value":{"id":"417f55c0-9f2f-42a8-8309-f07303d0e406","version":2,"type":"sub_sub_header","properties":{"title":[["2장 한줄평",[["b"]]]]},"created_time":1676046122582,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"bfd5656f-5900-455f-ba71-171c864a947e":{"role":"reader","value":{"id":"bfd5656f-5900-455f-ba71-171c864a947e","version":2,"type":"quote","properties":{"title":[["최적 방정식 부분을 좀 더 명확하게 다시 정리해 봐야겠다. 🤥"]]},"created_time":1676046122582,"last_edited_time":1676046122599,"parent_id":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"958d8ba1-193f-46b9-b050-473997d80235":{"role":"reader","value":{"id":"958d8ba1-193f-46b9-b050-473997d80235","version":18,"type":"equation","properties":{"title":[["v^*(s) = max_{a}E[R_{t+1} + \\gamma v^*(S_{t+1}) | S_t=s, A_t=a]"]]},"created_time":1676047172890,"last_edited_time":1676047187387,"parent_id":"7dcbb1c5-6694-4240-aa75-0d867c360590","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"7f7b990c-0446-48c9-9db3-e76fe4569f5c":{"role":"reader","value":{"id":"7f7b990c-0446-48c9-9db3-e76fe4569f5c","version":11,"type":"text","created_time":1676047042185,"last_edited_time":1676047175172,"parent_id":"7dcbb1c5-6694-4240-aa75-0d867c360590","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"3d335e41-f00a-45df-8432-34a266c7566a":{"role":"reader","value":{"id":"3d335e41-f00a-45df-8432-34a266c7566a","version":480,"type":"page","properties":{"title":[["About"]]},"content":["e1067486-5fb5-4dc3-9415-54abc63fc3a5","ab253a7b-1dec-4620-8c4e-e99f94e1f6c9","460c887f-cc3d-4351-8671-7cfbc6d3bc1d","c62b8a9d-e8cb-4c5b-be5a-48412e956c95","abe79af7-1342-438a-9a59-2d0b1ce59007","88d551b8-6b87-4540-89b9-3619a8fe4059","ae31ce74-317a-45ae-9dd3-16c14e764361","78f83c2a-05c2-4033-bcd8-945d700b2952","fd280029-9a4c-47fe-8e3c-ce8d3f146671","6a0d0c32-f1bc-4277-a128-33da8613ede4","2bcccf09-dccf-4896-8819-255cf75aabaf","d84b4bd5-3b6b-45dd-a807-5fea62c5213b","8b7fb4d3-0195-4f61-a0eb-330792f84053","e9394173-e197-4fad-a1db-ac74fdba3738","cb29948c-7258-4b05-9bfa-25bf19ad0946","a4e4243c-25aa-480d-a9ee-0af2230c7d76","172e2e25-e319-4766-ad38-02fc168994cd","6c428956-d84e-4855-a998-7a79f1fc8ea5","90f1cecf-6258-4b54-b0ee-26ee4d950872","3eec82e9-0211-4271-8c30-1dbc86ab3be1","8ce5f430-8c4c-4e8f-b6a4-23396731b27d","aeb9d85e-fffa-4c34-b253-cb0499f047a2","3d37f4c7-8064-4792-a863-234d8b4d05a6","77daa708-adbc-4092-b7f7-9e3c8a988049","26e3aca8-3e09-4a37-8c92-3c95cf579bbb"],"format":{"page_icon":"https://s3-us-west-2.amazonaws.com/secure.notion-static.com/fc853118-df50-43ed-96d9-0711493d5e25/jang_inspiration_logo.png","page_cover":"/images/page-cover/nasa_reduced_gravity_walking_simulator.jpg","copied_from_pointer":{"id":"f1199d37-579b-41cb-abfc-0b5174f4256a","table":"block","spaceId":"fde5ac74-eea3-4527-8f00-4482710e1af3"},"page_cover_position":0.5},"created_time":1675998770866,"last_edited_time":1677390731360,"parent_id":"d83b5165-627c-41b3-82f0-f1cbf904e176","parent_table":"block","alive":true,"copied_from":"f1199d37-579b-41cb-abfc-0b5174f4256a","file_ids":["c8028bee-f4c7-4736-8c36-fffcab5d977e","9407e769-d877-4de9-bbaa-9e5626d971ed","07d5a5b4-de2a-4322-bfcc-c4ade3a63b86","fc853118-df50-43ed-96d9-0711493d5e25"],"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"d83b5165-627c-41b3-82f0-f1cbf904e176":{"role":"reader","value":{"id":"d83b5165-627c-41b3-82f0-f1cbf904e176","version":17,"type":"column","content":["e1fe2c9c-6eb8-412c-9874-a8ac2fce9ed8","3d335e41-f00a-45df-8432-34a266c7566a"],"format":{"column_ratio":0.25},"created_time":1676020529237,"last_edited_time":1676091451534,"parent_id":"1109cf3f-d8f9-4532-a13e-1af177ad4fdd","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"1109cf3f-d8f9-4532-a13e-1af177ad4fdd":{"role":"reader","value":{"id":"1109cf3f-d8f9-4532-a13e-1af177ad4fdd","version":13,"type":"column_list","content":["91ac285d-9a69-4ec4-96f6-9b046a15647c","028efbb4-417e-4742-8474-dd7a2ddeb8ee","d63c036c-b99a-4aa9-a068-87abc40d37a6","d83b5165-627c-41b3-82f0-f1cbf904e176"],"format":{"block_width":720,"block_full_width":false,"block_page_width":true},"created_time":1676020365844,"last_edited_time":1676091451534,"parent_id":"6246082f-4014-4d06-98ab-59e9840b298a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"e1067486-5fb5-4dc3-9415-54abc63fc3a5":{"role":"reader","value":{"id":"e1067486-5fb5-4dc3-9415-54abc63fc3a5","version":86,"type":"text","properties":{"title":[["Instagram",[["h","blue_background"],["a","https://www.instagram.com/jang.inspiration/"]]],[" • "],["GitHub",[["h","teal_background"],["a","https://github.com/longshiine"]]],[" • "],["LinkedIn",[["h","pink_background"],["a","https://www.linkedin.com/in/jangyeong-kim-b7924422a/"]]]]},"format":{"copied_from_pointer":{"id":"0ab4ae84-463a-4b31-9430-f4ed513c5fc2","table":"block","spaceId":"fde5ac74-eea3-4527-8f00-4482710e1af3"}},"created_time":1675998770872,"last_edited_time":1676960330564,"parent_id":"3d335e41-f00a-45df-8432-34a266c7566a","parent_table":"block","alive":true,"copied_from":"0ab4ae84-463a-4b31-9430-f4ed513c5fc2","created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}}},"collection":{"ba8460cf-4781-486e-8976-01358ef4659d":{"role":"reader","value":{"id":"ba8460cf-4781-486e-8976-01358ef4659d","version":117,"schema":{";KhU":{"name":"Last Updated","type":"last_edited_time"},"==~K":{"name":"Public","type":"checkbox"},"=bhc":{"name":"Curated","type":"checkbox"},"AfoN":{"name":"Category","type":"select","options":[{"id":"20579d4b-5ad0-469e-8946-2560e458bb81","color":"orange","value":"강화학습"},{"id":"b78b3694-0089-4efb-802c-c288e6190037","color":"green","value":"알고리즘"},{"id":"34c6aef1-b3b3-490e-9369-b5a385f7da4e","color":"blue","value":"딥러닝"},{"id":"67465999-8b72-4fe4-92ce-db5a812b880a","color":"brown","value":"공학수학"},{"id":"8b385c1c-8e95-4a02-b009-6215a718ebef","color":"pink","value":"글쓰기"},{"id":"435e1850-8e40-4b73-8496-0a3f694b6aeb","color":"purple","value":"스타트업"},{"id":"2b34c718-9490-4d2c-8c1d-949ea1a5010c","color":"gray","value":"독서"}]},"BN]P":{"name":"Tags","type":"multi_select","options":[{"id":"210cfb45-7eae-44e5-83dc-dba99aa3a853","color":"green","value":"Node.js"},{"id":"02b16a55-92ee-455d-9449-5e5e0a67cd04","color":"brown","value":"Computer Science"},{"id":"7993ec69-9767-4b84-adb9-5f1c907a6c77","color":"blue","value":"React.js"},{"id":"264c4a74-71d6-4015-bc91-5742970abd89","color":"yellow","value":"OSS"},{"id":"da38c5e1-f969-4abe-b28f-389b37aa22e5","color":"pink","value":"Startups"},{"id":"ceb7f269-10b7-49a9-bd4e-ba99533dee7b","color":"red","value":"Career"},{"id":"46b1d846-537f-43b5-a18c-298386278e63","color":"default","value":"Video"},{"id":"e485cc4c-e0de-4363-b442-22a0f6f588a9","color":"orange","value":"Saasify"},{"id":"5b6ee85e-ba0a-4c12-8e27-00e5b868939e","color":"gray","value":"SaaS"},{"id":"01c8627e-ae07-4f7b-b248-f877f88c8c4f","color":"purple","value":"Web Dev"},{"id":"136170f8-fe43-466b-b790-087508e8bc27","color":"blue","value":"Software Development"},{"id":"77f16ef0-4622-4f47-8f39-f5913a96421d","color":"pink","value":"Projects"},{"id":"aba9d5d2-c4f2-40ad-9f37-db6211536f92","color":"blue","value":"App Dev"},{"id":"84f76e57-8d2d-4078-bb7d-cf85520ba75c","color":"orange","value":"Lifestyle"},{"id":"38b5979d-877b-4cf4-920b-8b5da5ce9ba1","color":"yellow","value":"Thought Experiments"},{"id":"452e3943-dbcc-4d48-95fc-20d5d6339ddd","color":"orange","value":"Research"},{"id":"534e3ab2-fe89-487b-9784-71d76d5396a1","color":"purple","value":"Passion Economy"},{"id":"aa48c5a8-cfe3-4195-bfaa-25a72353299a","color":"yellow","value":"Tech"},{"id":"ff6898c4-f3f1-4024-b6a9-1cb871133744","color":"blue","value":"Creator Economy"},{"id":"ec59ba49-f2d4-4317-9a0e-3c7fb0bc60b8","color":"green","value":"Crypto"},{"id":"3fc961e4-4408-4d65-b750-7c136977ce61","color":"gray","value":"Deep Learning"},{"id":"a1419788-3341-407e-8df0-cd3d1f4ff636","color":"brown","value":"Gradient Vanishing"},{"id":"50ebe91f-e808-4056-b8ec-3b0bae45d464","color":"yellow","value":"Convolution Layer"},{"id":"ae8b3ff9-00e0-4d25-8d8d-e7389fc7fc8c","color":"orange","value":"Dot Product"},{"id":"8f6c2d82-17e0-4fb1-82eb-a6047ec75d02","color":"orange","value":"Vector"},{"id":"96ed8c32-a637-4b4f-ba99-3b397d04435a","color":"red","value":"Auction Theory"},{"id":"d4090af3-0dfb-480d-9503-0d0328774c16","color":"pink","value":"Reinforcement Learning"},{"id":"6fbb9d5c-c0f4-4791-8397-6e89c63e0c82","color":"yellow","value":"MDP"},{"id":"92885d4f-2302-43ac-87b0-e9e1c7a8cd8b","color":"yellow","value":"Introduction"},{"id":"f5ee4f74-bef9-4701-9d7d-1b9e7068d702","color":"blue","value":"Value Function"},{"id":"a575fa70-0999-44a3-b96c-262d57984b63","color":"green","value":"bellman equation"},{"id":"807f640e-1abc-476a-8300-1fe0ec0a15a3","color":"gray","value":"Grid World"},{"id":"e959edcb-aeb9-4a66-91e9-7677c68a85a8","color":"purple","value":"Dynamic Programming"},{"id":"3c5ba2f8-7ec5-4d9c-89b8-2ae975f4740a","color":"default","value":"Policy Iteration"},{"id":"b79852f3-5c3c-4de7-8d6d-731673caae40","color":"green","value":"Value Iteration"},{"id":"c2bd765b-351d-4a6d-a75d-94e8c476a180","color":"red","value":"Policy Evaluation"},{"id":"4a911c23-ca00-4f29-ad2f-463eb166f8f2","color":"brown","value":"SARSA"},{"id":"563d1a3e-095d-4ad8-bb7e-cc8c8195a9c1","color":"green","value":"Q-Learning"},{"id":"9e1bb3b2-a545-4e9d-b1eb-c47d2db8f4a8","color":"gray","value":"Writing"},{"id":"fcaf8148-917c-44bd-8ca1-fce87ba28f55","color":"blue","value":"Nepal"},{"id":"d93da022-471d-4d13-8dbc-051aa3e4639f","color":"orange","value":"Travel"},{"id":"f20c5e6f-58fe-4208-baa3-20704f665d21","color":"purple","value":"Algorithm"},{"id":"447d9963-8cfc-4c4c-9adb-88f0dc85f249","color":"red","value":"Python"},{"id":"633d7256-ffbf-4e5b-9912-906130d85250","color":"yellow","value":"Big-O"},{"id":"1a1af600-6b9b-4b8e-9483-c038712cda7c","color":"default","value":"String"},{"id":"b24d85c4-b7af-411c-905f-c9ae68eab612","color":"pink","value":"Array"},{"id":"87516a34-7662-4a77-a219-149777b960dc","color":"red","value":"LinkedList"},{"id":"d63330f6-ba17-4b9c-b370-579f4c99358a","color":"blue","value":"Stack"},{"id":"04eff27e-18a0-4883-8915-6ba26059106d","color":"yellow","value":"Queue"},{"id":"328cf039-a0b4-4b5c-90f7-be587808a1dc","color":"orange","value":"Deque"},{"id":"3364095f-3a21-47c6-8460-16ddc7f02c13","color":"brown","value":"HashTable"},{"id":"64841489-db5b-45ba-972a-7af2f53f41c7","color":"yellow","value":"Graph"},{"id":"b6dea43b-7633-4290-91f0-9761023838b0","color":"purple","value":"Tree"},{"id":"9dc575d0-d0d5-4282-8430-aae294857404","color":"gray","value":"Heap"},{"id":"edc331ab-bf6d-4985-9ecc-6884de83e8fc","color":"brown","value":"Trie"},{"id":"f2fdcca6-c69d-4b53-ab23-665ca7a223b9","color":"yellow","value":"Sort"},{"id":"5e02ca54-b743-4501-9776-62e030442114","color":"default","value":"BinarySearch"},{"id":"9a317b05-c2df-41ff-bf3f-32c39b5c451b","color":"red","value":"Greedy"},{"id":"536f3cec-7932-4444-b8c7-154672ad5fe6","color":"pink","value":"DivideAndConquer"},{"id":"1fe05376-d59a-4e4a-8596-fcd70f194621","color":"orange","value":"Basic"},{"id":"6fdde3f1-6554-4e54-ba5b-af1fc95b0ce4","color":"green","value":"Linear"},{"id":"28fc1aec-d1dd-44bb-a3ab-78ebc6f74837","color":"yellow","value":"NonLinear"},{"id":"e74e9521-68b1-4c11-ab42-a034a44dfd24","color":"blue","value":"알고리즘"},{"id":"42f167a6-7e2e-4d8e-854d-426689132f08","color":"brown","value":"Amazon"},{"id":"8aefb99d-7b6b-453b-941f-e114e32c6438","color":"blue","value":"book"},{"id":"d5083966-d1d3-4116-a615-5107104890c2","color":"pink","value":"Business"},{"id":"df607bdb-4379-4273-a45e-e52d436aef0d","color":"red","value":"Diffusion Model"},{"id":"da21253b-a1e7-4fe8-9ed3-6073a347c457","color":"orange","value":"Paper Review"}]},"NVm^":{"name":"Slug","type":"text"},"QSi`":{"name":"Series","type":"checkbox"},"a\u003cql":{"name":"Published","type":"date"},"jhf;":{"name":"Tweet","type":"text"},"nAX{":{"name":"Created","type":"created_time"},"}nqi":{"name":"Author","type":"text"},"~]S\u003c":{"name":"Description","type":"text"},"title":{"name":"Name","type":"title"}},"format":{"copied_from_pointer":{"id":"e5fdcb8e-6e29-4bc9-828d-263749307808","table":"collection","spaceId":"fde5ac74-eea3-4527-8f00-4482710e1af3"},"property_visibility":[{"property":"AfoN","visibility":"hide_if_empty"},{"property":"BN]P","visibility":"hide_if_empty"},{"property":"a\u003cql","visibility":"hide_if_empty"},{"property":"}nqi","visibility":"hide_if_empty"},{"property":"~]S\u003c","visibility":"hide"},{"property":"==~K","visibility":"hide"},{"property":"jhf;","visibility":"hide"},{"property":"=bhc","visibility":"hide"},{"property":"NVm^","visibility":"hide"},{"property":"nAX{","visibility":"hide"},{"property":";KhU","visibility":"hide"},{"property":"QSi`","visibility":"hide"}],"collection_page_properties":[{"visible":false,"property":"AfoN"},{"visible":true,"property":"BN]P"},{"visible":true,"property":"a\u003cql"},{"visible":false,"property":"}nqi"},{"visible":true,"property":"~]S\u003c"},{"visible":true,"property":"==~K"},{"visible":true,"property":"jhf;"},{"visible":false,"property":"=bhc"},{"visible":false,"property":"NVm^"},{"visible":true,"property":"nAX{"},{"visible":false,"property":";KhU"}]},"parent_id":"4f20ede8-ccf7-4ae1-82e5-819e100dd032","parent_table":"block","alive":true,"copied_from":"e5fdcb8e-6e29-4bc9-828d-263749307808","template_pages":["cacb6437-50cd-4a3d-9d43-58962f75e40b"],"migrated":true,"space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6","deleted_schema":{":nQy":{"name":"Curating","type":"text"}}}}},"collection_view":{"d91647c5-6a81-48b1-a1ff-a04529d0ddba":{"role":"reader","value":{"id":"d91647c5-6a81-48b1-a1ff-a04529d0ddba","version":33,"type":"gallery","name":"Gallery view","format":{"gallery_cover":{"type":"page_cover"},"gallery_cover_size":"medium","gallery_properties":[{"visible":false,"property":"AfoN"},{"visible":false,"property":"jhf;"},{"visible":false,"property":"NVm^"},{"visible":false,"property":"==~K"},{"visible":false,"property":";KhU"},{"visible":false,"property":"=bhc"},{"visible":false,"property":"}nqi"},{"visible":false,"property":"nAX{"},{"visible":false,"property":"BN]P"},{"visible":true,"property":"title"},{"visible":true,"property":"~]S\u003c"},{"visible":true,"property":"a\u003cql"}],"gallery_cover_aspect":"cover","hide_linked_collection_name":true,"inline_collection_first_load_limit":{"type":"load_limit","limit":10}},"parent_id":"4f20ede8-ccf7-4ae1-82e5-819e100dd032","parent_table":"block","alive":true,"page_sort":["e3df5b79-eb34-4bec-a82f-628699f43852","7a8e9c32-39af-4bca-a26e-3f8f1c9a1762","7864e0d8-d646-4df5-a4ae-057371b7559d","36a57bd6-d63a-40f0-a0a2-6ba247885fab","c2994f3d-696a-4015-b7d1-2b71883562ad","58fc3d75-92a2-44e9-ab0d-84cb61086232","30ac6075-b07c-41fa-b684-15c16f1134ba","4c804cac-3119-4cc3-a621-a82040d9a0db","6ce6c0ba-0d6f-40b3-b7d4-afe8ca715c73","64158711-b8f3-4c3d-ba2f-3999059c5581","0c679f04-0865-4142-a65b-28668aa9daea","54fa8c94-ead7-4b14-b068-174936adb37f","ab2e2675-7daf-464f-a2c4-563136630232","ef52bf7e-8726-4804-a0d5-e9b5f7919284","460f888e-b262-4f14-a39f-93ae0090bb9d","eff2e07b-ee44-45e0-b46a-622deb611d0b","c94f3f08-d65f-4f7d-a409-72c353f9786d","daac7673-f388-475e-8f43-a345dc350bef","6ff46924-9cc3-4445-badc-28e7daa5fa5a","c114dc9f-b09a-4e30-9c0d-5ace92e6a8e3","51b78f8b-69ff-4a46-9ec6-74f0d5400d3e","06adcb1d-3acf-4266-a8d9-6cb0d3c439ce","22c49483-380f-4b94-be90-b437b7cbab52","45a3d677-b3df-4cc1-ac40-7e0ab214aeef","7a177274-38ce-4f03-b3f6-06e20b0ba4f8","15aa9135-7cca-40c7-b656-ba4457524924","599373d0-99e2-4c28-89de-d273b43aca5c","48756e2f-e604-4421-9b34-be2c90e20589","b94b3e79-f161-45a8-bf8e-5315f85dca99","3e71fe2e-c827-4fc6-b610-7d71147ae4a7","298f8e1a-a9f2-4274-b5f1-7279f7bc4e5b","c7f629b2-6f4e-43fe-8720-4a3a10d4e651","b1fded7b-ceea-46d0-89e2-32010807d35c","bd99ffd5-ce6d-4418-be0e-6db2cd8ae070","5e852682-71e3-45b9-8056-d40e972563fd","1db23f4a-8d7e-4445-b241-a8e4be5bd02a","70df362d-5c3a-4d2c-a3da-ef27e1f207e3","dae737f3-5216-4c50-b708-2bf7c2662020","a13c9d7e-5e2f-44c3-bd8c-17fd50ca4892","f3236b1d-73c7-45d5-920e-f7cac8c19573","c181e327-c3bf-42f8-b8aa-21a367ce64f2","f5613c52-6b68-4073-b593-03d26f51e710"],"query2":{"sort":[{"property":"=bhc","direction":"descending"},{"property":"a\u003cql","direction":"descending"}],"filter":{"filters":[{"filters":[{"filter":{"value":{"type":"exact","value":true},"operator":"checkbox_is"},"property":"==~K"},{"filter":{"value":{"type":"exact","value":true},"operator":"checkbox_is"},"property":"=bhc"},{"filter":{"value":{"type":"exact","value":true},"operator":"checkbox_is"},"property":"QSi`"}],"operator":"and"}],"operator":"and"},"aggregations":[{"aggregator":"count"}]},"space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"27bc73a3-5779-44e0-b617-2f0d26f5aa2f":{"role":"reader","value":{"id":"27bc73a3-5779-44e0-b617-2f0d26f5aa2f","version":1,"type":"table","name":"","format":{"table_properties":[{"width":293,"visible":true,"property":"title"},{"width":398,"visible":true,"property":"~]S\u003c"},{"width":81,"visible":true,"property":"==~K"},{"width":105,"visible":true,"property":"=bhc"},{"width":146,"visible":true,"property":"a\u003cql"},{"width":200,"visible":true,"property":"BN]P"},{"width":122,"visible":true,"property":"}nqi"},{"width":156,"visible":true,"property":"jhf;"},{"width":146,"visible":true,"property":"NVm^"},{"width":200,"visible":true,"property":";KhU"},{"width":200,"visible":false,"property":"nAX{"}]},"parent_id":"4f20ede8-ccf7-4ae1-82e5-819e100dd032","parent_table":"block","alive":true,"page_sort":["f3d44d4c-975d-4b11-8396-c68b35bfdb26","70df362d-5c3a-4d2c-a3da-ef27e1f207e3","f3236b1d-73c7-45d5-920e-f7cac8c19573","4117e62e-18ec-4503-a32a-6c31806a5e2a","6b9736df-63cb-4e7c-ba8e-d6e54f26d6c9","c2f261a2-c616-4198-b1a9-2caf6be162a0","a082287f-d4c9-4cfb-b3b2-72b8bbf043c6","4801bc76-a763-46a8-981d-79dc38c5d85a","e3df5b79-eb34-4bec-a82f-628699f43852","36a57bd6-d63a-40f0-a0a2-6ba247885fab","c2994f3d-696a-4015-b7d1-2b71883562ad","58fc3d75-92a2-44e9-ab0d-84cb61086232","30ac6075-b07c-41fa-b684-15c16f1134ba","4c804cac-3119-4cc3-a621-a82040d9a0db","6ce6c0ba-0d6f-40b3-b7d4-afe8ca715c73","64158711-b8f3-4c3d-ba2f-3999059c5581","0c679f04-0865-4142-a65b-28668aa9daea","54fa8c94-ead7-4b14-b068-174936adb37f","ab2e2675-7daf-464f-a2c4-563136630232","ef52bf7e-8726-4804-a0d5-e9b5f7919284","460f888e-b262-4f14-a39f-93ae0090bb9d","eff2e07b-ee44-45e0-b46a-622deb611d0b","c94f3f08-d65f-4f7d-a409-72c353f9786d","daac7673-f388-475e-8f43-a345dc350bef","6ff46924-9cc3-4445-badc-28e7daa5fa5a","c114dc9f-b09a-4e30-9c0d-5ace92e6a8e3","51b78f8b-69ff-4a46-9ec6-74f0d5400d3e","06adcb1d-3acf-4266-a8d9-6cb0d3c439ce","22c49483-380f-4b94-be90-b437b7cbab52","1db23f4a-8d7e-4445-b241-a8e4be5bd02a","dae737f3-5216-4c50-b708-2bf7c2662020","a13c9d7e-5e2f-44c3-bd8c-17fd50ca4892","7864e0d8-d646-4df5-a4ae-057371b7559d","7a8e9c32-39af-4bca-a26e-3f8f1c9a1762","0f58fdf7-da3e-4793-93f8-b2503443020d","ca43f24a-f6c2-4611-a92a-8051ec80fb0f","36b681a4-e13e-4a78-a2d6-f56b2e3657a1","8cfc0d5a-59ba-4ea0-9609-4720753d5fba","88891af2-1639-4eff-a2c9-f3ba6668b2ea","92db0a94-6e1e-44a6-8df5-882e60ff5b3f","0d958b63-df7a-44a4-b80c-5408c78f59e4","7a177274-38ce-4f03-b3f6-06e20b0ba4f8","ae0b3ee0-531c-4f81-8d4b-cc6dd040fea1","bf9953ee-2589-4969-948c-0d31106a9deb","fdb7d90f-a355-4e7d-8f9a-3d07a58a457e","561e9779-b56c-4310-8913-d54675e02c74","6951e99b-10cc-4833-a27f-f0e08f8941d0","76a4bb7d-be00-45f3-bbcd-cba28c38588b","18c92268-31e5-4509-8644-9c6ad9e44c10","6463af62-efa9-47dc-91ce-99df728f66e0"],"query2":{"sort":[{"property":"=bhc","direction":"descending"},{"property":"a\u003cql","direction":"descending"}],"aggregations":[{"property":"title","aggregator":"count"}]},"space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}}},"notion_user":{},"collection_query":{},"signed_urls":{},"preview_images":{"images.unsplash.com/photo-1563209259-b2fa97148ce1":{"originalWidth":4509,"originalHeight":3433,"dataURIBase64":"data:image/webp;base64,UklGRkQAAABXRUJQVlA4IDgAAAAQAgCdASoQAAwABUB8JZQCdAEN4Z4/0jgAAP7nI6g5q7g7NuXgHKw945x+RhNJbTW2vaRJ2E2wAA=="},"notion.so/image/notion.so%2Fimages%2Fpage-cover%2Fnasa_reduced_gravity_walking_simulator.jpg":{"originalWidth":2000,"originalHeight":1597,"dataURIBase64":"data:image/webp;base64,UklGRjwAAABXRUJQVlA4IDAAAADQAQCdASoQAA0ABUB8JaQAAuUwaXLIHAD+3O9cAAKxBQKBBYBng9tx6GTwHd24mAA="},"notion.so/image/s3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fb29e9b03-c79c-4e52-a45e-7228163ba524%2Fcompass-circular-tool_(3).png":{"originalWidth":2000,"originalHeight":2000,"dataURIBase64":"data:image/webp;base64,UklGRv4AAABXRUJQVlA4WAoAAAAQAAAADwAADwAAQUxQSFUAAAARL6CmkRQ4eF5eaio0EBGB5+2AUWxbbR5t/yWkKiDsUwVQnn8xoViI6P8EAOK9AiCF/BQQSXKHsKrOmo21PddC4xsKsbafNSc1jfQLgBSSGsD0HBoAAFZQOCCCAAAAsAIAnQEqEAAQAAVAfCWkAA+DSIAfdDvk/UudpKAAAPYcuQdG3VZft4V447Ryb0FWml/BLapzhboSZ6SLFp2WFSojeqJNq2VAANIrwpK47u1M/geoV8ECX9+TEWBDeQC4c2g3HcSldGi6FylcOf6PWeqJaegO6jdPFlsvX04FC2gAAA=="},"notion.so/image/longshiine.github.io%2F2021%2F01%2F22%2FReinforcement-Learning-Basic-2%2Fqfunction.jpeg":{"originalWidth":2000,"originalHeight":2348,"dataURIBase64":"data:image/webp;base64,UklGRkIAAABXRUJQVlA4IDYAAADwAQCdASoOABAABUB8JaQAAuddOGI6OAAA/vHapZeURANzT1qe3DO+M+KvbCrGHsNYXURsAAA="},"notion.so/image/longshiine.github.io%2F2021%2F01%2F22%2FReinforcement-Learning-Basic-2%2Fbellman.png":{"originalWidth":2000,"originalHeight":753,"dataURIBase64":"data:image/webp;base64,UklGRjIAAABXRUJQVlA4ICYAAAAwAQCdASoQAAYABUB8JaQAA3AA/u/25CyNHHTnULWZhZDJYSoAAA=="},"notion.so/image/s3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Ffc853118-df50-43ed-96d9-0711493d5e25%2Fjang_inspiration_logo.png":{"originalWidth":2000,"originalHeight":2000,"dataURIBase64":"data:image/webp;base64,UklGRv4AAABXRUJQVlA4WAoAAAAQAAAADwAADwAAQUxQSFUAAAARL6CmkRQ4eF5eaio0EBGB5+2AUWxbbR5t/yWkKiDsUwVQnn8xoViI6P8EAOK9AiCF/BQQSXKHsKrOmo21PddC4xsKsbafNSc1jfQLgBSSGsD0HBoAAFZQOCCCAAAAsAIAnQEqEAAQAAVAfCWkAA+DSIAfdDvk/UudpKAAAPYcuQdG3VZft4V447Ryb0FWml/BLapzhboSZ6SLFp2WFSojeqJNq2VAANIrwpK47u1M/geoV8ECX9+TEWBDeQC4c2g3HcSldGi6FylcOf6PWeqJaegO6jdPFlsvX04FC2gAAA=="}}},"pageId":"c6190e2c-1011-4990-9ce3-d219dfd1c0b4","rawPageId":"bellman-equation"},"__N_SSG":true},"page":"/[pageId]","query":{"pageId":"bellman-equation"},"buildId":"a6SgCHTQj8ioz2ipgYdLM","isFallback":false,"dynamicIds":[635,7274,3358],"gsp":true,"scriptLoader":[]}</script></body></html>
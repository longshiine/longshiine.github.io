<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta name="robots" content="index,follow"/><meta property="og:type" content="website"/><meta property="og:site_name" content="Jang. Inspiration"/><meta property="twitter:domain" content="jang-inspiration.com"/><meta name="description" content="살사(SARSA)와 큐러닝(Q-Learning) 그리고 각각의 코드를 살펴보자. 마지막으로 온폴리시 정책과 오프폴리시 정책의 차이에 대해서 톺아보자."/><meta property="og:description" content="살사(SARSA)와 큐러닝(Q-Learning) 그리고 각각의 코드를 살펴보자. 마지막으로 온폴리시 정책과 오프폴리시 정책의 차이에 대해서 톺아보자."/><meta name="twitter:description" content="살사(SARSA)와 큐러닝(Q-Learning) 그리고 각각의 코드를 살펴보자. 마지막으로 온폴리시 정책과 오프폴리시 정책의 차이에 대해서 톺아보자."/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:image" content="https://jang-inspiration.com/api/social-image?id=60a262e5-29a3-44e9-a757-cf3312bd21b5"/><meta property="og:image" content="https://jang-inspiration.com/api/social-image?id=60a262e5-29a3-44e9-a757-cf3312bd21b5"/><link rel="canonical" href="https://jang-inspiration.com/sarsa-qlearning"/><meta property="og:url" content="https://jang-inspiration.com/sarsa-qlearning"/><meta property="twitter:url" content="https://jang-inspiration.com/sarsa-qlearning"/><link rel="alternate" type="application/rss+xml" href="https://jang-inspiration.com/feed" title="Jang. Inspiration"/><meta property="og:title" content="&lt;파이썬과 케라스로 배우는 강화학습&gt; - (7) 살사(SARSA)와 큐러닝(Q-Learning)"/><meta name="twitter:title" content="&lt;파이썬과 케라스로 배우는 강화학습&gt; - (7) 살사(SARSA)와 큐러닝(Q-Learning)"/><title>&lt;파이썬과 케라스로 배우는 강화학습&gt; - (7) 살사(SARSA)와 큐러닝(Q-Learning)</title><meta name="naver-site-verification" content="3942485b5f7254d146b71f1249d907d89048a4d6"/><link rel="preload" as="image" href="https://images.unsplash.com/photo-1563209259-b2fa97148ce1?ixlib=rb-4.0.3&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb"/><meta name="next-head-count" content="22"/><link rel="shortcut icon" href="/favicon.ico"/><link rel="icon" type="image/png" sizes="32x32" href="favicon.png"/><link rel="manifest" href="/manifest.json"/><link rel="preload" href="/_next/static/css/d2e6a1cb5181bdcf.css" as="style"/><link rel="stylesheet" href="/_next/static/css/d2e6a1cb5181bdcf.css" data-n-g=""/><link rel="preload" href="/_next/static/css/4e32f0fa5eadbe4b.css" as="style"/><link rel="stylesheet" href="/_next/static/css/4e32f0fa5eadbe4b.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script defer="" src="/_next/static/chunks/3607272e.d338bf53926ee7c2.js"></script><script defer="" src="/_next/static/chunks/853.526a4df21aef109c.js"></script><script defer="" src="/_next/static/chunks/175675d1.a2f4b19cd9daa73f.js"></script><script defer="" src="/_next/static/chunks/274.59c48af6aaac8ebd.js"></script><script defer="" src="/_next/static/chunks/358.0027340467b29549.js"></script><script src="/_next/static/chunks/webpack-bcbf8f4abc46b243.js" defer=""></script><script src="/_next/static/chunks/framework-4556c45dd113b893.js" defer=""></script><script src="/_next/static/chunks/main-f08b69bdcc7bbb61.js" defer=""></script><script src="/_next/static/chunks/pages/_app-27630c741ae01a08.js" defer=""></script><script src="/_next/static/chunks/780-3413afc2700f261b.js" defer=""></script><script src="/_next/static/chunks/634-6b5e1cdbf7ca12e3.js" defer=""></script><script src="/_next/static/chunks/pages/%5BpageId%5D-35899cbd792aff57.js" defer=""></script><script src="/_next/static/a6SgCHTQj8ioz2ipgYdLM/_buildManifest.js" defer=""></script><script src="/_next/static/a6SgCHTQj8ioz2ipgYdLM/_ssgManifest.js" defer=""></script></head><body><script>
/** Inlined version of noflash.js from use-dark-mode */
;(function () {
  var storageKey = 'darkMode'
  var classNameDark = 'dark-mode'
  var classNameLight = 'light-mode'
  function setClassOnDocumentBody(darkMode) {
    document.body.classList.add(darkMode ? classNameDark : classNameLight)
    document.body.classList.remove(darkMode ? classNameLight : classNameDark)
  }
  var preferDarkQuery = '(prefers-color-scheme: dark)'
  var mql = window.matchMedia(preferDarkQuery)
  var supportsColorSchemeQuery = mql.media === preferDarkQuery
  var localStorageTheme = null
  try {
    localStorageTheme = localStorage.getItem(storageKey)
  } catch (err) {}
  var localStorageExists = localStorageTheme !== null
  if (localStorageExists) {
    localStorageTheme = JSON.parse(localStorageTheme)
  }
  // Determine the source of truth
  if (localStorageExists) {
    // source of truth from localStorage
    setClassOnDocumentBody(localStorageTheme)
  } else if (supportsColorSchemeQuery) {
    // source of truth from system
    setClassOnDocumentBody(mql.matches)
    localStorage.setItem(storageKey, mql.matches)
  } else {
    // source of truth from document.body
    var isDarkMode = document.body.classList.contains(classNameDark)
    localStorage.setItem(storageKey, JSON.stringify(isDarkMode))
  }
})();
</script><div id="__next"><div class="notion notion-app light-mode notion-block-60a262e529a344e9a757cf3312bd21b5"><div class="notion-viewport"></div><div class="notion-frame"><header class="notion-header"><div class="notion-nav-header"><div class="breadcrumbs"><a class="breadcrumb" href="/"><div class="notion-page-icon-inline notion-page-icon-image"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%272000%27%20height=%272000%27/%3e"/></span><img alt="Jang. Inspiration" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" class="icon notion-page-icon" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/webp;base64,UklGRv4AAABXRUJQVlA4WAoAAAAQAAAADwAADwAAQUxQSFUAAAARL6CmkRQ4eF5eaio0EBGB5+2AUWxbbR5t/yWkKiDsUwVQnn8xoViI6P8EAOK9AiCF/BQQSXKHsKrOmo21PddC4xsKsbafNSc1jfQLgBSSGsD0HBoAAFZQOCCCAAAAsAIAnQEqEAAQAAVAfCWkAA+DSIAfdDvk/UudpKAAAPYcuQdG3VZft4V447Ryb0FWml/BLapzhboSZ6SLFp2WFSojeqJNq2VAANIrwpK47u1M/geoV8ECX9+TEWBDeQC4c2g3HcSldGi6FylcOf6PWeqJaegO6jdPFlsvX04FC2gAAA==&quot;)"/><noscript><img alt="Jang. Inspiration" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fb29e9b03-c79c-4e52-a45e-7228163ba524%2Fcompass-circular-tool_(3).png?table=block&amp;id=6246082f-4014-4d06-98ab-59e9840b298a&amp;cache=v2" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="icon notion-page-icon" loading="lazy"/></noscript></span></div><span class="title">Jang. Inspiration</span></a></div><div class="notion-nav-header-rhs breadcrumbs"><a href="/about" class="breadcrumb button">About</a><div class="breadcrumb button styles_hidden__7gYve"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="32" d="M256 48v48m0 320v48m147.08-355.08l-33.94 33.94M142.86 369.14l-33.94 33.94M464 256h-48m-320 0H48m355.08 147.08l-33.94-33.94M142.86 142.86l-33.94-33.94"></path><circle cx="256" cy="256" r="80" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="32"></circle></svg></div><div role="button" class="breadcrumb button notion-search-button"><svg class="notion-icon searchIcon" viewBox="0 0 17 17"><path d="M6.78027 13.6729C8.24805 13.6729 9.60156 13.1982 10.709 12.4072L14.875 16.5732C15.0684 16.7666 15.3232 16.8633 15.5957 16.8633C16.167 16.8633 16.5713 16.4238 16.5713 15.8613C16.5713 15.5977 16.4834 15.3516 16.29 15.1582L12.1504 11.0098C13.0205 9.86719 13.5391 8.45215 13.5391 6.91406C13.5391 3.19629 10.498 0.155273 6.78027 0.155273C3.0625 0.155273 0.0214844 3.19629 0.0214844 6.91406C0.0214844 10.6318 3.0625 13.6729 6.78027 13.6729ZM6.78027 12.2139C3.87988 12.2139 1.48047 9.81445 1.48047 6.91406C1.48047 4.01367 3.87988 1.61426 6.78027 1.61426C9.68066 1.61426 12.0801 4.01367 12.0801 6.91406C12.0801 9.81445 9.68066 12.2139 6.78027 12.2139Z"></path></svg></div></div></div></header><div class="notion-page-scroller"><div class="notion-page-cover-wrapper"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%274509%27%20height=%273433%27/%3e"/></span><img alt="&lt;파이썬과 케라스로 배우는 강화학습&gt; - (7) 살사(SARSA)와 큐러닝(Q-Learning)" src="https://images.unsplash.com/photo-1563209259-b2fa97148ce1?ixlib=rb-4.0.3&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb" decoding="async" data-nimg="intrinsic" class="notion-page-cover" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-position:center 50%;background-size:cover;background-position:center 50%;filter:blur(20px);background-image:url(&quot;data:image/webp;base64,UklGRkQAAABXRUJQVlA4IDgAAAAQAgCdASoQAAwABUB8JZQCdAEN4Z4/0jgAAP7nI6g5q7g7NuXgHKw945x+RhNJbTW2vaRJ2E2wAA==&quot;)"/><noscript><img alt="&lt;파이썬과 케라스로 배우는 강화학습&gt; - (7) 살사(SARSA)와 큐러닝(Q-Learning)" src="https://images.unsplash.com/photo-1563209259-b2fa97148ce1?ixlib=rb-4.0.3&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-position:center 50%" class="notion-page-cover"/></noscript></span></div><main class="notion-page notion-page-has-cover notion-page-has-icon notion-page-has-text-icon notion-full-page"><div class="notion-page-icon-hero notion-page-icon-span"><span class="notion-page-icon" role="img" aria-label="🎮">🎮</span></div><h1 class="notion-title">&lt;파이썬과 케라스로 배우는 강화학습&gt; - (7) <b>살사(SARSA)와 큐러닝(Q-Learning)</b></h1><div class="notion-collection-page-properties"><div class="notion-collection-row"><div class="notion-collection-row-body"><div class="notion-collection-row-property"><div class="notion-collection-column-title"><svg viewBox="0 0 14 14" class="notion-collection-column-title-icon"><path d="M7 13A6 6 0 107 1a6 6 0 000 12zM3.751 5.323A.2.2 0 013.909 5h6.182a.2.2 0 01.158.323L7.158 9.297a.2.2 0 01-.316 0L3.751 5.323z"></path></svg><div class="notion-collection-column-title-body">Category</div></div><div class="notion-collection-row-value"><span class="notion-property notion-property-select"><div class="notion-property-select-item notion-item-orange">강화학습</div></span></div></div><div class="notion-collection-row-property"><div class="notion-collection-column-title"><svg viewBox="0 0 14 14" class="notion-collection-column-title-icon"><path d="M4 3a1 1 0 011-1h7a1 1 0 110 2H5a1 1 0 01-1-1zm0 4a1 1 0 011-1h7a1 1 0 110 2H5a1 1 0 01-1-1zm0 4a1 1 0 011-1h7a1 1 0 110 2H5a1 1 0 01-1-1zM2 4a1 1 0 110-2 1 1 0 010 2zm0 4a1 1 0 110-2 1 1 0 010 2zm0 4a1 1 0 110-2 1 1 0 010 2z"></path></svg><div class="notion-collection-column-title-body">Tags</div></div><div class="notion-collection-row-value"><span class="notion-property notion-property-multi_select"><div class="notion-property-multi_select-item notion-item-pink">Reinforcement Learning</div><div class="notion-property-multi_select-item notion-item-brown">SARSA</div><div class="notion-property-multi_select-item notion-item-green">Q-Learning</div></span></div></div><div class="notion-collection-row-property"><div class="notion-collection-column-title"><svg viewBox="0 0 14 14" class="notion-collection-column-title-icon"><path d="M10.889 5.5H3.11v1.556h7.778V5.5zm1.555-4.444h-.777V0H10.11v1.056H3.89V0H2.333v1.056h-.777c-.864 0-1.548.7-1.548 1.555L0 12.5c0 .856.692 1.5 1.556 1.5h10.888C13.3 14 14 13.356 14 12.5V2.611c0-.855-.7-1.555-1.556-1.555zm0 11.444H1.556V3.944h10.888V12.5zM8.556 8.611H3.11v1.556h5.445V8.61z"></path></svg><div class="notion-collection-column-title-body">Published</div></div><div class="notion-collection-row-value"><span class="notion-property notion-property-date">January 27, 2021</span></div></div><div class="notion-collection-row-property"><div class="notion-collection-column-title"><svg viewBox="0 0 14 14" class="notion-collection-column-title-icon"><path d="M7 4.568a.5.5 0 00-.5-.5h-6a.5.5 0 00-.5.5v1.046a.5.5 0 00.5.5h6a.5.5 0 00.5-.5V4.568zM.5 1a.5.5 0 00-.5.5v1.045a.5.5 0 00.5.5h12a.5.5 0 00.5-.5V1.5a.5.5 0 00-.5-.5H.5zM0 8.682a.5.5 0 00.5.5h11a.5.5 0 00.5-.5V7.636a.5.5 0 00-.5-.5H.5a.5.5 0 00-.5.5v1.046zm0 3.068a.5.5 0 00.5.5h9a.5.5 0 00.5-.5v-1.045a.5.5 0 00-.5-.5h-9a.5.5 0 00-.5.5v1.045z"></path></svg><div class="notion-collection-column-title-body">Author</div></div><div class="notion-collection-row-value"><span class="notion-property notion-property-text"><b>Jay</b></span></div></div></div></div></div><div class="notion-page-content notion-page-content-has-aside"><article class="notion-page-content-inner"><blockquote class="notion-quote notion-block-561d49ee83cb401daed35697479b918b"><div>본 포스트는 “파이썬과 케라스로 배우는 강화학습” 도서의 일곱번째 리뷰 포스트입니다.</div></blockquote><div class="notion-text notion-block-eeafe760921c4b8e91db9eb556a98956"><a target="_blank" rel="noopener noreferrer" class="notion-link" href="http://www.yes24.com/Product/Goods/44136413">http://www.yes24.com/Product/Goods/44136413</a></div><div class="notion-blank notion-block-324cf525cfac438db53729ae004aef8d"> </div><div class="notion-table-of-contents notion-gray notion-block-696d206d11e948b2b871f80f69f6f65a"><a href="#0d933e3f7aac41e4ae056d5ef1d99e74" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:0">4장 강화학습 기초 3: 살사(SARSA)와 큐러닝(Q-Learning)</span></a><a href="#4ce63a43b961470da1b52ea0f03608f8" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">살사(SARSA)</span></a><a href="#82b21d4b541d4f308d811ee73f8f3859" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">살사 코드 설명</span></a><a href="#132c2000aab74666a2273d8b2d71cdac" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">살사의 한계</span></a><a href="#807fb48ccfa84d1aa39e02bc0696b1fa" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">큐러닝</span></a><a href="#8681789e66a644d7863390997d43390d" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">큐러닝 코드 설명</span></a><a href="#01769602415547a69a327f17eabfc526" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">정리</span></a></div><div class="notion-blank notion-block-f898b3dd73b14a87a190ec2c824d7965"> </div><h3 class="notion-h notion-h2 notion-h-indent-0 notion-block-0d933e3f7aac41e4ae056d5ef1d99e74" data-id="0d933e3f7aac41e4ae056d5ef1d99e74"><span><div id="0d933e3f7aac41e4ae056d5ef1d99e74" class="notion-header-anchor"></div><a class="notion-hash-link" href="#0d933e3f7aac41e4ae056d5ef1d99e74" title="4장 강화학습 기초 3: 살사(SARSA)와 큐러닝(Q-Learning)"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>4장 강화학습 기초 3: 살사(SARSA)와 큐러닝(Q-Learning)</b></span></span></h3><hr class="notion-hr notion-block-b500cb16cda04f12a71a19d475be32ed"/><div class="notion-text notion-block-80cefca410914f1b9125bb547e6e168f">정책 이터레이션과 가치 이터레이션은 살사로 발전한다. <code class="notion-inline-code">살사</code>부터 강화학습이라 부른다. 각 이터레이션들이 어떻게 살사로 발전하는지, 살사와 같은 강화학습 알고리즘을 통해 에이전트가 어떻게 학습하는지 알아보자.</div><div class="notion-blank notion-block-17b16a0a93074ee6a30100e062c88853"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-4ce63a43b961470da1b52ea0f03608f8" data-id="4ce63a43b961470da1b52ea0f03608f8"><span><div id="4ce63a43b961470da1b52ea0f03608f8" class="notion-header-anchor"></div><a class="notion-hash-link" href="#4ce63a43b961470da1b52ea0f03608f8" title="살사(SARSA)"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>살사(SARSA)</b></span></span></h4><div class="notion-text notion-block-c6465428ec9c4be4aeb810cb186849cb">정책 이터레이션은 <b>정책 평가</b> 와 <b>정책 발전</b> 을 번갈아 가며 실행하는 과정이다. 벨만 기대 방정식을 이용해 현재의 정책에 대한 참 가치함수를 구하는 것이 정책 평가이며, 구한 가치함수에 따라 정책을 업데이트 하는 것이 정책 발전이다. 이러한 정책 이터레이션을 <code class="notion-inline-code">GPI(Generalized Policy Iteration)</code>이라고 한다. GPI에서는 단 한번만 정책을 평가해서 가치함수를 업데이트하고 바로 정책을 발전하는 과정을 반복한다.</div><div class="notion-text notion-block-4e79865c2c534355b1369f2bbb831025">GPI에서는 벨만 방정식에 따라 정책을 평가한다. 그 대신 강화학습에서는 <b>몬테카를로 예측</b> 이나 <b>시간차 예측</b> 을 사용하여 정책을 평가한다. <b>GPI의 탐욕 정책 발전</b> 은 주어진 가치함수에 대해 모든 상태에 대한 정책을 얻는 과정이다. <b>시간차 방법</b> 에서는 타임스텝마다 가치함수를 현재 상태에 대해서만 업데이트 하므로, 모든 상태의 정책을 발전시킬 수 없다.</div><div class="notion-text notion-block-fd539311758e4166acae3d184b1de06f">때문에 시간차(Temporal-Difference) 방법에서는 가치 이터레이션의 방법을 도입한다. 가치 이터레이션에서는 정책 이터레이션과는 달리 <b>별도의 정책 없이 가치함수에 대해 탐욕적으로 움직일 뿐</b> 이었고, 시간차 방법에서도 에이전트는 <b>현재 상태에서 가장 큰 가치를 지니는 행동을 선택하는</b> <code class="notion-inline-code">탐욕 정책</code> 을 사용한다. 시간차 예측과 탐욕 정책이 합쳐진 것을 <code class="notion-inline-code">시간차 제어(Temporal-difference control)</code>이라고 한다. 이들의 관계를 나타낸 것이 아래의 그림이다.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-bfe83e551b4b449986edceb422cd0fea"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:432px;max-width:100%;flex-direction:column"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%272000%27%20height=%27823%27/%3e"/></span><img alt="GPI와 시간차 제어의 관계" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/webp;base64,UklGRjQAAABXRUJQVlA4ICgAAADwAQCdASoQAAYABUB8JaQAAt0DcVCTkAAA/ufpe9jkz1wETzWpgAAA&quot;)"/><noscript><img alt="GPI와 시간차 제어의 관계" src="https://www.notion.so/image/https%3A%2F%2Flongshiine.github.io%2F2021%2F01%2F27%2FReinforcement-Learning-Basic-6%2Ftemporal.png?table=block&amp;id=bfe83e55-1b4b-4499-86ed-ceb422cd0fea&amp;cache=v2" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></span><figcaption class="notion-asset-caption">GPI와 시간차 제어의 관계</figcaption></div></figure><div class="notion-text notion-block-8105c1ddfd1c4ae5a7297876fe996c2f">탐욕 정책에서 <b>다음 상태의 가치함수를 보고 판단하는 것이 아닌 현재 상태의 큐함수를 보고 판단한다면 환경의 모델을 몰라도 된다.</b> (GPI의 탐욕 정책 발전에 따르면 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>을 계산하기 위해서 상태 변환 확률인  <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>를 알아야 했다. 행동을 했을 때, 이상한데로 튈 확률이랄까나. 요녀석은 환경의 일부로서 현실에서는 알기가 매우매우 힘든 정보이기 때문에, 이 녀석 없이 행동을 선택하게 하는 것이 필요하다!)시간차 제어에서는 아래의 <b>큐함수를 사용한 탐욕 정책</b> 을 통해 행동을 선택한다.</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-0e431f2cc78c4e81a0ded14398457020"><span></span></span><div class="notion-text notion-block-ec8fa191878446f5a927cadcfc5492c3">큐함수에 따라서 행동을 선택하려면 에이전트는 가치함수가 아닌 큐함수의 정보를 알아야 하므로, 업데이트의 대상은 가치함수가 아닌 큐함수가 되어야 한다. 때문에 <code class="notion-inline-code">시간차 제어</code>의 식은 다음과 같다.</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-f48dc1b8bdf54eed9cf4b77845a22ed7"><span></span></span><div class="notion-text notion-block-8e5203bdd83f457da0e778db4578b702">시간차 제어에서 큐함수를 업데이트 하려면 샘플이 필요하다. 시간차 제어에서는 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>을 샘플로 사용한다. 흐름을 한번 살펴보면</div><ul class="notion-list notion-list-disc notion-block-21fd1207e6e7483abc62b011251d0837"><li>에이전트는 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>에서 탐욕 정책에 따라 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>를 선택</li></ul><ul class="notion-list notion-list-disc notion-block-2d8766fec0024901b23d6277120dac4f"><li>환경은 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>을 제공하고 다음상태 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>을 제공</li></ul><ul class="notion-list notion-list-disc notion-block-962dff0124604a9aa5872bbc8e65fb2c"><li>에이전트는 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>에서 탐욕 정책에 따라 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>을 선택</li></ul><div class="notion-text notion-block-0eb7799d66454902a178b430344860cd">샘플의 형태 때문에 시간차 제어를 다른말로 <code class="notion-inline-code">살사(SARSA)</code>라고 부른다. 살사는 현재 가지고 있는 <b>큐함수</b> 를 토대로 샘플을 탐욕 정책으로 모으고, 그 샘플로 방문한 큐함수를 업데이트하는 과정을 반복하는 것이다.</div><div class="notion-text notion-block-f1af973aa99d497c885f3718226cec60">초기의 에이전트에게 탐욕정책은 <b>잘못된 학습으로 가게할 가능성이 크다.</b> 때문에, 큐함수가 잘못된 값에 수렴하는 것을 막기 위해 에이전트가 충분히 다양한 경험을 하도록 해야하고, 이를 위해 <b><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span></b><b>-탐욕 정책</b> 을 사용한다. 간단한 아이디어인데 <b><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span></b>의 확률로 탐욕적이지 않은 행동을 선택하게 하는 것이다.</div><div class="notion-text notion-block-0735987f213943abb7bc9b38c7399699">(물론 <b><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span></b>-탐욕 정책은 최적 큐함수를 찾았다 하더라도 <b><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span></b><b>의 확률로 계속 탐험한다는 한계</b> 가 있다. 따라서 학습을 진행함에 따라 <b><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span></b>값을 감소시키는 방법을 사용할 수도 있다.)</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-ed4d41066ca74328a5f631f962a685d9"><span></span></span><div class="notion-text notion-block-bbf9eee9974a4c7bbb68284dc67af41f">정리하자면, 살사는 간단히 두 단계로 생각하면 된다.</div><ol start="1" class="notion-list notion-list-numbered notion-block-6fecf4bef4da479993a7137e88767433"><li><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>-탐욕 정책을 통해 샘플 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>을 획득</li></ol><ol start="2" class="notion-list notion-list-numbered notion-block-e21a035d6a2a407eb330e093a2bfb99c"><li>획득한 샘플로 다음 식을 통해 큐함수 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>를 업데이트</li></ol><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-f1334736f1b140c6bec1a4ae056150b3"><span></span></span><div class="notion-text notion-block-e31036806bce464392a21c3e64f82ae0">이제 코드를 한번 살펴보자</div><div class="notion-blank notion-block-c21c39df95514779bf5ae640ab17eabc"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-82b21d4b541d4f308d811ee73f8f3859" data-id="82b21d4b541d4f308d811ee73f8f3859"><span><div id="82b21d4b541d4f308d811ee73f8f3859" class="notion-header-anchor"></div><a class="notion-hash-link" href="#82b21d4b541d4f308d811ee73f8f3859" title="살사 코드 설명"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>살사 코드 설명</b></span></span></h4><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python">class SARSAgent:
  def __init__(self,actions):
    self.actions = actions # 에이전트가 할 수 있는 행동 [상,하,좌,우]
    self.step_size = 0.01      # α
    self.discount_factor = 0.9 # γ
    self.epsilon = 0.1         # ϵ
    self.q_table  = defaultdict(lambda: [0.0, 0.0, 0.0, 0.0])</code></pre><div class="notion-text notion-block-5fe0c1c178ef4855ab9620163945139a">init을 통해 학습에 필요한 변수들을 할당해 주었다. 추가로 <b>SARSAgent</b> 에 어떤 함수가 필요한지를 알기 위해서는 에이전트가 환경과 어떻게 상호작용하고 학습하는지를 알아야 한다. 에이전트는 다음과 같은 순서로 상호작용한다.</div><ol start="1" class="notion-list notion-list-numbered notion-block-efd27f376fd64ca2bd3561fc98a9bf31"><li>현재 상태에서 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>-탐욕 정책에 따라 행동을 선택</li></ol><ol start="2" class="notion-list notion-list-numbered notion-block-b513324b7495404d93779fe4a3689f96"><li>선택한 행동으로 환경에서 한 타임스텝을 진행</li></ol><ol start="3" class="notion-list notion-list-numbered notion-block-94fcc00a1fdd45dabf11c5a6bb0f02b4"><li>환경으로부터 보상과 다음 상태를 받음</li></ol><ol start="4" class="notion-list notion-list-numbered notion-block-6d52178a28f4450d92910ece0bdd7ace"><li>다음 상태에서 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>-탐욕 정책에 따라 다음 행동을 선택</li></ol><ol start="5" class="notion-list notion-list-numbered notion-block-9ed9f5d8e6574b499c3471f5de36ded1"><li>(s,a,r,s’,a’)을 통해 큐함수 업데이트</li></ol><div class="notion-text notion-block-7f1f22da03f8435481c5b55a2464911a">get_action 함수는 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>-탐욕 정책에 따라 state를 입력으로 받아 action을 반환한다. q_table에 따라서 탐욕적으로 행동을 선택하며 이때 의 확률을 반영하여 무작위 행동을 반환하기도 한다.</div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python"># 입실론 탐욕 정책에 따라서 행동을 반환
def get_action(self, state):
    if np.random.rand() &lt; self.epsilon:
        # 무작위 행동 반환
        action = np.random.choice(self.actions)
    else:
        # 큐함수에 따른 행동 반환
        state = str(state)
        q_list = self.q_table[state]
        action = arg_max(q_list)
    return action</code></pre><div class="notion-text notion-block-d3393c641ccd41c683dd9d721dbd4882">현재 상태와 다음 상태에서의 행동을 선택해서 샘플(s,a,r,s’,a’)을 얻으면 에이전트는 학습을 진행한다. 즉 아래의 식의 역할을 하는 함수는 learn이며 코드는 다음과 같다.(굉장히 직관적이다!)</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-51cc2394af404195ae1b878938239137"><span></span></span><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python"># &lt;s, a, r, s&#x27;, a&#x27;&gt;의 샘플로부터 큐함수를 업데이트
def learn(self, state, action, reward, next_state, next_action):
    state, next_state = str(state), str(next_state)
    current_q = self.q_table[state][action]
    next_state_q = self.q_table[next_state][next_action]
    td = reward + self.discount_factor * next_state_q - current_q
    new_q = current_q + self.step_size * td
    self.q_table[state][action] = new_q</code></pre><div class="notion-text notion-block-fb401abb8e13437585a1d45d4228b771">get_action과 learn 함수를 통해 에이전트는 메인 루프에서 다음과 같이 환경과 상호작용한다.</div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python"># 행동을 위한 후 다음상태 보상 에피소드의 종료 여부를 받아옴
next_state, reward, done = env.step(action)
# 다음 상태에서의 다음 행동 선택
next_action = agent.get_action(next_state)
# &lt;s,a,r,s&#x27;,a&#x27;&gt;로 큐함수를 업데이트
agent.learn(state, action, reward, next_state, next_action)

state = next_state
action = next_action</code></pre><div class="notion-blank notion-block-06f2161015b7430197e92dea7111f4d0"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-132c2000aab74666a2273d8b2d71cdac" data-id="132c2000aab74666a2273d8b2d71cdac"><span><div id="132c2000aab74666a2273d8b2d71cdac" class="notion-header-anchor"></div><a class="notion-hash-link" href="#132c2000aab74666a2273d8b2d71cdac" title="살사의 한계"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>살사의 한계</b></span></span></h4><div class="notion-text notion-block-2d202c29fa2242879ad735b2747e3def">살사에서는 충분한 <b>탐험(Exploration)</b> 을 하기 위해 <!-- -->-탐욕 정책을 사용했다. 그런데 다음의 경우를 한번 생각해보자.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-b80d34e956ce4db4b2fa67a6fb6c0e83"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:432px;max-width:100%;flex-direction:column"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%272000%27%20height=%272072%27/%3e"/></span><img alt="살사의 학습과정 중 한순간" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/webp;base64,UklGRkYAAABXRUJQVlA4IDoAAADQAQCdASoPABAABUB8JQAAXadrlhggAAD+7agzm7v1GK2nhWMtYakTkx3nwHflQAAUz92sggJZIAAA&quot;)"/><noscript><img alt="살사의 학습과정 중 한순간" src="https://www.notion.so/image/https%3A%2F%2Flongshiine.github.io%2F2021%2F01%2F27%2FReinforcement-Learning-Basic-6%2Flimitation.png?table=block&amp;id=b80d34e9-56ce-4db4-b2fa-67a6fb6c0e83&amp;cache=v2" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></span><figcaption class="notion-asset-caption">살사의 학습과정 중 한순간</figcaption></div></figure><div class="notion-text notion-block-15dd170b5b6040d995b7315da75544ae">초기 에이전트가 만약 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>에서 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>라는 행동을 하고 다음 행동인 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>은 탐험을 통해서 가게 되었다고 생각해보자. 그럼 자연스럽게 초기 에이전트는 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>값을 낮출 것이고 이에 따라 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>에서 아래로 이동하는 행동이 안좋다고 판단할 것이다. 결국 에이전트가 특정 state에 <b>갇혀버리는 현상</b> 이 발생한다. 이렇게 자신이 행동한 대로 학습하는 것을 <b>On-Policy 시간차 제어</b> 라고 한다. 이러한 딜레마를 해결하기 위해 사용하는 것이 바로 <b>오프폴리시 시간차 제어</b> , <code class="notion-inline-code">큐러닝</code> 이다.</div><div class="notion-blank notion-block-3065c75c2b374e4797d7209adc310c66"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-807fb48ccfa84d1aa39e02bc0696b1fa" data-id="807fb48ccfa84d1aa39e02bc0696b1fa"><span><div id="807fb48ccfa84d1aa39e02bc0696b1fa" class="notion-header-anchor"></div><a class="notion-hash-link" href="#807fb48ccfa84d1aa39e02bc0696b1fa" title="큐러닝"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>큐러닝</b></span></span></h4><div class="notion-text notion-block-7e5a02095a6249a199437f68570ee922">큐러닝의 아이디어는 간단하다. <b>오프폴리시</b> 의 말 그대로 현재 행동하는 정책과는 독립적으로 학습한다는 것이다. 즉, <b>행동하는 정책과 학습하는 정책을 따로 분리</b> 한다. 이게 무슨 말일까? 예시로서 이해해보자.</div><div class="notion-text notion-block-dd25c7b5710d4fa89f456416a77865c9">에이전트가 현재 상황 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>에서 행동 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>를 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>-탐욕 정책에 따라 선택했다고 하자. 그러면 에이전트는 환경으로부터 보상 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>을 받고 다음 상태 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>을 받는다. 여기까지는 살사와 동일하다. 하지만 살사에서는 다음 상태 에서 또다시 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>-탐욕 정책에 따라 다음 행동을 선택한 후에 그것을 학습에 샘플로 사용한다. <b>큐러닝</b> 에서는 에이전트가 다음 상태 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>을 알게 되면 <b>그 상태(</b><b><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span></b><b>)에서 가장 큰 큐함수를 현재 큐함수의 업데이트에 사용한다.</b> 살사의 학습과정과 다르게 큐러닝은 아래와 같이 학습한다.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-bcab3ef11fae4b6f8e8e519b29cebd89"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:384px;max-width:100%;flex-direction:column"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%272000%27%20height=%272101%27/%3e"/></span><img alt="큐러닝의 학습과정 중 한순간" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/webp;base64,UklGRkIAAABXRUJQVlA4IDYAAACQAQCdASoPABAABUB8JYwAAsemtoAA/u3WsjjFPs/FGgDVtaaHvjiWGipkgI+uBAEVSE2iAAA=&quot;)"/><noscript><img alt="큐러닝의 학습과정 중 한순간" src="https://www.notion.so/image/https%3A%2F%2Flongshiine.github.io%2F2021%2F01%2F27%2FReinforcement-Learning-Basic-6%2Fqlearning.png?table=block&amp;id=bcab3ef1-1fae-4b6f-8e8e-519b29cebd89&amp;cache=v2" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></span><figcaption class="notion-asset-caption">큐러닝의 학습과정 중 한순간</figcaption></div></figure><div class="notion-text notion-block-5b978dcfa4b24356a7aee42635906bdf">큐러닝은 실제 다음 상태 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>에서 다음 행동을 해보는 것이 아니라 다음 상태 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>에서 가장 큰 큐함수를 가지고 업데이트 하는 것이다. 자세히 살펴보면 벨만 최적 방정식과 비슷하다는 생각이 들 것이다.</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-6de567fe01504b9caf072d4cb267d804"><span></span></span><div class="notion-text notion-block-c2cbe06191ac436e935af8bc879ef901">벨만 최적 방정식은 아래의 수식과 같은데 큐러닝에서 보상 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>은 실제 에이전트가 환경에게서 받는 값이므로 기댓값을 빼면 동일하다.</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-b3914227dd304219a49dc326b02b2b51"><span></span></span><ul class="notion-list notion-list-disc notion-block-58cba1a17eb14aa8bdaa39e5b01ac875"><li><code class="notion-inline-code">벨만 기대 방정식</code> –&gt; <code class="notion-inline-code">정책 이터레이션</code> –&gt; <code class="notion-inline-code">살사</code></li></ul><ul class="notion-list notion-list-disc notion-block-98de53f6556f45b29bb37abfa2ca0859"><li><code class="notion-inline-code">벨만 최적 방정식</code> –&gt; <code class="notion-inline-code">가치 이터레이션</code> –&gt; <code class="notion-inline-code">큐러닝</code></li></ul><div class="notion-text notion-block-1c6fa583b8eb4af8964c123defc6ccb8">큐러닝은 샘플로서 [s,a,r,s’]을 사용하며 실제 환경에서 행동을 하는 정책과 큐함수를 업데이트할 때 사용하는 정책이 다르기 때문에 큐러닝을 <code class="notion-inline-code">오프폴리시</code>라고 한다. 다른 오프폴리시 강화학습과 달리 큐함수가 간단하기 떄문에 이후에 많은 강화학습 알고리즘의 토대가 되었다.</div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-8681789e66a644d7863390997d43390d" data-id="8681789e66a644d7863390997d43390d"><span><div id="8681789e66a644d7863390997d43390d" class="notion-header-anchor"></div><a class="notion-hash-link" href="#8681789e66a644d7863390997d43390d" title="큐러닝 코드 설명"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>큐러닝 코드 설명</b></span></span></h4><div class="notion-text notion-block-6a4808ea797d47d3a779adfdf9c72c67">큐러닝 코드에서 살사 코드에서와 다른 점은 에이전트가 샘플을 가지고 학습하는 부분이다.</div><pre class="notion-code"><div class="notion-code-copy"><div class="notion-code-copy-button"><svg fill="currentColor" viewBox="0 0 16 16" width="1em" version="1.1"><path fill-rule="evenodd" d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 010 1.5h-1.5a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-1.5a.75.75 0 011.5 0v1.5A1.75 1.75 0 019.25 16h-7.5A1.75 1.75 0 010 14.25v-7.5z"></path><path fill-rule="evenodd" d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0114.25 11h-7.5A1.75 1.75 0 015 9.25v-7.5zm1.75-.25a.25.25 0 00-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 00.25-.25v-7.5a.25.25 0 00-.25-.25h-7.5z"></path></svg></div></div><code class="language-python"># &lt;s, a, r, s&#x27;&gt; 샘플로부터 큐함수 업데이트
    def learn(self, state, action, reward, next_state):
        state, next_state = str(state), str(next_state)
        q_1 = self.q_table[state][action]
        # 벨만 최적 방정식을 사용한 큐함수의 업데이트
        q_2 = reward + self.discount_factor * max(self.q_table[next_state])
        self.q_table[state][action] += self.step_size * (q_2 - q_1)</code></pre><div class="notion-text notion-block-7b9c4706f95b4d81a71837e3e6624a86">learn 코드는 위에서 살펴본 큐러닝의 업데이트 식을 구현한 것이다. self.q_table[next_state]에서 max 값을 업데이트에 사용하기 때문에 오프폴리시가 됩니다. 또한 max값을 취하면 되기 때문에 다음 상태에서의 행동을 알 필요가 없다.</div><div class="notion-text notion-block-be858c34d1264ac082bb621a80c77581">살사와 큐러닝의 차이는 <b>온폴리시와 오프폴리시</b> 의 차이라고 볼 수 있다. 온폴리시인 살사는 <b>지속적인 탐험</b> 으로인해 그리드월드 예제에서 왼쪽 위에 갇히곤 하지만 큐러닝은 현재 행동하는 정책과는 독립적으로 학습을 진행하기 때문에 갇히지 않고 벗어나는 정책을 학습할 수 있다.</div><div class="notion-blank notion-block-63034432bfbb4eb4aa6443178528224d"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-01769602415547a69a327f17eabfc526" data-id="01769602415547a69a327f17eabfc526"><span><div id="01769602415547a69a327f17eabfc526" class="notion-header-anchor"></div><a class="notion-hash-link" href="#01769602415547a69a327f17eabfc526" title="정리"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>정리</b></span></span></h4><ul class="notion-list notion-list-disc notion-block-8d05fd6500ad455cbc5127e492fe807c"><li><code class="notion-inline-code">몬테카를로 예측</code>: 기댓값을 샘플링을 통한 평균으로 대체</li><ul class="notion-list notion-list-disc notion-block-8d05fd6500ad455cbc5127e492fe807c"><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-e0ecf11861e841b3b11e6c7993107180"><span></span></span></ul></ul><ul class="notion-list notion-list-disc notion-block-f90fffa7815e4cf4b6f0a92cce2bd46e"><li><code class="notion-inline-code">시간차 예측</code>: 몬테카를로 예측과는 달리 타임스텝마다 큐함수를 업데이트</li><ul class="notion-list notion-list-disc notion-block-f90fffa7815e4cf4b6f0a92cce2bd46e"><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-6237bfaadae844129f0a41c2dbf60f4f"><span></span></span></ul></ul><ul class="notion-list notion-list-disc notion-block-85de806c0ad148b883189f076e016fef"><li><code class="notion-inline-code">살사(SARSA)</code>: 강화학습 제어에서 큐함수를 사용하며 하나의 샘플로 (s,a,r,s’,a’)을 사용하는 <b>시간차 제어</b></li><ul class="notion-list notion-list-disc notion-block-85de806c0ad148b883189f076e016fef"><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-48f7204abeba4551892209a8d7dc1d68"><span></span></span></ul></ul><ul class="notion-list notion-list-disc notion-block-50239149344540dc93888a272073902f"><li><code class="notion-inline-code">큐러닝(Q-learning)</code>: 오프폴리시 강화학습으로서 행동선택은 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>-<b>탐욕 정책</b> , 큐함수의 업데이트에는 <b>벨만 최적 방정식</b> 사용</li><ul class="notion-list notion-list-disc notion-block-50239149344540dc93888a272073902f"><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-ca595ed5f1ce4e75a6e8fd8e905f8413"><span></span></span></ul></ul></article><aside class="notion-aside"></aside></div></main></div></div></div><div style="width:100%;background-color:#ffffff;color:#373534;padding:20px"><div id="disqus_recommendations"></div><div id="disqus_thread"></div><footer class="styles_footer__RBpyk"><div class="styles_copyright__nhL_k">Copyright 2023 <!-- -->Jang Yeong</div><div class="styles_settings__GyEhi"></div><div class="styles_social__ptL3p"><a class="styles_github__0JN7a" href="https://github.com/longshiine" title="GitHub @longshiine" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a class="styles_linkedin__bgwDi" href="https://www.linkedin.com/in/jangyeong-kim-b7924422a" title="LinkedIn Jang Yeong" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a><a class="styles_instagram__BY5Hj" href="https://instagram.com/jang.inspiration" title="Instagram Jang Yeong" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><g><path fill="none" d="M0 0h24v24H0z"></path><path fill-rule="nonzero" d="M12 2c2.717 0 3.056.01 4.122.06 1.065.05 1.79.217 2.428.465.66.254 1.216.598 1.772 1.153a4.908 4.908 0 0 1 1.153 1.772c.247.637.415 1.363.465 2.428.047 1.066.06 1.405.06 4.122 0 2.717-.01 3.056-.06 4.122-.05 1.065-.218 1.79-.465 2.428a4.883 4.883 0 0 1-1.153 1.772 4.915 4.915 0 0 1-1.772 1.153c-.637.247-1.363.415-2.428.465-1.066.047-1.405.06-4.122.06-2.717 0-3.056-.01-4.122-.06-1.065-.05-1.79-.218-2.428-.465a4.89 4.89 0 0 1-1.772-1.153 4.904 4.904 0 0 1-1.153-1.772c-.248-.637-.415-1.363-.465-2.428C2.013 15.056 2 14.717 2 12c0-2.717.01-3.056.06-4.122.05-1.066.217-1.79.465-2.428a4.88 4.88 0 0 1 1.153-1.772A4.897 4.897 0 0 1 5.45 2.525c.638-.248 1.362-.415 2.428-.465C8.944 2.013 9.283 2 12 2zm0 5a5 5 0 1 0 0 10 5 5 0 0 0 0-10zm6.5-.25a1.25 1.25 0 0 0-2.5 0 1.25 1.25 0 0 0 2.5 0zM12 9a3 3 0 1 1 0 6 3 3 0 0 1 0-6z"></path></g></svg></a></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"site":{"domain":"jang-inspiration.com","name":"Jang. Inspiration","rootNotionPageId":"6246082f40144d0698ab59e9840b298a","rootNotionSpaceId":null,"description":"장영의 영감노트"},"recordMap":{"block":{"60a262e5-29a3-44e9-a757-cf3312bd21b5":{"role":"reader","value":{"id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","version":722,"type":"page","properties":{"==~K":[["Yes"]],"AfoN":[["강화학습"]],"BN]P":[["Reinforcement Learning,SARSA,Q-Learning"]],"NVm^":[["sarsa-qlearning"]],"a\u003cql":[["‣",[["d",{"type":"date","start_date":"2021-01-27"}]]]],"}nqi":[["Jay"]],"~]S\u003c":[["살사(SARSA)와 큐러닝(Q-Learning) 그리고 각각의 코드를 살펴보자. 마지막으로 온폴리시 정책과 오프폴리시 정책의 차이에 대해서 톺아보자."]],"title":[["\u003c파이썬과 케라스로 배우는 강화학습\u003e - (7) "],["살사(SARSA)와 큐러닝(Q-Learning)",[["b"]]]]},"content":["561d49ee-83cb-401d-aed3-5697479b918b","eeafe760-921c-4b8e-91db-9eb556a98956","324cf525-cfac-438d-b537-29ae004aef8d","696d206d-11e9-48b2-b871-f80f69f6f65a","f898b3dd-73b1-4a87-a190-ec2c824d7965","0d933e3f-7aac-41e4-ae05-6d5ef1d99e74","b500cb16-cda0-4f12-a71a-19d475be32ed","80cefca4-1091-4f1b-9125-bb547e6e168f","17b16a0a-9307-4ee6-a301-00e062c88853","4ce63a43-b961-470d-a1b5-2ea0f03608f8","c6465428-ec9c-4be4-aeb8-10cb186849cb","4e79865c-2c53-4355-b136-9f2bbb831025","fd539311-758e-4166-acae-3d184b1de06f","bfe83e55-1b4b-4499-86ed-ceb422cd0fea","8105c1dd-fd1c-4ae5-a729-7876fe996c2f","0e431f2c-c78c-4e81-a0de-d14398457020","ec8fa191-8784-46f5-a927-cadcfc5492c3","f48dc1b8-bdf5-4eed-9cf4-b77845a22ed7","8e5203bd-d83f-457d-a0e7-78db4578b702","21fd1207-e6e7-483a-bc62-b011251d0837","2d8766fe-c002-4901-b23d-6277120dac4f","962dff01-2460-4a9a-a587-2bbc8e65fb2c","0eb7799d-6645-4902-a178-b430344860cd","f1af973a-a99d-497c-885f-3718226cec60","0735987f-2139-43ab-b7bc-9b38c7399699","ed4d4106-6ca7-4328-a5f6-31f962a685d9","bbf9eee9-974a-4c7b-bb68-284dc67af41f","6fecf4be-f4da-4799-93a7-137e88767433","e21a035d-6a2a-407e-b330-e093a2bfb99c","f1334736-f1b1-40c6-bec1-a4ae056150b3","e3103680-6bce-4643-92a2-1c3e64f82ae0","c21c39df-9551-4779-bf5a-e640ab17eabc","82b21d4b-541d-4f30-8d81-1ee73f8f3859","8e174506-95ea-4fc0-b3a3-cf12a39cf08f","5fe0c1c1-78ef-4855-ab96-20163945139a","efd27f37-6fd6-4ca2-bd35-61fc98a9bf31","b513324b-7495-404d-9377-9fe4a3689f96","94fcc00a-1fdd-45da-bf11-c5a6bb0f02b4","6d52178a-28f4-450d-9291-0ece0bdd7ace","9ed9f5d8-e657-4b49-9c34-71f5de36ded1","7f1f22da-03f8-4354-81c5-b55a2464911a","6bce4139-bdb0-4da6-a4b0-b35f226d1a92","d3393c64-1ccd-41c6-83dd-9d721dbd4882","51cc2394-af40-4195-ae1b-878938239137","d748830b-0566-43ce-bae6-69aed6499c92","fb401abb-8e13-4375-85a1-d45d4228b771","f0712651-e3a2-4ea8-99b2-085d7e0754c3","06f21610-15b7-4301-97e9-2dea7111f4d0","132c2000-aab7-4666-a227-3d8b2d71cdac","2d202c29-fa22-4287-9ad7-35b2747e3def","b80d34e9-56ce-4db4-b2fa-67a6fb6c0e83","15dd170b-5b60-40d9-95b7-315da75544ae","3065c75c-2b37-4e47-97d7-209adc310c66","807fb48c-cfa8-4d1a-a39e-02bc0696b1fa","7e5a0209-5a62-49a1-9943-7f68570ee922","dd25c7b5-710d-4fa8-9f45-6416a77865c9","bcab3ef1-1fae-4b6f-8e8e-519b29cebd89","5b978dcf-a4b2-4356-a7ae-e42635906bdf","6de567fe-0150-4b9c-af07-2d4cb267d804","c2cbe061-91ac-436e-935a-f8bc879ef901","b3914227-dd30-4219-a49d-c326b02b2b51","58cba1a1-7eb1-4aa8-bdaa-39e5b01ac875","98de53f6-556f-45b2-9bb3-7abfa2ca0859","1c6fa583-b8eb-4af8-964c-123defc6ccb8","8681789e-66a6-44d7-8633-90997d43390d","6a4808ea-797d-47d3-a779-adfdf9c72c67","2a72bd90-cfbb-47e3-9329-f4d2b69b7580","7b9c4706-f95b-4d81-a718-37e3e6624a86","be858c34-d126-4ac0-82bb-621a80c77581","63034432-bfbb-4eb4-aa64-43178528224d","01769602-4155-47a6-9a32-7f17eabfc526","8d05fd65-00ad-455c-bc51-27e492fe807c","f90fffa7-815e-4cf4-b6f0-a92cce2bd46e","85de806c-0ad1-48b8-8318-9f076e016fef","50239149-3445-40dc-9388-8a272073902f"],"format":{"page_icon":"🎮","page_cover":"https://images.unsplash.com/photo-1563209259-b2fa97148ce1?ixlib=rb-4.0.3\u0026q=80\u0026fm=jpg\u0026crop=entropy\u0026cs=tinysrgb","page_cover_position":0.5},"created_time":1676050379458,"last_edited_time":1677651659302,"parent_id":"ba8460cf-4781-486e-8976-01358ef4659d","parent_table":"collection","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"4f20ede8-ccf7-4ae1-82e5-819e100dd032":{"role":"reader","value":{"id":"4f20ede8-ccf7-4ae1-82e5-819e100dd032","version":129,"type":"collection_view","view_ids":["d91647c5-6a81-48b1-a1ff-a04529d0ddba","27bc73a3-5779-44e0-b617-2f0d26f5aa2f"],"collection_id":"ba8460cf-4781-486e-8976-01358ef4659d","format":{"collection_pointer":{"id":"ba8460cf-4781-486e-8976-01358ef4659d","table":"collection","spaceId":"4af10338-3e65-4b50-af9f-798d59d5c8f6"},"copied_from_pointer":{"id":"3e3073e9-7aee-481c-b831-765e112ec7b5","table":"block","spaceId":"fde5ac74-eea3-4527-8f00-4482710e1af3"}},"created_time":1675998770865,"last_edited_time":1677136438825,"parent_id":"6246082f-4014-4d06-98ab-59e9840b298a","parent_table":"block","alive":true,"copied_from":"3e3073e9-7aee-481c-b831-765e112ec7b5","created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"6246082f-4014-4d06-98ab-59e9840b298a":{"role":"reader","value":{"id":"6246082f-4014-4d06-98ab-59e9840b298a","version":609,"type":"page","properties":{"title":[["Jang. Inspiration"]]},"content":["651dcdf3-689d-42d2-8497-64f8509d3504","0bd6df0e-6679-499e-b382-c9dc2c597776","f4c89bb7-a90e-41d7-a678-588b3deff765","d5557a1e-de5e-4085-896d-362a19928b69","4f20ede8-ccf7-4ae1-82e5-819e100dd032","ce518f27-4e46-4e98-ac75-13c467c1370c","dfee9c57-3be6-41db-8113-a54eeef675a7","1109cf3f-d8f9-4532-a13e-1af177ad4fdd","5855fe5e-17e2-4f14-8b66-702838bcc734"],"format":{"page_icon":"https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b29e9b03-c79c-4e52-a45e-7228163ba524/compass-circular-tool_(3).png","page_cover":"/images/page-cover/nasa_reduced_gravity_walking_simulator.jpg","copied_from_pointer":{"id":"78754261-97cf-4616-9880-9def95960ebf","table":"block","spaceId":"fde5ac74-eea3-4527-8f00-4482710e1af3"},"page_cover_position":0.5},"permissions":[{"role":"editor","type":"user_permission","user_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb"},{"role":"reader","type":"public_permission","added_timestamp":1675998834095}],"created_time":1675998770872,"last_edited_time":1677392850666,"parent_id":"3a2d56c9-1da9-4a62-b698-f0ad3576c8c1","parent_table":"block","alive":true,"copied_from":"78754261-97cf-4616-9880-9def95960ebf","file_ids":["f70d3dc6-ce97-4be2-9cde-b86606147b41","a2bd3317-78e4-48bc-8d27-9b733175a416","7fae9664-8795-4723-844e-0adecdea62dc","4235c094-2110-4aa6-b058-6b5fe220dbb7","c8194a03-81d0-482d-a7de-f491a6e85f54","7eb95609-c81b-48c1-969e-5ef2f220bc5a","160057d8-120e-4f9f-8c1f-6bcf31a50f15","0cb9278b-708b-4da1-929a-6696aa8cdfa3","3eb9471e-9b71-4bd3-a13d-c33158a442be","b29e9b03-c79c-4e52-a45e-7228163ba524"],"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"561d49ee-83cb-401d-aed3-5697479b918b":{"role":"reader","value":{"id":"561d49ee-83cb-401d-aed3-5697479b918b","version":8,"type":"quote","properties":{"title":[["본 포스트는 “파이썬과 케라스로 배우는 강화학습” 도서의 일곱번째 리뷰 포스트입니다."]]},"created_time":1676050481121,"last_edited_time":1676055529723,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"eeafe760-921c-4b8e-91db-9eb556a98956":{"role":"reader","value":{"id":"eeafe760-921c-4b8e-91db-9eb556a98956","version":7,"type":"text","properties":{"title":[["http://www.yes24.com/Product/Goods/44136413",[["a","http://www.yes24.com/Product/Goods/44136413"]]]]},"created_time":1676055530340,"last_edited_time":1676055530626,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"324cf525-cfac-438d-b537-29ae004aef8d":{"role":"reader","value":{"id":"324cf525-cfac-438d-b537-29ae004aef8d","version":7,"type":"text","created_time":1676050487503,"last_edited_time":1676050487653,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"696d206d-11e9-48b2-b871-f80f69f6f65a":{"role":"reader","value":{"id":"696d206d-11e9-48b2-b871-f80f69f6f65a","version":4,"type":"table_of_contents","format":{"block_color":"gray"},"created_time":1676050494430,"last_edited_time":1676050494432,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"f898b3dd-73b1-4a87-a190-ec2c824d7965":{"role":"reader","value":{"id":"f898b3dd-73b1-4a87-a190-ec2c824d7965","version":4,"type":"text","created_time":1676050487503,"last_edited_time":1676050487877,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"0d933e3f-7aac-41e4-ae05-6d5ef1d99e74":{"role":"reader","value":{"id":"0d933e3f-7aac-41e4-ae05-6d5ef1d99e74","version":9,"type":"sub_header","properties":{"title":[["4장 강화학습 기초 3: 살사(SARSA)와 큐러닝(Q-Learning)",[["b"]]]]},"created_time":1676050481122,"last_edited_time":1676050565474,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"b500cb16-cda0-4f12-a71a-19d475be32ed":{"role":"reader","value":{"id":"b500cb16-cda0-4f12-a71a-19d475be32ed","version":18,"type":"divider","created_time":1676050506744,"last_edited_time":1676050507461,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"80cefca4-1091-4f1b-9125-bb547e6e168f":{"role":"reader","value":{"id":"80cefca4-1091-4f1b-9125-bb547e6e168f","version":2,"type":"text","properties":{"title":[["정책 이터레이션과 가치 이터레이션은 살사로 발전한다. "],["살사",[["c"]]],["부터 강화학습이라 부른다. 각 이터레이션들이 어떻게 살사로 발전하는지, 살사와 같은 강화학습 알고리즘을 통해 에이전트가 어떻게 학습하는지 알아보자."]]},"created_time":1676050481122,"last_edited_time":1676050481137,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"17b16a0a-9307-4ee6-a301-00e062c88853":{"role":"reader","value":{"id":"17b16a0a-9307-4ee6-a301-00e062c88853","version":5,"type":"text","created_time":1676050509685,"last_edited_time":1676050509687,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"4ce63a43-b961-470d-a1b5-2ea0f03608f8":{"role":"reader","value":{"id":"4ce63a43-b961-470d-a1b5-2ea0f03608f8","version":2,"type":"sub_sub_header","properties":{"title":[["살사(SARSA)",[["b"]]]]},"created_time":1676050481122,"last_edited_time":1676050481137,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"c6465428-ec9c-4be4-aeb8-10cb186849cb":{"role":"reader","value":{"id":"c6465428-ec9c-4be4-aeb8-10cb186849cb","version":2,"type":"text","properties":{"title":[["정책 이터레이션은 "],["정책 평가",[["b"]]],[" 와 "],["정책 발전",[["b"]]],[" 을 번갈아 가며 실행하는 과정이다. 벨만 기대 방정식을 이용해 현재의 정책에 대한 참 가치함수를 구하는 것이 정책 평가이며, 구한 가치함수에 따라 정책을 업데이트 하는 것이 정책 발전이다. 이러한 정책 이터레이션을 "],["GPI(Generalized Policy Iteration)",[["c"]]],["이라고 한다. GPI에서는 단 한번만 정책을 평가해서 가치함수를 업데이트하고 바로 정책을 발전하는 과정을 반복한다."]]},"created_time":1676050481122,"last_edited_time":1676050481137,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"4e79865c-2c53-4355-b136-9f2bbb831025":{"role":"reader","value":{"id":"4e79865c-2c53-4355-b136-9f2bbb831025","version":2,"type":"text","properties":{"title":[["GPI에서는 벨만 방정식에 따라 정책을 평가한다. 그 대신 강화학습에서는 "],["몬테카를로 예측",[["b"]]],[" 이나 "],["시간차 예측",[["b"]]],[" 을 사용하여 정책을 평가한다. "],["GPI의 탐욕 정책 발전",[["b"]]],[" 은 주어진 가치함수에 대해 모든 상태에 대한 정책을 얻는 과정이다. "],["시간차 방법",[["b"]]],[" 에서는 타임스텝마다 가치함수를 현재 상태에 대해서만 업데이트 하므로, 모든 상태의 정책을 발전시킬 수 없다."]]},"created_time":1676050481122,"last_edited_time":1676050481137,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"fd539311-758e-4166-acae-3d184b1de06f":{"role":"reader","value":{"id":"fd539311-758e-4166-acae-3d184b1de06f","version":2,"type":"text","properties":{"title":[["때문에 시간차(Temporal-Difference) 방법에서는 가치 이터레이션의 방법을 도입한다. 가치 이터레이션에서는 정책 이터레이션과는 달리 "],["별도의 정책 없이 가치함수에 대해 탐욕적으로 움직일 뿐",[["b"]]],[" 이었고, 시간차 방법에서도 에이전트는 "],["현재 상태에서 가장 큰 가치를 지니는 행동을 선택하는",[["b"]]],[" "],["탐욕 정책",[["c"]]],[" 을 사용한다. 시간차 예측과 탐욕 정책이 합쳐진 것을 "],["시간차 제어(Temporal-difference control)",[["c"]]],["이라고 한다. 이들의 관계를 나타낸 것이 아래의 그림이다."]]},"created_time":1676050481122,"last_edited_time":1676050481137,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"bfe83e55-1b4b-4499-86ed-ceb422cd0fea":{"role":"reader","value":{"id":"bfe83e55-1b4b-4499-86ed-ceb422cd0fea","version":8,"type":"image","properties":{"source":[["https://longshiine.github.io/2021/01/27/Reinforcement-Learning-Basic-6/temporal.png"]],"caption":[["GPI와 시간차 제어의 관계"]]},"format":{"block_width":432,"block_full_width":false,"block_page_width":false},"created_time":1676050481122,"last_edited_time":1676050599453,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"8105c1dd-fd1c-4ae5-a729-7876fe996c2f":{"role":"reader","value":{"id":"8105c1dd-fd1c-4ae5-a729-7876fe996c2f","version":12,"type":"text","properties":{"title":[["탐욕 정책에서 "],["다음 상태의 가치함수를 보고 판단하는 것이 아닌 현재 상태의 큐함수를 보고 판단한다면 환경의 모델을 몰라도 된다.",[["b"]]],[" (GPI의 탐욕 정책 발전에 따르면 "],["⁍",[["e","E_\\pi [R_{t+1}+ \\gamma v_\\pi(S_{t+1})]"]]],["을 계산하기 위해서 상태 변환 확률인  "],["⁍",[["e","P_{ss'}^a"]]],["를 알아야 했다. 행동을 했을 때, 이상한데로 튈 확률이랄까나. 요녀석은 환경의 일부로서 현실에서는 알기가 매우매우 힘든 정보이기 때문에, 이 녀석 없이 행동을 선택하게 하는 것이 필요하다!)시간차 제어에서는 아래의 "],["큐함수를 사용한 탐욕 정책",[["b"]]],[" 을 통해 행동을 선택한다."]]},"created_time":1676050481123,"last_edited_time":1676050636155,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"0e431f2c-c78c-4e81-a0de-d14398457020":{"role":"reader","value":{"id":"0e431f2c-c78c-4e81-a0de-d14398457020","version":9,"type":"equation","properties":{"title":[["\\pi (s) = argmax_{a \\in A} \\\\\\, Q(s,a)"]]},"created_time":1676050648648,"last_edited_time":1676050652790,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"ec8fa191-8784-46f5-a927-cadcfc5492c3":{"role":"reader","value":{"id":"ec8fa191-8784-46f5-a927-cadcfc5492c3","version":2,"type":"text","properties":{"title":[["큐함수에 따라서 행동을 선택하려면 에이전트는 가치함수가 아닌 큐함수의 정보를 알아야 하므로, 업데이트의 대상은 가치함수가 아닌 큐함수가 되어야 한다. 때문에 "],["시간차 제어",[["c"]]],["의 식은 다음과 같다."]]},"created_time":1676050481123,"last_edited_time":1676050481137,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"f48dc1b8-bdf5-4eed-9cf4-b77845a22ed7":{"role":"reader","value":{"id":"f48dc1b8-bdf5-4eed-9cf4-b77845a22ed7","version":9,"type":"equation","properties":{"title":[["Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha (R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t))"]]},"created_time":1676050663717,"last_edited_time":1676050667290,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"8e5203bd-d83f-457d-a0e7-78db4578b702":{"role":"reader","value":{"id":"8e5203bd-d83f-457d-a0e7-78db4578b702","version":7,"type":"text","properties":{"title":[["시간차 제어에서 큐함수를 업데이트 하려면 샘플이 필요하다. 시간차 제어에서는 "],["⁍",[["e","[S_t, A_t,R_{t+1}, S_{t+1}, A_{t+1}]"]]],["을 샘플로 사용한다. 흐름을 한번 살펴보면"]]},"created_time":1676050481124,"last_edited_time":1676050686163,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"21fd1207-e6e7-483a-bc62-b011251d0837":{"role":"reader","value":{"id":"21fd1207-e6e7-483a-bc62-b011251d0837","version":24,"type":"bulleted_list","properties":{"title":[["에이전트는 "],["⁍",[["e","S_t"]]],["에서 탐욕 정책에 따라 "],["⁍",[["e","A_t"]]],["를 선택"]]},"created_time":1676050481124,"last_edited_time":1676050702656,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"2d8766fe-c002-4901-b23d-6277120dac4f":{"role":"reader","value":{"id":"2d8766fe-c002-4901-b23d-6277120dac4f","version":36,"type":"bulleted_list","properties":{"title":[["환경은 "],["⁍",[["e","R_{t+1}"]]],["을 제공하고 다음상태 "],["⁍",[["e","S_{t+1}"]]],["을 제공"]]},"created_time":1676050481124,"last_edited_time":1676050720562,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"962dff01-2460-4a9a-a587-2bbc8e65fb2c":{"role":"reader","value":{"id":"962dff01-2460-4a9a-a587-2bbc8e65fb2c","version":28,"type":"bulleted_list","properties":{"title":[["에이전트는 "],["⁍",[["e","S_{t+1}"]]],["에서 탐욕 정책에 따라 "],["⁍",[["e","A_{t+1}"]]],["을 선택"]]},"created_time":1676050481124,"last_edited_time":1676050738491,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"0eb7799d-6645-4902-a178-b430344860cd":{"role":"reader","value":{"id":"0eb7799d-6645-4902-a178-b430344860cd","version":2,"type":"text","properties":{"title":[["샘플의 형태 때문에 시간차 제어를 다른말로 "],["살사(SARSA)",[["c"]]],["라고 부른다. 살사는 현재 가지고 있는 "],["큐함수",[["b"]]],[" 를 토대로 샘플을 탐욕 정책으로 모으고, 그 샘플로 방문한 큐함수를 업데이트하는 과정을 반복하는 것이다."]]},"created_time":1676050481124,"last_edited_time":1676050481137,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"f1af973a-a99d-497c-885f-3718226cec60":{"role":"reader","value":{"id":"f1af973a-a99d-497c-885f-3718226cec60","version":15,"type":"text","properties":{"title":[["초기의 에이전트에게 탐욕정책은 "],["잘못된 학습으로 가게할 가능성이 크다.",[["b"]]],[" 때문에, 큐함수가 잘못된 값에 수렴하는 것을 막기 위해 에이전트가 충분히 다양한 경험을 하도록 해야하고, 이를 위해 "],["⁍",[["e","\\epsilon"],["b"]]],["-탐욕 정책",[["b"]]],[" 을 사용한다. 간단한 아이디어인데 "],["⁍",[["e","\\epsilon"],["b"]]],["의 확률로 탐욕적이지 않은 행동을 선택하게 하는 것이다."]]},"created_time":1676050481124,"last_edited_time":1676050776530,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"0735987f-2139-43ab-b7bc-9b38c7399699":{"role":"reader","value":{"id":"0735987f-2139-43ab-b7bc-9b38c7399699","version":20,"type":"text","properties":{"title":[["(물론 "],["⁍",[["e","\\epsilon"],["b"]]],["-탐욕 정책은 최적 큐함수를 찾았다 하더라도 "],["⁍",[["e","\\epsilon"],["b"]]],["의 확률로 계속 탐험한다는 한계",[["b"]]],[" 가 있다. 따라서 학습을 진행함에 따라 "],["⁍",[["e","\\epsilon"],["b"]]],["값을 감소시키는 방법을 사용할 수도 있다.)"]]},"created_time":1676050776528,"last_edited_time":1676050795266,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"ed4d4106-6ca7-4328-a5f6-31f962a685d9":{"role":"reader","value":{"id":"ed4d4106-6ca7-4328-a5f6-31f962a685d9","version":12,"type":"equation","properties":{"title":[["\\pi(s) = \\begin{cases}\n\\displaystyle a^* = argmax_{a \\in A} \\\\\\, Q(s,a), \\\\\\,\\\\\\,\\\\\\, 1-\\epsilon의\\\\\\,확률로 \\\\\\\\\n\\displaystyle a \\neq a^*, \\\\\\,\\\\\\,\\\\\\,\\\\\\,\\\\\\, \\epsilon의\\\\\\,확률로\n\\end{cases}"]],"language":[["Markdown"]]},"created_time":1676050481125,"last_edited_time":1676050810358,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"bbf9eee9-974a-4c7b-bb68-284dc67af41f":{"role":"reader","value":{"id":"bbf9eee9-974a-4c7b-bb68-284dc67af41f","version":2,"type":"text","properties":{"title":[["정리하자면, 살사는 간단히 두 단계로 생각하면 된다."]]},"created_time":1676050481125,"last_edited_time":1676050481137,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"6fecf4be-f4da-4799-93a7-137e88767433":{"role":"reader","value":{"id":"6fecf4be-f4da-4799-93a7-137e88767433","version":14,"type":"numbered_list","properties":{"title":[["⁍",[["e","\\epsilon"]]],["-탐욕 정책을 통해 샘플 "],["⁍",[["e","[S_t, A_t,R_{t+1}, S_{t+1}, A_{t+1}]"]]],["을 획득"]]},"created_time":1676050481125,"last_edited_time":1676050844072,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"e21a035d-6a2a-407e-b330-e093a2bfb99c":{"role":"reader","value":{"id":"e21a035d-6a2a-407e-b330-e093a2bfb99c","version":7,"type":"numbered_list","properties":{"title":[["획득한 샘플로 다음 식을 통해 큐함수 "],["⁍",[["e","Q(S_t, A_t)"]]],["를 업데이트"]]},"created_time":1676050481125,"last_edited_time":1676050853293,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"f1334736-f1b1-40c6-bec1-a4ae056150b3":{"role":"reader","value":{"id":"f1334736-f1b1-40c6-bec1-a4ae056150b3","version":8,"type":"equation","properties":{"title":[["Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha (R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t))"]]},"created_time":1676050481125,"last_edited_time":1676050866159,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"e3103680-6bce-4643-92a2-1c3e64f82ae0":{"role":"reader","value":{"id":"e3103680-6bce-4643-92a2-1c3e64f82ae0","version":4,"type":"text","properties":{"title":[["이제 코드를 한번 살펴보자"]]},"created_time":1676050481125,"last_edited_time":1676050862027,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"c21c39df-9551-4779-bf5a-e640ab17eabc":{"role":"reader","value":{"id":"c21c39df-9551-4779-bf5a-e640ab17eabc","version":5,"type":"text","created_time":1676050867854,"last_edited_time":1676050867856,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"82b21d4b-541d-4f30-8d81-1ee73f8f3859":{"role":"reader","value":{"id":"82b21d4b-541d-4f30-8d81-1ee73f8f3859","version":2,"type":"sub_sub_header","properties":{"title":[["살사 코드 설명",[["b"]]]]},"created_time":1676050481125,"last_edited_time":1676050481137,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"8e174506-95ea-4fc0-b3a3-cf12a39cf08f":{"role":"reader","value":{"id":"8e174506-95ea-4fc0-b3a3-cf12a39cf08f","version":13,"type":"code","properties":{"title":[["class SARSAgent:\n  def __init__(self,actions):\n    self.actions = actions # 에이전트가 할 수 있는 행동 [상,하,좌,우]\n    self.step_size = 0.01      # α\n    self.discount_factor = 0.9 # γ\n    self.epsilon = 0.1         # ϵ\n    self.q_table  = defaultdict(lambda: [0.0, 0.0, 0.0, 0.0])"]],"language":[["Python"]]},"created_time":1676050878651,"last_edited_time":1676050882392,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"5fe0c1c1-78ef-4855-ab96-20163945139a":{"role":"reader","value":{"id":"5fe0c1c1-78ef-4855-ab96-20163945139a","version":2,"type":"text","properties":{"title":[["init을 통해 학습에 필요한 변수들을 할당해 주었다. 추가로 "],["SARSAgent",[["b"]]],[" 에 어떤 함수가 필요한지를 알기 위해서는 에이전트가 환경과 어떻게 상호작용하고 학습하는지를 알아야 한다. 에이전트는 다음과 같은 순서로 상호작용한다."]]},"created_time":1676050481126,"last_edited_time":1676050481137,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"efd27f37-6fd6-4ca2-bd35-61fc98a9bf31":{"role":"reader","value":{"id":"efd27f37-6fd6-4ca2-bd35-61fc98a9bf31","version":23,"type":"numbered_list","properties":{"title":[["현재 상태에서 "],["⁍",[["e","\\epsilon"]]],["-탐욕 정책에 따라 행동을 선택"]]},"created_time":1676050481126,"last_edited_time":1676050894122,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"b513324b-7495-404d-9377-9fe4a3689f96":{"role":"reader","value":{"id":"b513324b-7495-404d-9377-9fe4a3689f96","version":2,"type":"numbered_list","properties":{"title":[["선택한 행동으로 환경에서 한 타임스텝을 진행"]]},"created_time":1676050481126,"last_edited_time":1676050481137,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"94fcc00a-1fdd-45da-bf11-c5a6bb0f02b4":{"role":"reader","value":{"id":"94fcc00a-1fdd-45da-bf11-c5a6bb0f02b4","version":2,"type":"numbered_list","properties":{"title":[["환경으로부터 보상과 다음 상태를 받음"]]},"created_time":1676050481126,"last_edited_time":1676050481137,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"6d52178a-28f4-450d-9291-0ece0bdd7ace":{"role":"reader","value":{"id":"6d52178a-28f4-450d-9291-0ece0bdd7ace","version":4,"type":"numbered_list","properties":{"title":[["다음 상태에서 "],["⁍",[["e","\\epsilon"]]],["-탐욕 정책에 따라 다음 행동을 선택"]]},"created_time":1676050481126,"last_edited_time":1676050901534,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"9ed9f5d8-e657-4b49-9c34-71f5de36ded1":{"role":"reader","value":{"id":"9ed9f5d8-e657-4b49-9c34-71f5de36ded1","version":2,"type":"numbered_list","properties":{"title":[["(s,a,r,s’,a’)을 통해 큐함수 업데이트"]]},"created_time":1676050481126,"last_edited_time":1676050481137,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"7f1f22da-03f8-4354-81c5-b55a2464911a":{"role":"reader","value":{"id":"7f1f22da-03f8-4354-81c5-b55a2464911a","version":6,"type":"text","properties":{"title":[["get_action 함수는 "],["⁍",[["e","\\epsilon"]]],["-탐욕 정책에 따라 state를 입력으로 받아 action을 반환한다. q_table에 따라서 탐욕적으로 행동을 선택하며 이때 의 확률을 반영하여 무작위 행동을 반환하기도 한다."]]},"created_time":1676050481126,"last_edited_time":1676050907977,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"6bce4139-bdb0-4da6-a4b0-b35f226d1a92":{"role":"reader","value":{"id":"6bce4139-bdb0-4da6-a4b0-b35f226d1a92","version":13,"type":"code","properties":{"title":[["# 입실론 탐욕 정책에 따라서 행동을 반환\ndef get_action(self, state):\n    if np.random.rand() \u003c self.epsilon:\n        # 무작위 행동 반환\n        action = np.random.choice(self.actions)\n    else:\n        # 큐함수에 따른 행동 반환\n        state = str(state)\n        q_list = self.q_table[state]\n        action = arg_max(q_list)\n    return action"]],"language":[["Python"]]},"created_time":1676050918317,"last_edited_time":1676050921240,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"d3393c64-1ccd-41c6-83dd-9d721dbd4882":{"role":"reader","value":{"id":"d3393c64-1ccd-41c6-83dd-9d721dbd4882","version":2,"type":"text","properties":{"title":[["현재 상태와 다음 상태에서의 행동을 선택해서 샘플(s,a,r,s’,a’)을 얻으면 에이전트는 학습을 진행한다. 즉 아래의 식의 역할을 하는 함수는 learn이며 코드는 다음과 같다.(굉장히 직관적이다!)"]]},"created_time":1676050481126,"last_edited_time":1676050481137,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"51cc2394-af40-4195-ae1b-878938239137":{"role":"reader","value":{"id":"51cc2394-af40-4195-ae1b-878938239137","version":9,"type":"equation","properties":{"title":[["Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha (R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t))"]]},"created_time":1676050933058,"last_edited_time":1676050936629,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"d748830b-0566-43ce-bae6-69aed6499c92":{"role":"reader","value":{"id":"d748830b-0566-43ce-bae6-69aed6499c92","version":11,"type":"code","properties":{"title":[["# \u003cs, a, r, s', a'\u003e의 샘플로부터 큐함수를 업데이트\ndef learn(self, state, action, reward, next_state, next_action):\n    state, next_state = str(state), str(next_state)\n    current_q = self.q_table[state][action]\n    next_state_q = self.q_table[next_state][next_action]\n    td = reward + self.discount_factor * next_state_q - current_q\n    new_q = current_q + self.step_size * td\n    self.q_table[state][action] = new_q"]],"language":[["Python"]]},"created_time":1676050944061,"last_edited_time":1676050948967,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"fb401abb-8e13-4375-85a1-d45d4228b771":{"role":"reader","value":{"id":"fb401abb-8e13-4375-85a1-d45d4228b771","version":2,"type":"text","properties":{"title":[["get_action과 learn 함수를 통해 에이전트는 메인 루프에서 다음과 같이 환경과 상호작용한다."]]},"created_time":1676050481127,"last_edited_time":1676050481137,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"f0712651-e3a2-4ea8-99b2-085d7e0754c3":{"role":"reader","value":{"id":"f0712651-e3a2-4ea8-99b2-085d7e0754c3","version":13,"type":"code","properties":{"title":[["# 행동을 위한 후 다음상태 보상 에피소드의 종료 여부를 받아옴\nnext_state, reward, done = env.step(action)\n# 다음 상태에서의 다음 행동 선택\nnext_action = agent.get_action(next_state)\n# \u003cs,a,r,s',a'\u003e로 큐함수를 업데이트\nagent.learn(state, action, reward, next_state, next_action)\n\nstate = next_state\naction = next_action"]],"language":[["Python"]]},"created_time":1676050957545,"last_edited_time":1676050960296,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"06f21610-15b7-4301-97e9-2dea7111f4d0":{"role":"reader","value":{"id":"06f21610-15b7-4301-97e9-2dea7111f4d0","version":3,"type":"text","created_time":1676050962759,"last_edited_time":1676050962760,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"132c2000-aab7-4666-a227-3d8b2d71cdac":{"role":"reader","value":{"id":"132c2000-aab7-4666-a227-3d8b2d71cdac","version":2,"type":"sub_sub_header","properties":{"title":[["살사의 한계",[["b"]]]]},"created_time":1676050481127,"last_edited_time":1676050481137,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"2d202c29-fa22-4287-9ad7-35b2747e3def":{"role":"reader","value":{"id":"2d202c29-fa22-4287-9ad7-35b2747e3def","version":2,"type":"text","properties":{"title":[["살사에서는 충분한 "],["탐험(Exploration)",[["b"]]],[" 을 하기 위해 "],["-탐욕 정책을 사용했다. 그런데 다음의 경우를 한번 생각해보자."]]},"created_time":1676050481127,"last_edited_time":1676050481137,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"b80d34e9-56ce-4db4-b2fa-67a6fb6c0e83":{"role":"reader","value":{"id":"b80d34e9-56ce-4db4-b2fa-67a6fb6c0e83","version":8,"type":"image","properties":{"source":[["https://longshiine.github.io/2021/01/27/Reinforcement-Learning-Basic-6/limitation.png"]],"caption":[["살사의 학습과정 중 한순간"]]},"format":{"block_width":432,"block_full_width":false,"block_page_width":false},"created_time":1676050481127,"last_edited_time":1676050974003,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"15dd170b-5b60-40d9-95b7-315da75544ae":{"role":"reader","value":{"id":"15dd170b-5b60-40d9-95b7-315da75544ae","version":17,"type":"text","properties":{"title":[["초기 에이전트가 만약 "],["⁍",[["e","s"]]],["에서 "],["⁍",[["e","a"]]],["라는 행동을 하고 다음 행동인 "],["⁍",[["e","a’"]]],["은 탐험을 통해서 가게 되었다고 생각해보자. 그럼 자연스럽게 초기 에이전트는 "],["⁍",[["e","Q(s,a) "]]],["값을 낮출 것이고 이에 따라 "],["⁍",[["e","s"]]],["에서 아래로 이동하는 행동이 안좋다고 판단할 것이다. 결국 에이전트가 특정 state에 "],["갇혀버리는 현상",[["b"]]],[" 이 발생한다. 이렇게 자신이 행동한 대로 학습하는 것을 "],["On-Policy 시간차 제어",[["b"]]],[" 라고 한다. 이러한 딜레마를 해결하기 위해 사용하는 것이 바로 "],["오프폴리시 시간차 제어",[["b"]]],[" , "],["큐러닝",[["c"]]],[" 이다."]]},"created_time":1676050481127,"last_edited_time":1676051000827,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"3065c75c-2b37-4e47-97d7-209adc310c66":{"role":"reader","value":{"id":"3065c75c-2b37-4e47-97d7-209adc310c66","version":5,"type":"text","created_time":1676051006944,"last_edited_time":1676051006949,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"807fb48c-cfa8-4d1a-a39e-02bc0696b1fa":{"role":"reader","value":{"id":"807fb48c-cfa8-4d1a-a39e-02bc0696b1fa","version":2,"type":"sub_sub_header","properties":{"title":[["큐러닝",[["b"]]]]},"created_time":1676050481127,"last_edited_time":1676050481137,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"7e5a0209-5a62-49a1-9943-7f68570ee922":{"role":"reader","value":{"id":"7e5a0209-5a62-49a1-9943-7f68570ee922","version":2,"type":"text","properties":{"title":[["큐러닝의 아이디어는 간단하다. "],["오프폴리시",[["b"]]],[" 의 말 그대로 현재 행동하는 정책과는 독립적으로 학습한다는 것이다. 즉, "],["행동하는 정책과 학습하는 정책을 따로 분리",[["b"]]],[" 한다. 이게 무슨 말일까? 예시로서 이해해보자."]]},"created_time":1676050481127,"last_edited_time":1676050481137,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"dd25c7b5-710d-4fa8-9f45-6416a77865c9":{"role":"reader","value":{"id":"dd25c7b5-710d-4fa8-9f45-6416a77865c9","version":124,"type":"text","properties":{"title":[["에이전트가 현재 상황 "],["⁍",[["e","s"]]],["에서 행동 "],["⁍",[["e","a"]]],["를 "],["⁍",[["e","\\epsilon"]]],["-탐욕 정책에 따라 선택했다고 하자. 그러면 에이전트는 환경으로부터 보상 "],["⁍",[["e","r"]]],["을 받고 다음 상태 "],["⁍",[["e","s’"]]],["을 받는다. 여기까지는 살사와 동일하다. 하지만 살사에서는 다음 상태 에서 또다시 "],["⁍",[["e","\\epsilon"]]],["-탐욕 정책에 따라 다음 행동을 선택한 후에 그것을 학습에 샘플로 사용한다. "],["큐러닝",[["b"]]],[" 에서는 에이전트가 다음 상태 "],["⁍",[["e","s’"]]],["을 알게 되면 "],["그 상태(",[["b"]]],["⁍",[["e","s’"],["b"]]],[")에서 가장 큰 큐함수를 현재 큐함수의 업데이트에 사용한다.",[["b"]]],[" 살사의 학습과정과 다르게 큐러닝은 아래와 같이 학습한다."]]},"created_time":1676050481127,"last_edited_time":1676051070088,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"bcab3ef1-1fae-4b6f-8e8e-519b29cebd89":{"role":"reader","value":{"id":"bcab3ef1-1fae-4b6f-8e8e-519b29cebd89","version":10,"type":"image","properties":{"source":[["https://longshiine.github.io/2021/01/27/Reinforcement-Learning-Basic-6/qlearning.png"]],"caption":[["큐러닝의 학습과정 중 한순간"]]},"format":{"block_width":384,"block_full_width":false,"block_page_width":false},"created_time":1676050481128,"last_edited_time":1676051088027,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"5b978dcf-a4b2-4356-a7ae-e42635906bdf":{"role":"reader","value":{"id":"5b978dcf-a4b2-4356-a7ae-e42635906bdf","version":27,"type":"text","properties":{"title":[["큐러닝은 실제 다음 상태 "],["⁍",[["e","s’"]]],["에서 다음 행동을 해보는 것이 아니라 다음 상태 "],["⁍",[["e","s’"]]],["에서 가장 큰 큐함수를 가지고 업데이트 하는 것이다. 자세히 살펴보면 벨만 최적 방정식과 비슷하다는 생각이 들 것이다."]]},"created_time":1676050481128,"last_edited_time":1676051126312,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"6de567fe-0150-4b9c-af07-2d4cb267d804":{"role":"reader","value":{"id":"6de567fe-0150-4b9c-af07-2d4cb267d804","version":14,"type":"equation","properties":{"title":[["Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha (R_{t+1} + \\gamma max_{a'} Q(S_{t+1}, a') - Q(S_t, A_t))"]]},"created_time":1676051117881,"last_edited_time":1676051132129,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"c2cbe061-91ac-436e-935a-f8bc879ef901":{"role":"reader","value":{"id":"c2cbe061-91ac-436e-935a-f8bc879ef901","version":19,"type":"text","properties":{"title":[["벨만 최적 방정식은 아래의 수식과 같은데 큐러닝에서 보상 "],["⁍",[["e","R_{t+1}"]]],["은 실제 에이전트가 환경에게서 받는 값이므로 기댓값을 빼면 동일하다."]]},"created_time":1676050481128,"last_edited_time":1676051143347,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"b3914227-dd30-4219-a49d-c326b02b2b51":{"role":"reader","value":{"id":"b3914227-dd30-4219-a49d-c326b02b2b51","version":9,"type":"equation","properties":{"title":[["q^"],["(s,a) = E[R_{t+1} + \\gamma max_{a'} q^",[["i"]]],["(S_{t+1},a') | S_t=s, A_t=a]"]]},"created_time":1676051147726,"last_edited_time":1676051157351,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"58cba1a1-7eb1-4aa8-bdaa-39e5b01ac875":{"role":"reader","value":{"id":"58cba1a1-7eb1-4aa8-bdaa-39e5b01ac875","version":2,"type":"bulleted_list","properties":{"title":[["벨만 기대 방정식",[["c"]]],[" –\u003e "],["정책 이터레이션",[["c"]]],[" –\u003e "],["살사",[["c"]]]]},"created_time":1676050481128,"last_edited_time":1676050481137,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"98de53f6-556f-45b2-9bb3-7abfa2ca0859":{"role":"reader","value":{"id":"98de53f6-556f-45b2-9bb3-7abfa2ca0859","version":2,"type":"bulleted_list","properties":{"title":[["벨만 최적 방정식",[["c"]]],[" –\u003e "],["가치 이터레이션",[["c"]]],[" –\u003e "],["큐러닝",[["c"]]]]},"created_time":1676050481128,"last_edited_time":1676050481137,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"1c6fa583-b8eb-4af8-964c-123defc6ccb8":{"role":"reader","value":{"id":"1c6fa583-b8eb-4af8-964c-123defc6ccb8","version":2,"type":"text","properties":{"title":[["큐러닝은 샘플로서 [s,a,r,s’]을 사용하며 실제 환경에서 행동을 하는 정책과 큐함수를 업데이트할 때 사용하는 정책이 다르기 때문에 큐러닝을 "],["오프폴리시",[["c"]]],["라고 한다. 다른 오프폴리시 강화학습과 달리 큐함수가 간단하기 떄문에 이후에 많은 강화학습 알고리즘의 토대가 되었다."]]},"created_time":1676050481128,"last_edited_time":1676050481137,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"8681789e-66a6-44d7-8633-90997d43390d":{"role":"reader","value":{"id":"8681789e-66a6-44d7-8633-90997d43390d","version":2,"type":"sub_sub_header","properties":{"title":[["큐러닝 코드 설명",[["b"]]]]},"created_time":1676050481128,"last_edited_time":1676050481137,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"6a4808ea-797d-47d3-a779-adfdf9c72c67":{"role":"reader","value":{"id":"6a4808ea-797d-47d3-a779-adfdf9c72c67","version":2,"type":"text","properties":{"title":[["큐러닝 코드에서 살사 코드에서와 다른 점은 에이전트가 샘플을 가지고 학습하는 부분이다."]]},"created_time":1676050481128,"last_edited_time":1676050481137,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"2a72bd90-cfbb-47e3-9329-f4d2b69b7580":{"role":"reader","value":{"id":"2a72bd90-cfbb-47e3-9329-f4d2b69b7580","version":13,"type":"code","properties":{"title":[["# \u003cs, a, r, s'\u003e 샘플로부터 큐함수 업데이트\n    def learn(self, state, action, reward, next_state):\n        state, next_state = str(state), str(next_state)\n        q_1 = self.q_table[state][action]\n        # 벨만 최적 방정식을 사용한 큐함수의 업데이트\n        q_2 = reward + self.discount_factor * max(self.q_table[next_state])\n        self.q_table[state][action] += self.step_size * (q_2 - q_1)"]],"language":[["Python"]]},"created_time":1676051168965,"last_edited_time":1676051171931,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"7b9c4706-f95b-4d81-a718-37e3e6624a86":{"role":"reader","value":{"id":"7b9c4706-f95b-4d81-a718-37e3e6624a86","version":2,"type":"text","properties":{"title":[["learn 코드는 위에서 살펴본 큐러닝의 업데이트 식을 구현한 것이다. self.q_table[next_state]에서 max 값을 업데이트에 사용하기 때문에 오프폴리시가 됩니다. 또한 max값을 취하면 되기 때문에 다음 상태에서의 행동을 알 필요가 없다."]]},"created_time":1676050481128,"last_edited_time":1676050481137,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"be858c34-d126-4ac0-82bb-621a80c77581":{"role":"reader","value":{"id":"be858c34-d126-4ac0-82bb-621a80c77581","version":2,"type":"text","properties":{"title":[["살사와 큐러닝의 차이는 "],["온폴리시와 오프폴리시",[["b"]]],[" 의 차이라고 볼 수 있다. 온폴리시인 살사는 "],["지속적인 탐험",[["b"]]],[" 으로인해 그리드월드 예제에서 왼쪽 위에 갇히곤 하지만 큐러닝은 현재 행동하는 정책과는 독립적으로 학습을 진행하기 때문에 갇히지 않고 벗어나는 정책을 학습할 수 있다."]]},"created_time":1676050481128,"last_edited_time":1676050481137,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"63034432-bfbb-4eb4-aa64-43178528224d":{"role":"reader","value":{"id":"63034432-bfbb-4eb4-aa64-43178528224d","version":5,"type":"text","created_time":1676051290592,"last_edited_time":1676051290595,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"01769602-4155-47a6-9a32-7f17eabfc526":{"role":"reader","value":{"id":"01769602-4155-47a6-9a32-7f17eabfc526","version":2,"type":"sub_sub_header","properties":{"title":[["정리",[["b"]]]]},"created_time":1676050481128,"last_edited_time":1676050481137,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"8d05fd65-00ad-455c-bc51-27e492fe807c":{"role":"reader","value":{"id":"8d05fd65-00ad-455c-bc51-27e492fe807c","version":4,"type":"bulleted_list","properties":{"title":[["몬테카를로 예측",[["c"]]],[": 기댓값을 샘플링을 통한 평균으로 대체"]]},"content":["e0ecf118-61e8-41b3-b11e-6c7993107180"],"created_time":1676050481129,"last_edited_time":1676051181763,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"e0ecf118-61e8-41b3-b11e-6c7993107180":{"role":"reader","value":{"id":"e0ecf118-61e8-41b3-b11e-6c7993107180","version":14,"type":"equation","properties":{"title":[["V(s) \\leftarrow V(s) + \\alpha (G(s) - V(s))"]]},"created_time":1676051181439,"last_edited_time":1676051191229,"parent_id":"8d05fd65-00ad-455c-bc51-27e492fe807c","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"f90fffa7-815e-4cf4-b6f0-a92cce2bd46e":{"role":"reader","value":{"id":"f90fffa7-815e-4cf4-b6f0-a92cce2bd46e","version":4,"type":"bulleted_list","properties":{"title":[["시간차 예측",[["c"]]],[": 몬테카를로 예측과는 달리 타임스텝마다 큐함수를 업데이트"]]},"content":["6237bfaa-dae8-4412-9f0a-41c2dbf60f4f"],"created_time":1676050481129,"last_edited_time":1676051193995,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"6237bfaa-dae8-4412-9f0a-41c2dbf60f4f":{"role":"reader","value":{"id":"6237bfaa-dae8-4412-9f0a-41c2dbf60f4f","version":14,"type":"equation","properties":{"title":[["V(S_t) \\leftarrow V(S_t) + \\alpha (R_{t+1}+ \\gamma V(S_{t+1}) - V(S_t))"]]},"created_time":1676051193860,"last_edited_time":1676051203596,"parent_id":"f90fffa7-815e-4cf4-b6f0-a92cce2bd46e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"85de806c-0ad1-48b8-8318-9f076e016fef":{"role":"reader","value":{"id":"85de806c-0ad1-48b8-8318-9f076e016fef","version":4,"type":"bulleted_list","properties":{"title":[["살사(SARSA)",[["c"]]],[": 강화학습 제어에서 큐함수를 사용하며 하나의 샘플로 (s,a,r,s’,a’)을 사용하는 "],["시간차 제어",[["b"]]]]},"content":["48f7204a-beba-4551-8922-09a8d7dc1d68"],"created_time":1676050481129,"last_edited_time":1676051205802,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"48f7204a-beba-4551-8922-09a8d7dc1d68":{"role":"reader","value":{"id":"48f7204a-beba-4551-8922-09a8d7dc1d68","version":14,"type":"equation","properties":{"title":[["Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha (R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t))"]]},"created_time":1676051205457,"last_edited_time":1676051218396,"parent_id":"85de806c-0ad1-48b8-8318-9f076e016fef","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"50239149-3445-40dc-9388-8a272073902f":{"role":"reader","value":{"id":"50239149-3445-40dc-9388-8a272073902f","version":29,"type":"bulleted_list","properties":{"title":[["큐러닝(Q-learning)",[["c"]]],[": 오프폴리시 강화학습으로서 행동선택은 "],["⁍",[["e","\\epsilon"]]],["-"],["탐욕 정책",[["b"]]],[" , 큐함수의 업데이트에는 "],["벨만 최적 방정식",[["b"]]],[" 사용"]]},"content":["ca595ed5-f1ce-4e75-a6e8-fd8e905f8413"],"created_time":1676050481129,"last_edited_time":1676051241100,"parent_id":"60a262e5-29a3-44e9-a757-cf3312bd21b5","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"ca595ed5-f1ce-4e75-a6e8-fd8e905f8413":{"role":"reader","value":{"id":"ca595ed5-f1ce-4e75-a6e8-fd8e905f8413","version":14,"type":"equation","properties":{"title":[["Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha (R_{t+1} + \\gamma max_{a'} Q(S_{t+1}, a') - Q(S_t, A_t))"]]},"created_time":1676050481129,"last_edited_time":1676051233630,"parent_id":"50239149-3445-40dc-9388-8a272073902f","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"3d335e41-f00a-45df-8432-34a266c7566a":{"role":"reader","value":{"id":"3d335e41-f00a-45df-8432-34a266c7566a","version":480,"type":"page","properties":{"title":[["About"]]},"content":["e1067486-5fb5-4dc3-9415-54abc63fc3a5","ab253a7b-1dec-4620-8c4e-e99f94e1f6c9","460c887f-cc3d-4351-8671-7cfbc6d3bc1d","c62b8a9d-e8cb-4c5b-be5a-48412e956c95","abe79af7-1342-438a-9a59-2d0b1ce59007","88d551b8-6b87-4540-89b9-3619a8fe4059","ae31ce74-317a-45ae-9dd3-16c14e764361","78f83c2a-05c2-4033-bcd8-945d700b2952","fd280029-9a4c-47fe-8e3c-ce8d3f146671","6a0d0c32-f1bc-4277-a128-33da8613ede4","2bcccf09-dccf-4896-8819-255cf75aabaf","d84b4bd5-3b6b-45dd-a807-5fea62c5213b","8b7fb4d3-0195-4f61-a0eb-330792f84053","e9394173-e197-4fad-a1db-ac74fdba3738","cb29948c-7258-4b05-9bfa-25bf19ad0946","a4e4243c-25aa-480d-a9ee-0af2230c7d76","172e2e25-e319-4766-ad38-02fc168994cd","6c428956-d84e-4855-a998-7a79f1fc8ea5","90f1cecf-6258-4b54-b0ee-26ee4d950872","3eec82e9-0211-4271-8c30-1dbc86ab3be1","8ce5f430-8c4c-4e8f-b6a4-23396731b27d","aeb9d85e-fffa-4c34-b253-cb0499f047a2","3d37f4c7-8064-4792-a863-234d8b4d05a6","77daa708-adbc-4092-b7f7-9e3c8a988049","26e3aca8-3e09-4a37-8c92-3c95cf579bbb"],"format":{"page_icon":"https://s3-us-west-2.amazonaws.com/secure.notion-static.com/fc853118-df50-43ed-96d9-0711493d5e25/jang_inspiration_logo.png","page_cover":"/images/page-cover/nasa_reduced_gravity_walking_simulator.jpg","copied_from_pointer":{"id":"f1199d37-579b-41cb-abfc-0b5174f4256a","table":"block","spaceId":"fde5ac74-eea3-4527-8f00-4482710e1af3"},"page_cover_position":0.5},"created_time":1675998770866,"last_edited_time":1677390731360,"parent_id":"d83b5165-627c-41b3-82f0-f1cbf904e176","parent_table":"block","alive":true,"copied_from":"f1199d37-579b-41cb-abfc-0b5174f4256a","file_ids":["c8028bee-f4c7-4736-8c36-fffcab5d977e","9407e769-d877-4de9-bbaa-9e5626d971ed","07d5a5b4-de2a-4322-bfcc-c4ade3a63b86","fc853118-df50-43ed-96d9-0711493d5e25"],"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"d83b5165-627c-41b3-82f0-f1cbf904e176":{"role":"reader","value":{"id":"d83b5165-627c-41b3-82f0-f1cbf904e176","version":17,"type":"column","content":["e1fe2c9c-6eb8-412c-9874-a8ac2fce9ed8","3d335e41-f00a-45df-8432-34a266c7566a"],"format":{"column_ratio":0.25},"created_time":1676020529237,"last_edited_time":1676091451534,"parent_id":"1109cf3f-d8f9-4532-a13e-1af177ad4fdd","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"1109cf3f-d8f9-4532-a13e-1af177ad4fdd":{"role":"reader","value":{"id":"1109cf3f-d8f9-4532-a13e-1af177ad4fdd","version":13,"type":"column_list","content":["91ac285d-9a69-4ec4-96f6-9b046a15647c","028efbb4-417e-4742-8474-dd7a2ddeb8ee","d63c036c-b99a-4aa9-a068-87abc40d37a6","d83b5165-627c-41b3-82f0-f1cbf904e176"],"format":{"block_width":720,"block_full_width":false,"block_page_width":true},"created_time":1676020365844,"last_edited_time":1676091451534,"parent_id":"6246082f-4014-4d06-98ab-59e9840b298a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"e1067486-5fb5-4dc3-9415-54abc63fc3a5":{"role":"reader","value":{"id":"e1067486-5fb5-4dc3-9415-54abc63fc3a5","version":86,"type":"text","properties":{"title":[["Instagram",[["h","blue_background"],["a","https://www.instagram.com/jang.inspiration/"]]],[" • "],["GitHub",[["h","teal_background"],["a","https://github.com/longshiine"]]],[" • "],["LinkedIn",[["h","pink_background"],["a","https://www.linkedin.com/in/jangyeong-kim-b7924422a/"]]]]},"format":{"copied_from_pointer":{"id":"0ab4ae84-463a-4b31-9430-f4ed513c5fc2","table":"block","spaceId":"fde5ac74-eea3-4527-8f00-4482710e1af3"}},"created_time":1675998770872,"last_edited_time":1676960330564,"parent_id":"3d335e41-f00a-45df-8432-34a266c7566a","parent_table":"block","alive":true,"copied_from":"0ab4ae84-463a-4b31-9430-f4ed513c5fc2","created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}}},"collection":{"ba8460cf-4781-486e-8976-01358ef4659d":{"role":"reader","value":{"id":"ba8460cf-4781-486e-8976-01358ef4659d","version":117,"schema":{";KhU":{"name":"Last Updated","type":"last_edited_time"},"==~K":{"name":"Public","type":"checkbox"},"=bhc":{"name":"Curated","type":"checkbox"},"AfoN":{"name":"Category","type":"select","options":[{"id":"20579d4b-5ad0-469e-8946-2560e458bb81","color":"orange","value":"강화학습"},{"id":"b78b3694-0089-4efb-802c-c288e6190037","color":"green","value":"알고리즘"},{"id":"34c6aef1-b3b3-490e-9369-b5a385f7da4e","color":"blue","value":"딥러닝"},{"id":"67465999-8b72-4fe4-92ce-db5a812b880a","color":"brown","value":"공학수학"},{"id":"8b385c1c-8e95-4a02-b009-6215a718ebef","color":"pink","value":"글쓰기"},{"id":"435e1850-8e40-4b73-8496-0a3f694b6aeb","color":"purple","value":"스타트업"},{"id":"2b34c718-9490-4d2c-8c1d-949ea1a5010c","color":"gray","value":"독서"}]},"BN]P":{"name":"Tags","type":"multi_select","options":[{"id":"210cfb45-7eae-44e5-83dc-dba99aa3a853","color":"green","value":"Node.js"},{"id":"02b16a55-92ee-455d-9449-5e5e0a67cd04","color":"brown","value":"Computer Science"},{"id":"7993ec69-9767-4b84-adb9-5f1c907a6c77","color":"blue","value":"React.js"},{"id":"264c4a74-71d6-4015-bc91-5742970abd89","color":"yellow","value":"OSS"},{"id":"da38c5e1-f969-4abe-b28f-389b37aa22e5","color":"pink","value":"Startups"},{"id":"ceb7f269-10b7-49a9-bd4e-ba99533dee7b","color":"red","value":"Career"},{"id":"46b1d846-537f-43b5-a18c-298386278e63","color":"default","value":"Video"},{"id":"e485cc4c-e0de-4363-b442-22a0f6f588a9","color":"orange","value":"Saasify"},{"id":"5b6ee85e-ba0a-4c12-8e27-00e5b868939e","color":"gray","value":"SaaS"},{"id":"01c8627e-ae07-4f7b-b248-f877f88c8c4f","color":"purple","value":"Web Dev"},{"id":"136170f8-fe43-466b-b790-087508e8bc27","color":"blue","value":"Software Development"},{"id":"77f16ef0-4622-4f47-8f39-f5913a96421d","color":"pink","value":"Projects"},{"id":"aba9d5d2-c4f2-40ad-9f37-db6211536f92","color":"blue","value":"App Dev"},{"id":"84f76e57-8d2d-4078-bb7d-cf85520ba75c","color":"orange","value":"Lifestyle"},{"id":"38b5979d-877b-4cf4-920b-8b5da5ce9ba1","color":"yellow","value":"Thought Experiments"},{"id":"452e3943-dbcc-4d48-95fc-20d5d6339ddd","color":"orange","value":"Research"},{"id":"534e3ab2-fe89-487b-9784-71d76d5396a1","color":"purple","value":"Passion Economy"},{"id":"aa48c5a8-cfe3-4195-bfaa-25a72353299a","color":"yellow","value":"Tech"},{"id":"ff6898c4-f3f1-4024-b6a9-1cb871133744","color":"blue","value":"Creator Economy"},{"id":"ec59ba49-f2d4-4317-9a0e-3c7fb0bc60b8","color":"green","value":"Crypto"},{"id":"3fc961e4-4408-4d65-b750-7c136977ce61","color":"gray","value":"Deep Learning"},{"id":"a1419788-3341-407e-8df0-cd3d1f4ff636","color":"brown","value":"Gradient Vanishing"},{"id":"50ebe91f-e808-4056-b8ec-3b0bae45d464","color":"yellow","value":"Convolution Layer"},{"id":"ae8b3ff9-00e0-4d25-8d8d-e7389fc7fc8c","color":"orange","value":"Dot Product"},{"id":"8f6c2d82-17e0-4fb1-82eb-a6047ec75d02","color":"orange","value":"Vector"},{"id":"96ed8c32-a637-4b4f-ba99-3b397d04435a","color":"red","value":"Auction Theory"},{"id":"d4090af3-0dfb-480d-9503-0d0328774c16","color":"pink","value":"Reinforcement Learning"},{"id":"6fbb9d5c-c0f4-4791-8397-6e89c63e0c82","color":"yellow","value":"MDP"},{"id":"92885d4f-2302-43ac-87b0-e9e1c7a8cd8b","color":"yellow","value":"Introduction"},{"id":"f5ee4f74-bef9-4701-9d7d-1b9e7068d702","color":"blue","value":"Value Function"},{"id":"a575fa70-0999-44a3-b96c-262d57984b63","color":"green","value":"bellman equation"},{"id":"807f640e-1abc-476a-8300-1fe0ec0a15a3","color":"gray","value":"Grid World"},{"id":"e959edcb-aeb9-4a66-91e9-7677c68a85a8","color":"purple","value":"Dynamic Programming"},{"id":"3c5ba2f8-7ec5-4d9c-89b8-2ae975f4740a","color":"default","value":"Policy Iteration"},{"id":"b79852f3-5c3c-4de7-8d6d-731673caae40","color":"green","value":"Value Iteration"},{"id":"c2bd765b-351d-4a6d-a75d-94e8c476a180","color":"red","value":"Policy Evaluation"},{"id":"4a911c23-ca00-4f29-ad2f-463eb166f8f2","color":"brown","value":"SARSA"},{"id":"563d1a3e-095d-4ad8-bb7e-cc8c8195a9c1","color":"green","value":"Q-Learning"},{"id":"9e1bb3b2-a545-4e9d-b1eb-c47d2db8f4a8","color":"gray","value":"Writing"},{"id":"fcaf8148-917c-44bd-8ca1-fce87ba28f55","color":"blue","value":"Nepal"},{"id":"d93da022-471d-4d13-8dbc-051aa3e4639f","color":"orange","value":"Travel"},{"id":"f20c5e6f-58fe-4208-baa3-20704f665d21","color":"purple","value":"Algorithm"},{"id":"447d9963-8cfc-4c4c-9adb-88f0dc85f249","color":"red","value":"Python"},{"id":"633d7256-ffbf-4e5b-9912-906130d85250","color":"yellow","value":"Big-O"},{"id":"1a1af600-6b9b-4b8e-9483-c038712cda7c","color":"default","value":"String"},{"id":"b24d85c4-b7af-411c-905f-c9ae68eab612","color":"pink","value":"Array"},{"id":"87516a34-7662-4a77-a219-149777b960dc","color":"red","value":"LinkedList"},{"id":"d63330f6-ba17-4b9c-b370-579f4c99358a","color":"blue","value":"Stack"},{"id":"04eff27e-18a0-4883-8915-6ba26059106d","color":"yellow","value":"Queue"},{"id":"328cf039-a0b4-4b5c-90f7-be587808a1dc","color":"orange","value":"Deque"},{"id":"3364095f-3a21-47c6-8460-16ddc7f02c13","color":"brown","value":"HashTable"},{"id":"64841489-db5b-45ba-972a-7af2f53f41c7","color":"yellow","value":"Graph"},{"id":"b6dea43b-7633-4290-91f0-9761023838b0","color":"purple","value":"Tree"},{"id":"9dc575d0-d0d5-4282-8430-aae294857404","color":"gray","value":"Heap"},{"id":"edc331ab-bf6d-4985-9ecc-6884de83e8fc","color":"brown","value":"Trie"},{"id":"f2fdcca6-c69d-4b53-ab23-665ca7a223b9","color":"yellow","value":"Sort"},{"id":"5e02ca54-b743-4501-9776-62e030442114","color":"default","value":"BinarySearch"},{"id":"9a317b05-c2df-41ff-bf3f-32c39b5c451b","color":"red","value":"Greedy"},{"id":"536f3cec-7932-4444-b8c7-154672ad5fe6","color":"pink","value":"DivideAndConquer"},{"id":"1fe05376-d59a-4e4a-8596-fcd70f194621","color":"orange","value":"Basic"},{"id":"6fdde3f1-6554-4e54-ba5b-af1fc95b0ce4","color":"green","value":"Linear"},{"id":"28fc1aec-d1dd-44bb-a3ab-78ebc6f74837","color":"yellow","value":"NonLinear"},{"id":"e74e9521-68b1-4c11-ab42-a034a44dfd24","color":"blue","value":"알고리즘"},{"id":"42f167a6-7e2e-4d8e-854d-426689132f08","color":"brown","value":"Amazon"},{"id":"8aefb99d-7b6b-453b-941f-e114e32c6438","color":"blue","value":"book"},{"id":"d5083966-d1d3-4116-a615-5107104890c2","color":"pink","value":"Business"},{"id":"df607bdb-4379-4273-a45e-e52d436aef0d","color":"red","value":"Diffusion Model"},{"id":"da21253b-a1e7-4fe8-9ed3-6073a347c457","color":"orange","value":"Paper Review"}]},"NVm^":{"name":"Slug","type":"text"},"QSi`":{"name":"Series","type":"checkbox"},"a\u003cql":{"name":"Published","type":"date"},"jhf;":{"name":"Tweet","type":"text"},"nAX{":{"name":"Created","type":"created_time"},"}nqi":{"name":"Author","type":"text"},"~]S\u003c":{"name":"Description","type":"text"},"title":{"name":"Name","type":"title"}},"format":{"copied_from_pointer":{"id":"e5fdcb8e-6e29-4bc9-828d-263749307808","table":"collection","spaceId":"fde5ac74-eea3-4527-8f00-4482710e1af3"},"property_visibility":[{"property":"AfoN","visibility":"hide_if_empty"},{"property":"BN]P","visibility":"hide_if_empty"},{"property":"a\u003cql","visibility":"hide_if_empty"},{"property":"}nqi","visibility":"hide_if_empty"},{"property":"~]S\u003c","visibility":"hide"},{"property":"==~K","visibility":"hide"},{"property":"jhf;","visibility":"hide"},{"property":"=bhc","visibility":"hide"},{"property":"NVm^","visibility":"hide"},{"property":"nAX{","visibility":"hide"},{"property":";KhU","visibility":"hide"},{"property":"QSi`","visibility":"hide"}],"collection_page_properties":[{"visible":false,"property":"AfoN"},{"visible":true,"property":"BN]P"},{"visible":true,"property":"a\u003cql"},{"visible":false,"property":"}nqi"},{"visible":true,"property":"~]S\u003c"},{"visible":true,"property":"==~K"},{"visible":true,"property":"jhf;"},{"visible":false,"property":"=bhc"},{"visible":false,"property":"NVm^"},{"visible":true,"property":"nAX{"},{"visible":false,"property":";KhU"}]},"parent_id":"4f20ede8-ccf7-4ae1-82e5-819e100dd032","parent_table":"block","alive":true,"copied_from":"e5fdcb8e-6e29-4bc9-828d-263749307808","template_pages":["cacb6437-50cd-4a3d-9d43-58962f75e40b"],"migrated":true,"space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6","deleted_schema":{":nQy":{"name":"Curating","type":"text"}}}}},"collection_view":{"d91647c5-6a81-48b1-a1ff-a04529d0ddba":{"role":"reader","value":{"id":"d91647c5-6a81-48b1-a1ff-a04529d0ddba","version":33,"type":"gallery","name":"Gallery view","format":{"gallery_cover":{"type":"page_cover"},"gallery_cover_size":"medium","gallery_properties":[{"visible":false,"property":"AfoN"},{"visible":false,"property":"jhf;"},{"visible":false,"property":"NVm^"},{"visible":false,"property":"==~K"},{"visible":false,"property":";KhU"},{"visible":false,"property":"=bhc"},{"visible":false,"property":"}nqi"},{"visible":false,"property":"nAX{"},{"visible":false,"property":"BN]P"},{"visible":true,"property":"title"},{"visible":true,"property":"~]S\u003c"},{"visible":true,"property":"a\u003cql"}],"gallery_cover_aspect":"cover","hide_linked_collection_name":true,"inline_collection_first_load_limit":{"type":"load_limit","limit":10}},"parent_id":"4f20ede8-ccf7-4ae1-82e5-819e100dd032","parent_table":"block","alive":true,"page_sort":["e3df5b79-eb34-4bec-a82f-628699f43852","7a8e9c32-39af-4bca-a26e-3f8f1c9a1762","7864e0d8-d646-4df5-a4ae-057371b7559d","36a57bd6-d63a-40f0-a0a2-6ba247885fab","c2994f3d-696a-4015-b7d1-2b71883562ad","58fc3d75-92a2-44e9-ab0d-84cb61086232","30ac6075-b07c-41fa-b684-15c16f1134ba","4c804cac-3119-4cc3-a621-a82040d9a0db","6ce6c0ba-0d6f-40b3-b7d4-afe8ca715c73","64158711-b8f3-4c3d-ba2f-3999059c5581","0c679f04-0865-4142-a65b-28668aa9daea","54fa8c94-ead7-4b14-b068-174936adb37f","ab2e2675-7daf-464f-a2c4-563136630232","ef52bf7e-8726-4804-a0d5-e9b5f7919284","460f888e-b262-4f14-a39f-93ae0090bb9d","eff2e07b-ee44-45e0-b46a-622deb611d0b","c94f3f08-d65f-4f7d-a409-72c353f9786d","daac7673-f388-475e-8f43-a345dc350bef","6ff46924-9cc3-4445-badc-28e7daa5fa5a","c114dc9f-b09a-4e30-9c0d-5ace92e6a8e3","51b78f8b-69ff-4a46-9ec6-74f0d5400d3e","06adcb1d-3acf-4266-a8d9-6cb0d3c439ce","22c49483-380f-4b94-be90-b437b7cbab52","45a3d677-b3df-4cc1-ac40-7e0ab214aeef","7a177274-38ce-4f03-b3f6-06e20b0ba4f8","15aa9135-7cca-40c7-b656-ba4457524924","599373d0-99e2-4c28-89de-d273b43aca5c","48756e2f-e604-4421-9b34-be2c90e20589","b94b3e79-f161-45a8-bf8e-5315f85dca99","3e71fe2e-c827-4fc6-b610-7d71147ae4a7","298f8e1a-a9f2-4274-b5f1-7279f7bc4e5b","c7f629b2-6f4e-43fe-8720-4a3a10d4e651","b1fded7b-ceea-46d0-89e2-32010807d35c","bd99ffd5-ce6d-4418-be0e-6db2cd8ae070","5e852682-71e3-45b9-8056-d40e972563fd","1db23f4a-8d7e-4445-b241-a8e4be5bd02a","70df362d-5c3a-4d2c-a3da-ef27e1f207e3","dae737f3-5216-4c50-b708-2bf7c2662020","a13c9d7e-5e2f-44c3-bd8c-17fd50ca4892","f3236b1d-73c7-45d5-920e-f7cac8c19573","c181e327-c3bf-42f8-b8aa-21a367ce64f2","f5613c52-6b68-4073-b593-03d26f51e710"],"query2":{"sort":[{"property":"=bhc","direction":"descending"},{"property":"a\u003cql","direction":"descending"}],"filter":{"filters":[{"filters":[{"filter":{"value":{"type":"exact","value":true},"operator":"checkbox_is"},"property":"==~K"},{"filter":{"value":{"type":"exact","value":true},"operator":"checkbox_is"},"property":"=bhc"},{"filter":{"value":{"type":"exact","value":true},"operator":"checkbox_is"},"property":"QSi`"}],"operator":"and"}],"operator":"and"},"aggregations":[{"aggregator":"count"}]},"space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"27bc73a3-5779-44e0-b617-2f0d26f5aa2f":{"role":"reader","value":{"id":"27bc73a3-5779-44e0-b617-2f0d26f5aa2f","version":1,"type":"table","name":"","format":{"table_properties":[{"width":293,"visible":true,"property":"title"},{"width":398,"visible":true,"property":"~]S\u003c"},{"width":81,"visible":true,"property":"==~K"},{"width":105,"visible":true,"property":"=bhc"},{"width":146,"visible":true,"property":"a\u003cql"},{"width":200,"visible":true,"property":"BN]P"},{"width":122,"visible":true,"property":"}nqi"},{"width":156,"visible":true,"property":"jhf;"},{"width":146,"visible":true,"property":"NVm^"},{"width":200,"visible":true,"property":";KhU"},{"width":200,"visible":false,"property":"nAX{"}]},"parent_id":"4f20ede8-ccf7-4ae1-82e5-819e100dd032","parent_table":"block","alive":true,"page_sort":["f3d44d4c-975d-4b11-8396-c68b35bfdb26","70df362d-5c3a-4d2c-a3da-ef27e1f207e3","f3236b1d-73c7-45d5-920e-f7cac8c19573","4117e62e-18ec-4503-a32a-6c31806a5e2a","6b9736df-63cb-4e7c-ba8e-d6e54f26d6c9","c2f261a2-c616-4198-b1a9-2caf6be162a0","a082287f-d4c9-4cfb-b3b2-72b8bbf043c6","4801bc76-a763-46a8-981d-79dc38c5d85a","e3df5b79-eb34-4bec-a82f-628699f43852","36a57bd6-d63a-40f0-a0a2-6ba247885fab","c2994f3d-696a-4015-b7d1-2b71883562ad","58fc3d75-92a2-44e9-ab0d-84cb61086232","30ac6075-b07c-41fa-b684-15c16f1134ba","4c804cac-3119-4cc3-a621-a82040d9a0db","6ce6c0ba-0d6f-40b3-b7d4-afe8ca715c73","64158711-b8f3-4c3d-ba2f-3999059c5581","0c679f04-0865-4142-a65b-28668aa9daea","54fa8c94-ead7-4b14-b068-174936adb37f","ab2e2675-7daf-464f-a2c4-563136630232","ef52bf7e-8726-4804-a0d5-e9b5f7919284","460f888e-b262-4f14-a39f-93ae0090bb9d","eff2e07b-ee44-45e0-b46a-622deb611d0b","c94f3f08-d65f-4f7d-a409-72c353f9786d","daac7673-f388-475e-8f43-a345dc350bef","6ff46924-9cc3-4445-badc-28e7daa5fa5a","c114dc9f-b09a-4e30-9c0d-5ace92e6a8e3","51b78f8b-69ff-4a46-9ec6-74f0d5400d3e","06adcb1d-3acf-4266-a8d9-6cb0d3c439ce","22c49483-380f-4b94-be90-b437b7cbab52","1db23f4a-8d7e-4445-b241-a8e4be5bd02a","dae737f3-5216-4c50-b708-2bf7c2662020","a13c9d7e-5e2f-44c3-bd8c-17fd50ca4892","7864e0d8-d646-4df5-a4ae-057371b7559d","7a8e9c32-39af-4bca-a26e-3f8f1c9a1762","0f58fdf7-da3e-4793-93f8-b2503443020d","ca43f24a-f6c2-4611-a92a-8051ec80fb0f","36b681a4-e13e-4a78-a2d6-f56b2e3657a1","8cfc0d5a-59ba-4ea0-9609-4720753d5fba","88891af2-1639-4eff-a2c9-f3ba6668b2ea","92db0a94-6e1e-44a6-8df5-882e60ff5b3f","0d958b63-df7a-44a4-b80c-5408c78f59e4","7a177274-38ce-4f03-b3f6-06e20b0ba4f8","ae0b3ee0-531c-4f81-8d4b-cc6dd040fea1","bf9953ee-2589-4969-948c-0d31106a9deb","fdb7d90f-a355-4e7d-8f9a-3d07a58a457e","561e9779-b56c-4310-8913-d54675e02c74","6951e99b-10cc-4833-a27f-f0e08f8941d0","76a4bb7d-be00-45f3-bbcd-cba28c38588b","18c92268-31e5-4509-8644-9c6ad9e44c10","6463af62-efa9-47dc-91ce-99df728f66e0"],"query2":{"sort":[{"property":"=bhc","direction":"descending"},{"property":"a\u003cql","direction":"descending"}],"aggregations":[{"property":"title","aggregator":"count"}]},"space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}}},"notion_user":{},"collection_query":{},"signed_urls":{},"preview_images":{"images.unsplash.com/photo-1563209259-b2fa97148ce1":{"originalWidth":4509,"originalHeight":3433,"dataURIBase64":"data:image/webp;base64,UklGRkQAAABXRUJQVlA4IDgAAAAQAgCdASoQAAwABUB8JZQCdAEN4Z4/0jgAAP7nI6g5q7g7NuXgHKw945x+RhNJbTW2vaRJ2E2wAA=="},"notion.so/image/notion.so%2Fimages%2Fpage-cover%2Fnasa_reduced_gravity_walking_simulator.jpg":{"originalWidth":2000,"originalHeight":1597,"dataURIBase64":"data:image/webp;base64,UklGRjwAAABXRUJQVlA4IDAAAADQAQCdASoQAA0ABUB8JaQAAuUwaXLIHAD+3O9cAAKxBQKBBYBng9tx6GTwHd24mAA="},"notion.so/image/s3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fb29e9b03-c79c-4e52-a45e-7228163ba524%2Fcompass-circular-tool_(3).png":{"originalWidth":2000,"originalHeight":2000,"dataURIBase64":"data:image/webp;base64,UklGRv4AAABXRUJQVlA4WAoAAAAQAAAADwAADwAAQUxQSFUAAAARL6CmkRQ4eF5eaio0EBGB5+2AUWxbbR5t/yWkKiDsUwVQnn8xoViI6P8EAOK9AiCF/BQQSXKHsKrOmo21PddC4xsKsbafNSc1jfQLgBSSGsD0HBoAAFZQOCCCAAAAsAIAnQEqEAAQAAVAfCWkAA+DSIAfdDvk/UudpKAAAPYcuQdG3VZft4V447Ryb0FWml/BLapzhboSZ6SLFp2WFSojeqJNq2VAANIrwpK47u1M/geoV8ECX9+TEWBDeQC4c2g3HcSldGi6FylcOf6PWeqJaegO6jdPFlsvX04FC2gAAA=="},"notion.so/image/longshiine.github.io%2F2021%2F01%2F27%2FReinforcement-Learning-Basic-6%2Ftemporal.png":{"originalWidth":2000,"originalHeight":823,"dataURIBase64":"data:image/webp;base64,UklGRjQAAABXRUJQVlA4ICgAAADwAQCdASoQAAYABUB8JaQAAt0DcVCTkAAA/ufpe9jkz1wETzWpgAAA"},"notion.so/image/longshiine.github.io%2F2021%2F01%2F27%2FReinforcement-Learning-Basic-6%2Flimitation.png":{"originalWidth":2000,"originalHeight":2072,"dataURIBase64":"data:image/webp;base64,UklGRkYAAABXRUJQVlA4IDoAAADQAQCdASoPABAABUB8JQAAXadrlhggAAD+7agzm7v1GK2nhWMtYakTkx3nwHflQAAUz92sggJZIAAA"},"notion.so/image/longshiine.github.io%2F2021%2F01%2F27%2FReinforcement-Learning-Basic-6%2Fqlearning.png":{"originalWidth":2000,"originalHeight":2101,"dataURIBase64":"data:image/webp;base64,UklGRkIAAABXRUJQVlA4IDYAAACQAQCdASoPABAABUB8JYwAAsemtoAA/u3WsjjFPs/FGgDVtaaHvjiWGipkgI+uBAEVSE2iAAA="},"notion.so/image/s3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Ffc853118-df50-43ed-96d9-0711493d5e25%2Fjang_inspiration_logo.png":{"originalWidth":2000,"originalHeight":2000,"dataURIBase64":"data:image/webp;base64,UklGRv4AAABXRUJQVlA4WAoAAAAQAAAADwAADwAAQUxQSFUAAAARL6CmkRQ4eF5eaio0EBGB5+2AUWxbbR5t/yWkKiDsUwVQnn8xoViI6P8EAOK9AiCF/BQQSXKHsKrOmo21PddC4xsKsbafNSc1jfQLgBSSGsD0HBoAAFZQOCCCAAAAsAIAnQEqEAAQAAVAfCWkAA+DSIAfdDvk/UudpKAAAPYcuQdG3VZft4V447Ryb0FWml/BLapzhboSZ6SLFp2WFSojeqJNq2VAANIrwpK47u1M/geoV8ECX9+TEWBDeQC4c2g3HcSldGi6FylcOf6PWeqJaegO6jdPFlsvX04FC2gAAA=="}}},"pageId":"60a262e5-29a3-44e9-a757-cf3312bd21b5","rawPageId":"sarsa-qlearning"},"__N_SSG":true},"page":"/[pageId]","query":{"pageId":"sarsa-qlearning"},"buildId":"a6SgCHTQj8ioz2ipgYdLM","isFallback":false,"dynamicIds":[635,7274,3358],"gsp":true,"scriptLoader":[]}</script></body></html>
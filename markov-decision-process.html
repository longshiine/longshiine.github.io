<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta name="robots" content="index,follow"/><meta property="og:type" content="website"/><meta property="og:site_name" content="Jang. Inspiration"/><meta property="twitter:domain" content="jang-inspiration.com"/><meta name="description" content="마르코프 결정 과정(MDP)의 정의부터 상태(State), 행동(Action), 보상함수(Reward Function), 할인율(Discount Factor), 정책(Policy) 등 핵심적인 개념을 톺아보자."/><meta property="og:description" content="마르코프 결정 과정(MDP)의 정의부터 상태(State), 행동(Action), 보상함수(Reward Function), 할인율(Discount Factor), 정책(Policy) 등 핵심적인 개념을 톺아보자."/><meta name="twitter:description" content="마르코프 결정 과정(MDP)의 정의부터 상태(State), 행동(Action), 보상함수(Reward Function), 할인율(Discount Factor), 정책(Policy) 등 핵심적인 개념을 톺아보자."/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:image" content="https://jang-inspiration.com/api/social-image?id=d1e3fc6c-4e58-42f7-9779-b9e78243e18e"/><meta property="og:image" content="https://jang-inspiration.com/api/social-image?id=d1e3fc6c-4e58-42f7-9779-b9e78243e18e"/><link rel="canonical" href="https://jang-inspiration.com/markov-decision-process"/><meta property="og:url" content="https://jang-inspiration.com/markov-decision-process"/><meta property="twitter:url" content="https://jang-inspiration.com/markov-decision-process"/><link rel="alternate" type="application/rss+xml" href="https://jang-inspiration.com/feed" title="Jang. Inspiration"/><meta property="og:title" content="&lt;파이썬과 케라스로 배우는 강화학습&gt; - (2) MDP(Markov Decision Process)"/><meta name="twitter:title" content="&lt;파이썬과 케라스로 배우는 강화학습&gt; - (2) MDP(Markov Decision Process)"/><title>&lt;파이썬과 케라스로 배우는 강화학습&gt; - (2) MDP(Markov Decision Process)</title><meta name="naver-site-verification" content="3942485b5f7254d146b71f1249d907d89048a4d6"/><link rel="preload" as="image" href="https://images.unsplash.com/photo-1563209259-b2fa97148ce1?ixlib=rb-4.0.3&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb"/><meta name="next-head-count" content="22"/><link rel="shortcut icon" href="/favicon.ico"/><link rel="icon" type="image/png" sizes="32x32" href="favicon.png"/><link rel="manifest" href="/manifest.json"/><link rel="preload" href="/_next/static/css/d2e6a1cb5181bdcf.css" as="style"/><link rel="stylesheet" href="/_next/static/css/d2e6a1cb5181bdcf.css" data-n-g=""/><link rel="preload" href="/_next/static/css/4e32f0fa5eadbe4b.css" as="style"/><link rel="stylesheet" href="/_next/static/css/4e32f0fa5eadbe4b.css" data-n-p=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js"></script><script defer="" src="/_next/static/chunks/3607272e.d338bf53926ee7c2.js"></script><script defer="" src="/_next/static/chunks/853.526a4df21aef109c.js"></script><script defer="" src="/_next/static/chunks/175675d1.a2f4b19cd9daa73f.js"></script><script defer="" src="/_next/static/chunks/274.59c48af6aaac8ebd.js"></script><script src="/_next/static/chunks/webpack-bcbf8f4abc46b243.js" defer=""></script><script src="/_next/static/chunks/framework-4556c45dd113b893.js" defer=""></script><script src="/_next/static/chunks/main-f08b69bdcc7bbb61.js" defer=""></script><script src="/_next/static/chunks/pages/_app-27630c741ae01a08.js" defer=""></script><script src="/_next/static/chunks/780-3413afc2700f261b.js" defer=""></script><script src="/_next/static/chunks/634-6b5e1cdbf7ca12e3.js" defer=""></script><script src="/_next/static/chunks/pages/%5BpageId%5D-35899cbd792aff57.js" defer=""></script><script src="/_next/static/a6SgCHTQj8ioz2ipgYdLM/_buildManifest.js" defer=""></script><script src="/_next/static/a6SgCHTQj8ioz2ipgYdLM/_ssgManifest.js" defer=""></script></head><body><script>
/** Inlined version of noflash.js from use-dark-mode */
;(function () {
  var storageKey = 'darkMode'
  var classNameDark = 'dark-mode'
  var classNameLight = 'light-mode'
  function setClassOnDocumentBody(darkMode) {
    document.body.classList.add(darkMode ? classNameDark : classNameLight)
    document.body.classList.remove(darkMode ? classNameLight : classNameDark)
  }
  var preferDarkQuery = '(prefers-color-scheme: dark)'
  var mql = window.matchMedia(preferDarkQuery)
  var supportsColorSchemeQuery = mql.media === preferDarkQuery
  var localStorageTheme = null
  try {
    localStorageTheme = localStorage.getItem(storageKey)
  } catch (err) {}
  var localStorageExists = localStorageTheme !== null
  if (localStorageExists) {
    localStorageTheme = JSON.parse(localStorageTheme)
  }
  // Determine the source of truth
  if (localStorageExists) {
    // source of truth from localStorage
    setClassOnDocumentBody(localStorageTheme)
  } else if (supportsColorSchemeQuery) {
    // source of truth from system
    setClassOnDocumentBody(mql.matches)
    localStorage.setItem(storageKey, mql.matches)
  } else {
    // source of truth from document.body
    var isDarkMode = document.body.classList.contains(classNameDark)
    localStorage.setItem(storageKey, JSON.stringify(isDarkMode))
  }
})();
</script><div id="__next"><div class="notion notion-app light-mode notion-block-d1e3fc6c4e5842f79779b9e78243e18e"><div class="notion-viewport"></div><div class="notion-frame"><header class="notion-header"><div class="notion-nav-header"><div class="breadcrumbs"><a class="breadcrumb" href="/"><div class="notion-page-icon-inline notion-page-icon-image"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%272000%27%20height=%272000%27/%3e"/></span><img alt="Jang. Inspiration" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" class="icon notion-page-icon" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/webp;base64,UklGRv4AAABXRUJQVlA4WAoAAAAQAAAADwAADwAAQUxQSFUAAAARL6CmkRQ4eF5eaio0EBGB5+2AUWxbbR5t/yWkKiDsUwVQnn8xoViI6P8EAOK9AiCF/BQQSXKHsKrOmo21PddC4xsKsbafNSc1jfQLgBSSGsD0HBoAAFZQOCCCAAAAsAIAnQEqEAAQAAVAfCWkAA+DSIAfdDvk/UudpKAAAPYcuQdG3VZft4V447Ryb0FWml/BLapzhboSZ6SLFp2WFSojeqJNq2VAANIrwpK47u1M/geoV8ECX9+TEWBDeQC4c2g3HcSldGi6FylcOf6PWeqJaegO6jdPFlsvX04FC2gAAA==&quot;)"/><noscript><img alt="Jang. Inspiration" src="https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fb29e9b03-c79c-4e52-a45e-7228163ba524%2Fcompass-circular-tool_(3).png?table=block&amp;id=6246082f-4014-4d06-98ab-59e9840b298a&amp;cache=v2" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%" class="icon notion-page-icon" loading="lazy"/></noscript></span></div><span class="title">Jang. Inspiration</span></a></div><div class="notion-nav-header-rhs breadcrumbs"><a href="/about" class="breadcrumb button">About</a><div class="breadcrumb button styles_hidden__7gYve"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="32" d="M256 48v48m0 320v48m147.08-355.08l-33.94 33.94M142.86 369.14l-33.94 33.94M464 256h-48m-320 0H48m355.08 147.08l-33.94-33.94M142.86 142.86l-33.94-33.94"></path><circle cx="256" cy="256" r="80" fill="none" stroke-linecap="round" stroke-miterlimit="10" stroke-width="32"></circle></svg></div><div role="button" class="breadcrumb button notion-search-button"><svg class="notion-icon searchIcon" viewBox="0 0 17 17"><path d="M6.78027 13.6729C8.24805 13.6729 9.60156 13.1982 10.709 12.4072L14.875 16.5732C15.0684 16.7666 15.3232 16.8633 15.5957 16.8633C16.167 16.8633 16.5713 16.4238 16.5713 15.8613C16.5713 15.5977 16.4834 15.3516 16.29 15.1582L12.1504 11.0098C13.0205 9.86719 13.5391 8.45215 13.5391 6.91406C13.5391 3.19629 10.498 0.155273 6.78027 0.155273C3.0625 0.155273 0.0214844 3.19629 0.0214844 6.91406C0.0214844 10.6318 3.0625 13.6729 6.78027 13.6729ZM6.78027 12.2139C3.87988 12.2139 1.48047 9.81445 1.48047 6.91406C1.48047 4.01367 3.87988 1.61426 6.78027 1.61426C9.68066 1.61426 12.0801 4.01367 12.0801 6.91406C12.0801 9.81445 9.68066 12.2139 6.78027 12.2139Z"></path></svg></div></div></div></header><div class="notion-page-scroller"><div class="notion-page-cover-wrapper"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%274509%27%20height=%273433%27/%3e"/></span><img alt="&lt;파이썬과 케라스로 배우는 강화학습&gt; - (2) MDP(Markov Decision Process)" src="https://images.unsplash.com/photo-1563209259-b2fa97148ce1?ixlib=rb-4.0.3&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb" decoding="async" data-nimg="intrinsic" class="notion-page-cover" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-position:center 50%;background-size:cover;background-position:center 50%;filter:blur(20px);background-image:url(&quot;data:image/webp;base64,UklGRkQAAABXRUJQVlA4IDgAAAAQAgCdASoQAAwABUB8JZQCdAEN4Z4/0jgAAP7nI6g5q7g7NuXgHKw945x+RhNJbTW2vaRJ2E2wAA==&quot;)"/><noscript><img alt="&lt;파이썬과 케라스로 배우는 강화학습&gt; - (2) MDP(Markov Decision Process)" src="https://images.unsplash.com/photo-1563209259-b2fa97148ce1?ixlib=rb-4.0.3&amp;q=80&amp;fm=jpg&amp;crop=entropy&amp;cs=tinysrgb" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-position:center 50%" class="notion-page-cover"/></noscript></span></div><main class="notion-page notion-page-has-cover notion-page-has-icon notion-page-has-text-icon notion-full-page"><div class="notion-page-icon-hero notion-page-icon-span"><span class="notion-page-icon" role="img" aria-label="🎮">🎮</span></div><h1 class="notion-title">&lt;파이썬과 케라스로 배우는 강화학습&gt; - (2) <b>MDP(Markov Decision Process)</b></h1><div class="notion-collection-page-properties"><div class="notion-collection-row"><div class="notion-collection-row-body"><div class="notion-collection-row-property"><div class="notion-collection-column-title"><svg viewBox="0 0 14 14" class="notion-collection-column-title-icon"><path d="M7 13A6 6 0 107 1a6 6 0 000 12zM3.751 5.323A.2.2 0 013.909 5h6.182a.2.2 0 01.158.323L7.158 9.297a.2.2 0 01-.316 0L3.751 5.323z"></path></svg><div class="notion-collection-column-title-body">Category</div></div><div class="notion-collection-row-value"><span class="notion-property notion-property-select"><div class="notion-property-select-item notion-item-orange">강화학습</div></span></div></div><div class="notion-collection-row-property"><div class="notion-collection-column-title"><svg viewBox="0 0 14 14" class="notion-collection-column-title-icon"><path d="M4 3a1 1 0 011-1h7a1 1 0 110 2H5a1 1 0 01-1-1zm0 4a1 1 0 011-1h7a1 1 0 110 2H5a1 1 0 01-1-1zm0 4a1 1 0 011-1h7a1 1 0 110 2H5a1 1 0 01-1-1zM2 4a1 1 0 110-2 1 1 0 010 2zm0 4a1 1 0 110-2 1 1 0 010 2zm0 4a1 1 0 110-2 1 1 0 010 2z"></path></svg><div class="notion-collection-column-title-body">Tags</div></div><div class="notion-collection-row-value"><span class="notion-property notion-property-multi_select"><div class="notion-property-multi_select-item notion-item-pink">Reinforcement Learning</div><div class="notion-property-multi_select-item notion-item-yellow">MDP</div></span></div></div><div class="notion-collection-row-property"><div class="notion-collection-column-title"><svg viewBox="0 0 14 14" class="notion-collection-column-title-icon"><path d="M10.889 5.5H3.11v1.556h7.778V5.5zm1.555-4.444h-.777V0H10.11v1.056H3.89V0H2.333v1.056h-.777c-.864 0-1.548.7-1.548 1.555L0 12.5c0 .856.692 1.5 1.556 1.5h10.888C13.3 14 14 13.356 14 12.5V2.611c0-.855-.7-1.555-1.556-1.555zm0 11.444H1.556V3.944h10.888V12.5zM8.556 8.611H3.11v1.556h5.445V8.61z"></path></svg><div class="notion-collection-column-title-body">Published</div></div><div class="notion-collection-row-value"><span class="notion-property notion-property-date">January 20, 2021</span></div></div><div class="notion-collection-row-property"><div class="notion-collection-column-title"><svg viewBox="0 0 14 14" class="notion-collection-column-title-icon"><path d="M7 4.568a.5.5 0 00-.5-.5h-6a.5.5 0 00-.5.5v1.046a.5.5 0 00.5.5h6a.5.5 0 00.5-.5V4.568zM.5 1a.5.5 0 00-.5.5v1.045a.5.5 0 00.5.5h12a.5.5 0 00.5-.5V1.5a.5.5 0 00-.5-.5H.5zM0 8.682a.5.5 0 00.5.5h11a.5.5 0 00.5-.5V7.636a.5.5 0 00-.5-.5H.5a.5.5 0 00-.5.5v1.046zm0 3.068a.5.5 0 00.5.5h9a.5.5 0 00.5-.5v-1.045a.5.5 0 00-.5-.5h-9a.5.5 0 00-.5.5v1.045z"></path></svg><div class="notion-collection-column-title-body">Author</div></div><div class="notion-collection-row-value"><span class="notion-property notion-property-text"><b>Jay</b></span></div></div></div></div></div><div class="notion-page-content notion-page-content-has-aside"><article class="notion-page-content-inner"><blockquote class="notion-quote notion-block-ed886261c38e4c5fb82d7478ed25cf11"><div>본 포스트는 “파이썬과 케라스로 배우는 강화학습” 도서의 두번째 리뷰 포스트입니다.</div></blockquote><div class="notion-text notion-block-1411732a9b3441338adf1021ba502655"><a target="_blank" rel="noopener noreferrer" class="notion-link" href="http://www.yes24.com/Product/Goods/44136413">http://www.yes24.com/Product/Goods/44136413</a></div><div class="notion-blank notion-block-5ad325136f084205905f70db4fe624ba"> </div><div class="notion-table-of-contents notion-gray notion-block-43e688bc61264419b9da67483caf50aa"><a href="#16877cb1fbea4651b5df59a985de8d7b" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:0">2장 강화학습 기초 1: MDP(Markov Decision Process)</span></a><a href="#3168087e4a434c1a8f12463a9755a2a1" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">MDP</span></a><a href="#9e46e38a6a88453fb645050cef2805d2" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">상태</span></a><a href="#97aa50bedee1493a9d8733c18b3d2118" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">행동</span></a><a href="#a9ff620cdbee48ee9156c057b85c0766" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">보상함수</span></a><a href="#6a91998bc72e44b6b4170dca3ba704ee" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">상태 변환 확률</span></a><a href="#4c15dd486ab04ea4953743247bd86ada" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">할인율</span></a><a href="#d4ceba4a1548496ba0785c4a266a6821" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">정책</span></a><a href="#5611a4c2acdc4216a8e408b47f389637" class="notion-table-of-contents-item"><span class="notion-table-of-contents-item-body" style="display:inline-block;margin-left:24px">정리</span></a></div><div class="notion-blank notion-block-ddcf4d6d2b6044cdb0c0467338e8d2e2"> </div><h3 class="notion-h notion-h2 notion-h-indent-0 notion-block-16877cb1fbea4651b5df59a985de8d7b" data-id="16877cb1fbea4651b5df59a985de8d7b"><span><div id="16877cb1fbea4651b5df59a985de8d7b" class="notion-header-anchor"></div><a class="notion-hash-link" href="#16877cb1fbea4651b5df59a985de8d7b" title="2장 강화학습 기초 1: MDP(Markov Decision Process)"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>2장 강화학습 기초 1: MDP(Markov Decision Process)</b></span></span></h3><hr class="notion-hr notion-block-86e937c43b994b80a0cece906c8809ce"/><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-3168087e4a434c1a8f12463a9755a2a1" data-id="3168087e4a434c1a8f12463a9755a2a1"><span><div id="3168087e4a434c1a8f12463a9755a2a1" class="notion-header-anchor"></div><a class="notion-hash-link" href="#3168087e4a434c1a8f12463a9755a2a1" title="MDP"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>MDP</b></span></span></h4><div class="notion-text notion-block-e5e1b9313eb64e329aac47c1cf6aa612">앞서 살펴보았듯 <code class="notion-inline-code">MDP</code>는 순차적으로 결정해야하는 문제를 수학적으로 표현한다.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-81310d4ca99847999f30cd69b1698720"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:480px;max-width:100%;flex-direction:column"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%272000%27%20height=%27600%27/%3e"/></span><img alt="MDP의 구성요소" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/webp;base64,UklGRkIAAABXRUJQVlA4IDYAAACwAQCdASoQAAUABUB8JZQC7ADx9MAAAP6yIRD7JlC+hmv02zQnELzepk5gtoAwEAITIASQAAA=&quot;)"/><noscript><img alt="MDP의 구성요소" src="https://www.notion.so/image/https%3A%2F%2Flongshiine.github.io%2F2021%2F01%2F20%2FReinforcement-Learning-Basic-1%2FMDP.png?table=block&amp;id=81310d4c-a998-4799-9f30-cd69b1698720&amp;cache=v2" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></span><figcaption class="notion-asset-caption">MDP의 구성요소</figcaption></div></figure><div class="notion-text notion-block-d98f45af55924057a796e90a6a179811">문제를 잘못 정의하면 에이전트가 학습을 못할 수도 있다. 따라서 문제의 정의는 에이전트가 학습하는 데 가장 중요한 단계 중 하나이다. 에이전트를 구현하는 사람은 학습하기에 많지도 않고 적지도 않은 <b>적절한 정보</b>를 에이전트가 알 수도 있도록 문제를 정의해야 한다. <code class="notion-inline-code">MDP</code>의 이해를 돕기위해 이제 <code class="notion-inline-code">그리드 월드(Grid World)</code>라는 예제를 살펴볼 것인데, 그리드는 격자이고 그리드 월드는 격자로 이뤄진 환경에서 문제를 푸는 각종 예제를 뜻한다. 어디한번 MDP의 구성요소를 하나하나 살펴보자.</div><div class="notion-blank notion-block-0576d5f1e72c40c2918f76a88f46db37"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-9e46e38a6a88453fb645050cef2805d2" data-id="9e46e38a6a88453fb645050cef2805d2"><span><div id="9e46e38a6a88453fb645050cef2805d2" class="notion-header-anchor"></div><a class="notion-hash-link" href="#9e46e38a6a88453fb645050cef2805d2" title="상태"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>상태</b></span></span></h4><div class="notion-text notion-block-e0a662b609af47e4af46a7ae142b1225"><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span> 는 에이전트가 관찰 가능한 상태의 집합이다. 상태라는 말의 의미가 모호할 수 있는데 <b>자신의 상황에 대한 관찰</b> 이 상태에 대한 가장 정확한 표현이다. 로봇과 같은 실제 세상에서의 에이전트에게 상태는 센서 값이 될 것이다. 하지만 이 책에서와 같이 게임을 학습하기 위한 에이전트는 사용자가 상태를 정의해줘야 한다. 이때 <b>‘내가 정의하는 상태가 에이전트가 학습하기에 충분한 정보를 주는 것인가?’</b> 라는 질문을 스스로에게 던져보는 것이 좋다.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-89dc8a237a9c4a66845e289d4ed8206c"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:480px;max-width:100%;flex-direction:column"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%272000%27%20height=%271469%27/%3e"/></span><img alt="그리드월드에서 상태는 좌표를 의미한다" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/webp;base64,UklGRjwAAABXRUJQVlA4IDAAAAAwAQCdASoQAAwABUB8JZwAA3AA/vAR1PXtQWFU4Gxe1p0CcP6WEUad3UHYCuZeAAA=&quot;)"/><noscript><img alt="그리드월드에서 상태는 좌표를 의미한다" src="https://www.notion.so/image/https%3A%2F%2Flongshiine.github.io%2F2021%2F01%2F20%2FReinforcement-Learning-Basic-1%2Fgreedworld.png?table=block&amp;id=89dc8a23-7a9c-4a66-845e-289d4ed8206c&amp;cache=v2" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></span><figcaption class="notion-asset-caption">그리드월드에서 상태는 좌표를 의미한다</figcaption></div></figure><div class="notion-text notion-block-e5e4f7405fe747e495f9c74bd2adbfcd">위의 그림과 같이 그리드월드(5x5)에서는 <b>격자 상의 각 위치(좌표)</b> 가 상태가 된다. 그리드월드(5x5)의 상태는 모든 격자의 위치로서 총 25개가 있다. 각 상태는 (x,y)로 이뤄진 좌표로서 그리드월드의 가로축이 x축이고 세로축이 y축이다. 식으로는 다음과 같이 표현할 수 있다.</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-bb8235a947004f5cb3ec39c915700b36"><span></span></span><div class="notion-text notion-block-5d592407a16b4395a741bd1f0c0abba9">에이전트는 시간에 따라 25개의 상태의 집합 안에 있는 상태를 탐험하게 된다. 시간은 t라고 표현하고, 시간 t일때의 상태를 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span> 라고 표현하는데, 만약 시간이 t일때 상태가 (1,3)이라면 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span> 와 같이 표현한다.</div><div class="notion-text notion-block-98bcc864707a42a89315136347a74bbc"><code class="notion-inline-code">MDP</code> 에서 상태는 시간에 따라 확률적으로 변한다. t = 1일때 상태는 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span> 일 수도 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span> 일 수도 있다. 시간 t에서 에이전트가 있을 상태가 <b>확률 변수</b> 라는 뜻이다. 예를 들어 주사위를 던진다고 할때, 주사위를 던지는 실험은 임의 실험이고 주사위를 던져서 나오는 값은 변수가 된다. 임의의 실험에서 나오는 변수는 자신이 나타날 확률값을 가지고 있으며, 이 변수는 확률 변수가 된다. 보통 <b>“시간 t에서의 상태 </b><b><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span></b><b>가 어떤 상태 </b><b><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span></b><b>다”</b> 를 표현할때 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>와 같이 적는다.</div><div class="notion-blank notion-block-4106414f3ff64eefb08b4047fa8e59c2"> </div><blockquote class="notion-quote notion-block-7068fdfc116a4ad2933e611a5cfb30c3"><div>임의의 시간 t에서의 상태 =&gt; <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span></div></blockquote><blockquote class="notion-quote notion-block-4a9159a051cb4b539ad3a75acc9e08d4"><div><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>가 어떤 상태 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>다 =&gt; <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span></div></blockquote><div class="notion-blank notion-block-9abcc5b6a1ba4e92a361e98b919ce458"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-97aa50bedee1493a9d8733c18b3d2118" data-id="97aa50bedee1493a9d8733c18b3d2118"><span><div id="97aa50bedee1493a9d8733c18b3d2118" class="notion-header-anchor"></div><a class="notion-hash-link" href="#97aa50bedee1493a9d8733c18b3d2118" title="행동"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>행동</b></span></span></h4><div class="notion-text notion-block-8a38bbbfb9c34d96b65a7fa0b5ce2d54">에이전트가 상태<span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>에서 할 수 있는 가능한 행동의 집합은 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>이다. 보통 에이전트가 할 수 있는 행동은 모든 상태에서 같다. 따라서 하나의 집합 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>로 나타낼 수 있다. <b>시간 t에서의 행동 a</b>는 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>와 같이 표현하며, t라는 시간에 에이전트가 어떤 행동을 할지 정해져 있는 것이 아니므로 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>와 같이 대문자로 표현한다. 즉 확률변수이다 </div><div class="notion-text notion-block-1011d5708eab45bc946085a4dad1827b">그리드월드에서 에이전트가 할 수 있는 행동의 집합은 아래와 같다.</div><div class="notion-text notion-block-8e5283d44bd141079de13b626c31259c">만약 시간 t에서 St=(3,1)이고 At=right 라면 St+1=(4,1)이다. 그런데 바람과 같은 예상치 못한 요소가 있다면 에이전트는 (4,1)에 도달하지 못할 수도 있고, 이러한 요소를 포함하여 에이전트가 어디로 이동할지 결정하는 것을 <code class="notion-inline-code">상태 변환 확률</code>이라고 한다. 이는 조금 뒤 자세히 다루도록 하겠다.</div><div class="notion-blank notion-block-ac6edb5e40f94de2b7bd8f245e4dd7af"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-a9ff620cdbee48ee9156c057b85c0766" data-id="a9ff620cdbee48ee9156c057b85c0766"><span><div id="a9ff620cdbee48ee9156c057b85c0766" class="notion-header-anchor"></div><a class="notion-hash-link" href="#a9ff620cdbee48ee9156c057b85c0766" title="보상함수"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>보상함수</b></span></span></h4><div class="notion-text notion-block-30e15b59cef846eba2b0ac6461b322d4"><b>보상(reward)</b> 은 에이전트가 학습할 수 있는 유일한 정보로서 환경이 에이전트에게 주는 정보이다. 시간 t에서 상태가 St=s이고 행동이 At=a일 때 보상함수는 아래와 같이 정의된다.</div><div class="notion-text notion-block-c7acbc0b0a564650aa3d364f988e2b08">보상함수는 시간 t일 때 상태가 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>이고 그 상태에서 행동 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>를 했을 경우에 받을 보상에 대한 기댓값 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>이다. 여기서 <b>기댓값</b> 은 무엇일까? (한번쯤 쉽게 풀어써보고 싶었는데 책의 예시가 아주 적절하다🤩)</div><div class="notion-text notion-block-8934455e025b4b46aa3a53478f4f2c7a"><b>기댓값</b> 이란 일종의 평균이다. 주사위의 기댓값을 한번 생각해보면, 모든 면이 <code class="notion-inline-code"><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span></code>의 동등한 확률로 나올테고, 주사위에서는 다음과 같은 값이 나올 것이라고 <b>기대</b> 할 수 있다.</div><span role="button" tabindex="0" class="notion-equation notion-equation-block notion-block-387148f8474746aa848bc15b0e7bc8b1"><span></span></span><div class="notion-text notion-block-d21071faa52b411992aa3ae4a689f25e">다시 본론으로 돌아와, <b>보상함수는 왜 기댓값으로 표현하는 것일까?</b></div><blockquote class="notion-quote notion-block-9e541795d40a412ea21816ce4af69367"><div>보상을 에이전트에게 주는 것은 환경이고,환경에 따라서 같은 상태에서 같은 행동을 취하더라도 다른 보상을 줄 수 있기 때문이다.</div></blockquote><div class="notion-text notion-block-5f677d450a8f442e8c01844353fd63a5">또한 보상함수에서 특이한 점은 에이전트가 어떤 상태에서 행동한 것은 시간 t에서인데 보상을 받는 것은 t+1이라는 것이다. 이는 보상을 에이전트가 알고 있는 것이 아니라 환경이 알려주기 때문이다. 때문에 에이전트가 받는 보상은 하나의 시간 단위가 지난 다음에 주어진다. 이 시간단위를 앞으로 <code class="notion-inline-code">타임스텝(time step)</code>이라고 한다.</div><div class="notion-blank notion-block-bed4cfe4ae4d4404be5b9163d823d358"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-6a91998bc72e44b6b4170dca3ba704ee" data-id="6a91998bc72e44b6b4170dca3ba704ee"><span><div id="6a91998bc72e44b6b4170dca3ba704ee" class="notion-header-anchor"></div><a class="notion-hash-link" href="#6a91998bc72e44b6b4170dca3ba704ee" title="상태 변환 확률"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>상태 변환 확률</b></span></span></h4><div class="notion-text notion-block-061ee495d5794cb68bfed85e01b9bff1">에이전트가 어떤 상태에서 어떤 행동을 취한다면 에이전트의 상태는 변할 것이다. <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>은 다음 스텝에 에이전트가 갈 수 있는 어떤 특정한 상태를 의미하는데, 꼭 다음 상태에 도달하리라는 보장은 없다. 옆에서 바람이 불 수도 있고 갑자기 넘어질 수 있는 것이다. 이처럼 상태의 변화에는 확률적인 요인이 들어가고, 이를 수치적으로 표현한 것이 <code class="notion-inline-code">상태 변환 확률</code> <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>이다.</div><div class="notion-text notion-block-4c8e837e15164dd797efd80134593734"><code class="notion-inline-code">상태 변환 확률</code>은 상태 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>에서 행동 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>를 취했을 때 다른 상태 <b><span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span></b>으로 도달할 확률이다. 이 값은 보상과 마찬가지로 에이전트가 알지 못하는 값으로서 에이전트가 아닌 환경의 일부이다. 다른 말로 <b>환경의 모델(model)</b> 이라 부르기도 한다.</div><div class="notion-text notion-block-3fe44ec4a4b04a60aa3b82aff4f04e13">환경은 에이전트가 행동을 취하면 <code class="notion-inline-code">상태 변환 확률</code>을 통해 다음에 에이전트가 갈 상태를 알려준다.</div><div class="notion-blank notion-block-a3facfd4732e476ba647b33c406fed43"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-4c15dd486ab04ea4953743247bd86ada" data-id="4c15dd486ab04ea4953743247bd86ada"><span><div id="4c15dd486ab04ea4953743247bd86ada" class="notion-header-anchor"></div><a class="notion-hash-link" href="#4c15dd486ab04ea4953743247bd86ada" title="할인율"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>할인율</b></span></span></h4><div class="notion-text notion-block-f1ad4ed7b87d4ebb99a81a1481ccd8ed">에이전트가 항상 현재에 판단을 내리기 때문에 현재에 가까운 보상일 수록 더 큰 가치를 지닌다. 가령 우리가 10억원 짜리 복권에 당첨되었다고 생각해보자. 지금 당장 받을 수도 10년 뒤에 받을 수도 있다고 할때, 우리는 당연히 당장 받는 것을 선호할 것이다. 시간이 지남에 따라 이자가 붙을 것을 가정하기 때문이다. 이는 다른 말로 <b>같은 보상이면 나중에 받을수록 가치가 줄어든다</b> 고 말할 수 있다. 강화학습에도 이와 같은 가정이 적용되고 이를 수학적으로 표현하기 위해 <code class="notion-inline-code">할인율(Discount Factor)</code>이라는 개념을 도입한다.</div><div class="notion-text notion-block-a5db69cae1ab49f7b0f65b0d5525de69"><code class="notion-inline-code">할인율</code>은 0과 1사이의 값이고, 보상에 곱해지면 보상이 감소한다. 이렇게 미래의 가치를 현재의 가치로 환산하는 것을 <b>할인한다</b> 고 하고, 시간에 따라 할인하는 비율을 할인율이라고 한다. 만약 현재의 시간 t로부터 시간 k가 지난 후에 보상을 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>만큼 받을 것이라고 하면 그 보상의 가치는 아래와 같다</div><blockquote class="notion-quote notion-block-894f81afc6004e3eab315b5a7806c12e"><div>더 먼 미래에 받는 보상일수록 현재의 에이전트는 더 작은 값으로 받아들인다.</div></blockquote><div class="notion-blank notion-block-b554f847bf2f477c8ab4051220685d95"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-d4ceba4a1548496ba0785c4a266a6821" data-id="d4ceba4a1548496ba0785c4a266a6821"><span><div id="d4ceba4a1548496ba0785c4a266a6821" class="notion-header-anchor"></div><a class="notion-hash-link" href="#d4ceba4a1548496ba0785c4a266a6821" title="정책"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>정책</b></span></span></h4><div class="notion-text notion-block-d267a9c1d23743b6b88e98b7abdcf0ab"><code class="notion-inline-code">정책</code>은 모든 상태에서 에이전트가 할 행동이다. 상태가 입력으로 들어오면 행동을 출력으로 내보내는 <b>일종의 함수</b> 라고 생각해도 좋다. 정책은 각 상태에서 단 하나의 행동만을 나타낼 수도 있고, 확률적으로 <span role="button" tabindex="0" class="notion-equation notion-equation-inline"><span></span></span>와 같이 나타낼 수도 있다. 에이전트가 학습하고 있을 땐, 정책이 하나의 행동만을 선택하기 보다는 확률적으로 여러개의 행동을 선택할 수 있어야 하며 수식으로 나타내면 아래와 같다.</div><div class="notion-text notion-block-cbbde32923fc4883b5d08ed6992bdb1d">이러한 정책은 하나의 예시로서 각 상태마다 어떤 행동을 해야할지 아래의 그림과 같이 알려준다.</div><figure class="notion-asset-wrapper notion-asset-wrapper-image notion-block-fee7c1007bb44663ad01dff0f9c2e169"><div style="position:relative;display:flex;justify-content:center;align-self:center;width:384px;max-width:100%;flex-direction:column"><span style="box-sizing:border-box;display:inline-block;overflow:hidden;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;position:relative;max-width:100%"><span style="box-sizing:border-box;display:block;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0;max-width:100%"><img style="display:block;max-width:100%;width:initial;height:initial;background:none;opacity:1;border:0;margin:0;padding:0" alt="" aria-hidden="true" src="data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%272000%27%20height=%272000%27/%3e"/></span><img alt="그리드월드에서의 정책" src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover;background-size:cover;background-position:0% 0%;filter:blur(20px);background-image:url(&quot;data:image/webp;base64,UklGRtAAAABXRUJQVlA4WAoAAAAQAAAADwAADwAAQUxQSCgAAAARL6AQQADE37nRiIj4JyiGJInZCwKnkPwl53kAEf2fgJvGFGdxmvA/VlA4IIIAAABwAgCdASoQABAABUB8JaQAH0AgDN1mZSci2r8AAP7iJ/n8v1+Xhzrvoramiai4mPCF5lS1Wv+BU/lB6obS9/x7BsFec4bRQK5UjOCdby1+nrtV4sWcj6lanSrI97znvumfQr2KwWdXhXfEzSA7wqvxsLy1GfYmUxCbx5uAo8dDMAAA&quot;)"/><noscript><img alt="그리드월드에서의 정책" src="https://www.notion.so/image/https%3A%2F%2Flongshiine.github.io%2F2021%2F01%2F20%2FReinforcement-Learning-Basic-1%2Fpolicy.png?table=block&amp;id=fee7c100-7bb4-4663-ad01-dff0f9c2e169&amp;cache=v2" decoding="async" data-nimg="intrinsic" style="position:absolute;top:0;left:0;bottom:0;right:0;box-sizing:border-box;padding:0;border:none;margin:auto;display:block;width:0;height:0;min-width:100%;max-width:100%;min-height:100%;max-height:100%;object-fit:cover" loading="lazy"/></noscript></span><figcaption class="notion-asset-caption">그리드월드에서의 정책</figcaption></div></figure><div class="notion-text notion-block-3283e945b41d487594716c863c386326">정책만 가지고 있으면 에이전트는 사실 모든 상태에서 자신이 해야 할 행동을 알 수 있다. 그러나 강화학습 문제를 통해 알고 싶은 것은 그냥 정책이 아니라 <b>최적의 정책</b> 이다.</div><div class="notion-blank notion-block-5de300af89c54411af867eeb407c6b22"> </div><blockquote class="notion-quote notion-block-f9b6c2e9db6949eebf57c1787e344962"><div>최적 정책을 얻기 위해서 현재의 정책보다 더 좋은 정책을 학습해나가는 것이 강화학습이다.</div></blockquote><div class="notion-blank notion-block-d55efc54ce564ad4a76fdaa39b4f7047"> </div><h4 class="notion-h notion-h3 notion-h-indent-1 notion-block-5611a4c2acdc4216a8e408b47f389637" data-id="5611a4c2acdc4216a8e408b47f389637"><span><div id="5611a4c2acdc4216a8e408b47f389637" class="notion-header-anchor"></div><a class="notion-hash-link" href="#5611a4c2acdc4216a8e408b47f389637" title="정리"><svg viewBox="0 0 16 16" width="16" height="16"><path fill-rule="evenodd" d="M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z"></path></svg></a><span class="notion-h-title"><b>정리</b></span></span></h4><div class="notion-text notion-block-aad9bafafc9a4da38d0ba40b0aa045ef">이처럼 <code class="notion-inline-code">MDP</code>를 통해 순차적 행동 결정 문제를 정의했다. 에이전트가 현재 상태에서 앞으로 받을 보상들을 고려해서 행동을 결정한다. 그러면 환경은 에이전트에게 실제 보상과 다음 상태를 알려준다. 이러한 과정을 반복하면서 에이전트는 어떤 상태에서 앞으로 받을 것이라 예상했던 보상에 대해 틀렸다는 것을 알게 된다. 이때 앞으로 받을 것이라 예상하는 보상을 <code class="notion-inline-code">가치함수(Value Function)</code>라고 하며, 다음 장에서 설명된다. 그러한 과정에서 에이전트는 실제로 받은 보상을 토대로 가치함수와 정책을 바꿔나간다. 이러한 학습 과정을 충분히 반복한다면 가장 많은 보상을 받게하는 정책을 학습할 수 있다.</div></article><aside class="notion-aside"></aside></div></main></div></div></div><div style="width:100%;background-color:#ffffff;color:#373534;padding:20px"><div id="disqus_recommendations"></div><div id="disqus_thread"></div><footer class="styles_footer__RBpyk"><div class="styles_copyright__nhL_k">Copyright 2023 <!-- -->Jang Yeong</div><div class="styles_settings__GyEhi"></div><div class="styles_social__ptL3p"><a class="styles_github__0JN7a" href="https://github.com/longshiine" title="GitHub @longshiine" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a><a class="styles_linkedin__bgwDi" href="https://www.linkedin.com/in/jangyeong-kim-b7924422a" title="LinkedIn Jang Yeong" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3 0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2 0 38.5 17.3 38.5 38.5 0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9V416z"></path></svg></a><a class="styles_instagram__BY5Hj" href="https://instagram.com/jang.inspiration" title="Instagram Jang Yeong" target="_blank" rel="noopener noreferrer"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 24 24" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><g><path fill="none" d="M0 0h24v24H0z"></path><path fill-rule="nonzero" d="M12 2c2.717 0 3.056.01 4.122.06 1.065.05 1.79.217 2.428.465.66.254 1.216.598 1.772 1.153a4.908 4.908 0 0 1 1.153 1.772c.247.637.415 1.363.465 2.428.047 1.066.06 1.405.06 4.122 0 2.717-.01 3.056-.06 4.122-.05 1.065-.218 1.79-.465 2.428a4.883 4.883 0 0 1-1.153 1.772 4.915 4.915 0 0 1-1.772 1.153c-.637.247-1.363.415-2.428.465-1.066.047-1.405.06-4.122.06-2.717 0-3.056-.01-4.122-.06-1.065-.05-1.79-.218-2.428-.465a4.89 4.89 0 0 1-1.772-1.153 4.904 4.904 0 0 1-1.153-1.772c-.248-.637-.415-1.363-.465-2.428C2.013 15.056 2 14.717 2 12c0-2.717.01-3.056.06-4.122.05-1.066.217-1.79.465-2.428a4.88 4.88 0 0 1 1.153-1.772A4.897 4.897 0 0 1 5.45 2.525c.638-.248 1.362-.415 2.428-.465C8.944 2.013 9.283 2 12 2zm0 5a5 5 0 1 0 0 10 5 5 0 0 0 0-10zm6.5-.25a1.25 1.25 0 0 0-2.5 0 1.25 1.25 0 0 0 2.5 0zM12 9a3 3 0 1 1 0 6 3 3 0 0 1 0-6z"></path></g></svg></a></div></footer></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"site":{"domain":"jang-inspiration.com","name":"Jang. Inspiration","rootNotionPageId":"6246082f40144d0698ab59e9840b298a","rootNotionSpaceId":null,"description":"장영의 영감노트"},"recordMap":{"block":{"d1e3fc6c-4e58-42f7-9779-b9e78243e18e":{"role":"reader","value":{"id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","version":872,"type":"page","properties":{"==~K":[["Yes"]],"AfoN":[["강화학습"]],"BN]P":[["Reinforcement Learning,MDP"]],"NVm^":[["markov-decision-process"]],"a\u003cql":[["‣",[["d",{"type":"date","start_date":"2021-01-20"}]]]],"}nqi":[["Jay"]],"~]S\u003c":[["마르코프 결정 과정(MDP)의 정의부터 상태(State), 행동(Action), 보상함수(Reward Function), 할인율(Discount Factor), 정책(Policy) 등 핵심적인 개념을 톺아보자."]],"title":[["\u003c파이썬과 케라스로 배우는 강화학습\u003e - (2) "],["MDP(Markov Decision Process)",[["b"]]]]},"content":["ed886261-c38e-4c5f-b82d-7478ed25cf11","1411732a-9b34-4133-8adf-1021ba502655","5ad32513-6f08-4205-905f-70db4fe624ba","43e688bc-6126-4419-b9da-67483caf50aa","ddcf4d6d-2b60-44cd-b0c0-467338e8d2e2","16877cb1-fbea-4651-b5df-59a985de8d7b","86e937c4-3b99-4b80-a0ce-ce906c8809ce","3168087e-4a43-4c1a-8f12-463a9755a2a1","e5e1b931-3eb6-4e32-9aac-47c1cf6aa612","81310d4c-a998-4799-9f30-cd69b1698720","d98f45af-5592-4057-a796-e90a6a179811","0576d5f1-e72c-40c2-918f-76a88f46db37","9e46e38a-6a88-453f-b645-050cef2805d2","e0a662b6-09af-47e4-af46-a7ae142b1225","89dc8a23-7a9c-4a66-845e-289d4ed8206c","e5e4f740-5fe7-47e4-95f9-c74bd2adbfcd","bb8235a9-4700-4f5c-b3ec-39c915700b36","5d592407-a16b-4395-a741-bd1f0c0abba9","98bcc864-707a-42a8-9315-136347a74bbc","4106414f-3ff6-4eef-b08b-4047fa8e59c2","7068fdfc-116a-4ad2-933e-611a5cfb30c3","4a9159a0-51cb-4b53-9ad3-a75acc9e08d4","9abcc5b6-a1ba-4e92-a361-e98b919ce458","97aa50be-dee1-493a-9d87-33c18b3d2118","8a38bbbf-b9c3-4d96-b65a-7fa0b5ce2d54","1011d570-8eab-45bc-9460-85a4dad1827b","752e8374-5b1d-4e59-9f8a-0c331a77931d","8e5283d4-4bd1-4107-9de1-3b626c31259c","ac6edb5e-40f9-4de2-b7bd-8f245e4dd7af","a9ff620c-dbee-48ee-9156-c057b85c0766","30e15b59-cef8-46eb-a2b0-ac6461b322d4","231735d2-1015-4cac-a64e-8171dc788fef","c7acbc0b-0a56-4650-aa3d-364f988e2b08","8934455e-025b-4b46-aa3a-53478f4f2c7a","387148f8-4747-46aa-848b-c15b0e7bc8b1","d21071fa-a52b-4119-92aa-3ae4a689f25e","9e541795-d40a-412e-a218-16ce4af69367","5f677d45-0a8f-442e-8c01-844353fd63a5","bed4cfe4-ae4d-4404-be5b-9163d823d358","6a91998b-c72e-44b6-b417-0dca3ba704ee","061ee495-d579-4cb6-8bfe-d85e01b9bff1","224fd49b-8e74-4c6b-a9b8-5486c558ee29","4c8e837e-1516-4dd7-97ef-d80134593734","3fe44ec4-a4b0-4a60-aa3b-82aff4f04e13","a3facfd4-732e-476b-a647-b33c406fed43","4c15dd48-6ab0-4ea4-9537-43247bd86ada","f1ad4ed7-b87d-4ebb-99a8-1a1481ccd8ed","c14e7f62-c3d6-4034-90dd-3c5c7468094d","a5db69ca-e1ab-49f7-b0f6-5b0d5525de69","43186572-acce-473c-aa7c-ecf622c08808","894f81af-c600-4e3e-ab31-5b5a7806c12e","b554f847-bf2f-477c-8ab4-051220685d95","d4ceba4a-1548-496b-a078-5c4a266a6821","d267a9c1-d237-43b6-b88e-98b7abdcf0ab","9d3cdb38-cc12-4ef0-ba7e-b6c4daa9f29f","cbbde329-23fc-4883-b5d0-8ed6992bdb1d","fee7c100-7bb4-4663-ad01-dff0f9c2e169","3283e945-b41d-4875-9471-6c863c386326","5de300af-89c5-4411-af86-7eeb407c6b22","f9b6c2e9-db69-49ee-bf57-c1787e344962","d55efc54-ce56-4ad4-a76f-daa39b4f7047","5611a4c2-acdc-4216-a8e4-08b47f389637","aad9bafa-fc9a-4da3-8d0b-a40b0aa045ef"],"format":{"page_icon":"🎮","page_cover":"https://images.unsplash.com/photo-1563209259-b2fa97148ce1?ixlib=rb-4.0.3\u0026q=80\u0026fm=jpg\u0026crop=entropy\u0026cs=tinysrgb","page_cover_position":0.5},"created_time":1676044235713,"last_edited_time":1677651620138,"parent_id":"ba8460cf-4781-486e-8976-01358ef4659d","parent_table":"collection","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"4f20ede8-ccf7-4ae1-82e5-819e100dd032":{"role":"reader","value":{"id":"4f20ede8-ccf7-4ae1-82e5-819e100dd032","version":129,"type":"collection_view","view_ids":["d91647c5-6a81-48b1-a1ff-a04529d0ddba","27bc73a3-5779-44e0-b617-2f0d26f5aa2f"],"collection_id":"ba8460cf-4781-486e-8976-01358ef4659d","format":{"collection_pointer":{"id":"ba8460cf-4781-486e-8976-01358ef4659d","table":"collection","spaceId":"4af10338-3e65-4b50-af9f-798d59d5c8f6"},"copied_from_pointer":{"id":"3e3073e9-7aee-481c-b831-765e112ec7b5","table":"block","spaceId":"fde5ac74-eea3-4527-8f00-4482710e1af3"}},"created_time":1675998770865,"last_edited_time":1677136438825,"parent_id":"6246082f-4014-4d06-98ab-59e9840b298a","parent_table":"block","alive":true,"copied_from":"3e3073e9-7aee-481c-b831-765e112ec7b5","created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"6246082f-4014-4d06-98ab-59e9840b298a":{"role":"reader","value":{"id":"6246082f-4014-4d06-98ab-59e9840b298a","version":609,"type":"page","properties":{"title":[["Jang. Inspiration"]]},"content":["651dcdf3-689d-42d2-8497-64f8509d3504","0bd6df0e-6679-499e-b382-c9dc2c597776","f4c89bb7-a90e-41d7-a678-588b3deff765","d5557a1e-de5e-4085-896d-362a19928b69","4f20ede8-ccf7-4ae1-82e5-819e100dd032","ce518f27-4e46-4e98-ac75-13c467c1370c","dfee9c57-3be6-41db-8113-a54eeef675a7","1109cf3f-d8f9-4532-a13e-1af177ad4fdd","5855fe5e-17e2-4f14-8b66-702838bcc734"],"format":{"page_icon":"https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b29e9b03-c79c-4e52-a45e-7228163ba524/compass-circular-tool_(3).png","page_cover":"/images/page-cover/nasa_reduced_gravity_walking_simulator.jpg","copied_from_pointer":{"id":"78754261-97cf-4616-9880-9def95960ebf","table":"block","spaceId":"fde5ac74-eea3-4527-8f00-4482710e1af3"},"page_cover_position":0.5},"permissions":[{"role":"editor","type":"user_permission","user_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb"},{"role":"reader","type":"public_permission","added_timestamp":1675998834095}],"created_time":1675998770872,"last_edited_time":1677392850666,"parent_id":"3a2d56c9-1da9-4a62-b698-f0ad3576c8c1","parent_table":"block","alive":true,"copied_from":"78754261-97cf-4616-9880-9def95960ebf","file_ids":["f70d3dc6-ce97-4be2-9cde-b86606147b41","a2bd3317-78e4-48bc-8d27-9b733175a416","7fae9664-8795-4723-844e-0adecdea62dc","4235c094-2110-4aa6-b058-6b5fe220dbb7","c8194a03-81d0-482d-a7de-f491a6e85f54","7eb95609-c81b-48c1-969e-5ef2f220bc5a","160057d8-120e-4f9f-8c1f-6bcf31a50f15","0cb9278b-708b-4da1-929a-6696aa8cdfa3","3eb9471e-9b71-4bd3-a13d-c33158a442be","b29e9b03-c79c-4e52-a45e-7228163ba524"],"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"ed886261-c38e-4c5f-b82d-7478ed25cf11":{"role":"reader","value":{"id":"ed886261-c38e-4c5f-b82d-7478ed25cf11","version":27,"type":"quote","properties":{"title":[["본 포스트는 “파이썬과 케라스로 배우는 강화학습” 도서의 두번째 리뷰 포스트입니다."]]},"created_time":1676044667522,"last_edited_time":1676055484545,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"1411732a-9b34-4133-8adf-1021ba502655":{"role":"reader","value":{"id":"1411732a-9b34-4133-8adf-1021ba502655","version":9,"type":"text","properties":{"title":[["http://www.yes24.com/Product/Goods/44136413",[["a","http://www.yes24.com/Product/Goods/44136413"]]]]},"created_time":1676055485957,"last_edited_time":1676055489733,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"5ad32513-6f08-4205-905f-70db4fe624ba":{"role":"reader","value":{"id":"5ad32513-6f08-4205-905f-70db4fe624ba","version":9,"type":"text","created_time":1676049215990,"last_edited_time":1676049217490,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"43e688bc-6126-4419-b9da-67483caf50aa":{"role":"reader","value":{"id":"43e688bc-6126-4419-b9da-67483caf50aa","version":4,"type":"table_of_contents","format":{"block_color":"gray"},"created_time":1676049227368,"last_edited_time":1676049227369,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"ddcf4d6d-2b60-44cd-b0c0-467338e8d2e2":{"role":"reader","value":{"id":"ddcf4d6d-2b60-44cd-b0c0-467338e8d2e2","version":4,"type":"text","created_time":1676049215990,"last_edited_time":1676049216122,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"16877cb1-fbea-4651-b5df-59a985de8d7b":{"role":"reader","value":{"id":"16877cb1-fbea-4651-b5df-59a985de8d7b","version":2,"type":"sub_header","properties":{"title":[["2장 강화학습 기초 1: MDP(Markov Decision Process)",[["b"]]]]},"created_time":1676044667522,"last_edited_time":1676044667537,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"86e937c4-3b99-4b80-a0ce-ce906c8809ce":{"role":"reader","value":{"id":"86e937c4-3b99-4b80-a0ce-ce906c8809ce","version":2,"type":"divider","created_time":1676044667523,"last_edited_time":1676044667537,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"3168087e-4a43-4c1a-8f12-463a9755a2a1":{"role":"reader","value":{"id":"3168087e-4a43-4c1a-8f12-463a9755a2a1","version":2,"type":"sub_sub_header","properties":{"title":[["MDP",[["b"]]]]},"created_time":1676044667523,"last_edited_time":1676044667537,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"e5e1b931-3eb6-4e32-9aac-47c1cf6aa612":{"role":"reader","value":{"id":"e5e1b931-3eb6-4e32-9aac-47c1cf6aa612","version":2,"type":"text","properties":{"title":[["앞서 살펴보았듯 "],["MDP",[["c"]]],["는 순차적으로 결정해야하는 문제를 수학적으로 표현한다."]]},"created_time":1676044667523,"last_edited_time":1676044667537,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"81310d4c-a998-4799-9f30-cd69b1698720":{"role":"reader","value":{"id":"81310d4c-a998-4799-9f30-cd69b1698720","version":8,"type":"image","properties":{"source":[["https://longshiine.github.io/2021/01/20/Reinforcement-Learning-Basic-1/MDP.png"]],"caption":[["MDP의 구성요소"]]},"format":{"block_width":480,"block_full_width":false,"block_page_width":false},"created_time":1676044667523,"last_edited_time":1676052102599,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"d98f45af-5592-4057-a796-e90a6a179811":{"role":"reader","value":{"id":"d98f45af-5592-4057-a796-e90a6a179811","version":2,"type":"text","properties":{"title":[["문제를 잘못 정의하면 에이전트가 학습을 못할 수도 있다. 따라서 문제의 정의는 에이전트가 학습하는 데 가장 중요한 단계 중 하나이다. 에이전트를 구현하는 사람은 학습하기에 많지도 않고 적지도 않은 "],["적절한 정보",[["b"]]],["를 에이전트가 알 수도 있도록 문제를 정의해야 한다. "],["MDP",[["c"]]],["의 이해를 돕기위해 이제 "],["그리드 월드(Grid World)",[["c"]]],["라는 예제를 살펴볼 것인데, 그리드는 격자이고 그리드 월드는 격자로 이뤄진 환경에서 문제를 푸는 각종 예제를 뜻한다. 어디한번 MDP의 구성요소를 하나하나 살펴보자."]]},"created_time":1676044667524,"last_edited_time":1676044667537,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"0576d5f1-e72c-40c2-918f-76a88f46db37":{"role":"reader","value":{"id":"0576d5f1-e72c-40c2-918f-76a88f46db37","version":5,"type":"text","created_time":1676044724467,"last_edited_time":1676044724469,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"9e46e38a-6a88-453f-b645-050cef2805d2":{"role":"reader","value":{"id":"9e46e38a-6a88-453f-b645-050cef2805d2","version":2,"type":"sub_sub_header","properties":{"title":[["상태",[["b"]]]]},"created_time":1676044667524,"last_edited_time":1676044667537,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"e0a662b6-09af-47e4-af46-a7ae142b1225":{"role":"reader","value":{"id":"e0a662b6-09af-47e4-af46-a7ae142b1225","version":9,"type":"text","properties":{"title":[["⁍",[["e","S"]]],[" 는 에이전트가 관찰 가능한 상태의 집합이다. 상태라는 말의 의미가 모호할 수 있는데 "],["자신의 상황에 대한 관찰",[["b"]]],[" 이 상태에 대한 가장 정확한 표현이다. 로봇과 같은 실제 세상에서의 에이전트에게 상태는 센서 값이 될 것이다. 하지만 이 책에서와 같이 게임을 학습하기 위한 에이전트는 사용자가 상태를 정의해줘야 한다. 이때 "],["‘내가 정의하는 상태가 에이전트가 학습하기에 충분한 정보를 주는 것인가?’",[["b"]]],[" 라는 질문을 스스로에게 던져보는 것이 좋다."]]},"created_time":1676044667524,"last_edited_time":1676044836499,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"89dc8a23-7a9c-4a66-845e-289d4ed8206c":{"role":"reader","value":{"id":"89dc8a23-7a9c-4a66-845e-289d4ed8206c","version":8,"type":"image","properties":{"source":[["https://longshiine.github.io/2021/01/20/Reinforcement-Learning-Basic-1/greedworld.png"]],"caption":[["그리드월드에서 상태는 좌표를 의미한다"]]},"format":{"block_width":480,"block_full_width":false,"block_page_width":false},"created_time":1676044667524,"last_edited_time":1676052099965,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"e5e4f740-5fe7-47e4-95f9-c74bd2adbfcd":{"role":"reader","value":{"id":"e5e4f740-5fe7-47e4-95f9-c74bd2adbfcd","version":2,"type":"text","properties":{"title":[["위의 그림과 같이 그리드월드(5x5)에서는 "],["격자 상의 각 위치(좌표)",[["b"]]],[" 가 상태가 된다. 그리드월드(5x5)의 상태는 모든 격자의 위치로서 총 25개가 있다. 각 상태는 (x,y)로 이뤄진 좌표로서 그리드월드의 가로축이 x축이고 세로축이 y축이다. 식으로는 다음과 같이 표현할 수 있다."]]},"created_time":1676044667524,"last_edited_time":1676044667537,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"bb8235a9-4700-4f5c-b3ec-39c915700b36":{"role":"reader","value":{"id":"bb8235a9-4700-4f5c-b3ec-39c915700b36","version":24,"type":"equation","properties":{"title":[["S = \\{ (1,1),(1,2),(1,3),...,(5,5)\\}"]]},"created_time":1676044667524,"last_edited_time":1676044795120,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"5d592407-a16b-4395-a741-bd1f0c0abba9":{"role":"reader","value":{"id":"5d592407-a16b-4395-a741-bd1f0c0abba9","version":44,"type":"text","properties":{"title":[["에이전트는 시간에 따라 25개의 상태의 집합 안에 있는 상태를 탐험하게 된다. 시간은 t라고 표현하고, 시간 t일때의 상태를 "],["⁍",[["e","S_t"]]],[" 라고 표현하는데, 만약 시간이 t일때 상태가 (1,3)이라면 "],["⁍",[["e","S_t=(1,3)"]]],[" 와 같이 표현한다."]]},"created_time":1676044667524,"last_edited_time":1676044873268,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"98bcc864-707a-42a8-9315-136347a74bbc":{"role":"reader","value":{"id":"98bcc864-707a-42a8-9315-136347a74bbc","version":49,"type":"text","properties":{"title":[["MDP",[["c"]]],[" 에서 상태는 시간에 따라 확률적으로 변한다. t = 1일때 상태는 "],["⁍",[["e","S_t=(1,3)"]]],[" 일 수도 "],["⁍",[["e","S_t=(4,2)"]]],[" 일 수도 있다. 시간 t에서 에이전트가 있을 상태가 "],["확률 변수",[["b"]]],[" 라는 뜻이다. 예를 들어 주사위를 던진다고 할때, 주사위를 던지는 실험은 임의 실험이고 주사위를 던져서 나오는 값은 변수가 된다. 임의의 실험에서 나오는 변수는 자신이 나타날 확률값을 가지고 있으며, 이 변수는 확률 변수가 된다. 보통 "],["“시간 t에서의 상태 ",[["b"]]],["⁍",[["e","S_t"],["b"]]],["가 어떤 상태 ",[["b"]]],["⁍",[["e","s"],["b"]]],["다”",[["b"]]],[" 를 표현할때 "],["⁍",[["e","S_t=s"]]],["와 같이 적는다."]]},"created_time":1676044667525,"last_edited_time":1676044931148,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"4106414f-3ff6-4eef-b08b-4047fa8e59c2":{"role":"reader","value":{"id":"4106414f-3ff6-4eef-b08b-4047fa8e59c2","version":5,"type":"text","created_time":1676044979929,"last_edited_time":1676044979932,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"7068fdfc-116a-4ad2-933e-611a5cfb30c3":{"role":"reader","value":{"id":"7068fdfc-116a-4ad2-933e-611a5cfb30c3","version":46,"type":"quote","properties":{"title":[["임의의 시간 t에서의 상태 =\u003e "],["⁍",[["e","S_t"]]]]},"created_time":1676044667525,"last_edited_time":1676045019753,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"4a9159a0-51cb-4b53-9ad3-a75acc9e08d4":{"role":"reader","value":{"id":"4a9159a0-51cb-4b53-9ad3-a75acc9e08d4","version":29,"type":"quote","properties":{"title":[["⁍",[["e","S_t"]]],["가 어떤 상태 "],["⁍",[["e","s"]]],["다 =\u003e "],["⁍",[["e","S_t=s"]]]]},"created_time":1676045019747,"last_edited_time":1676045029479,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"9abcc5b6-a1ba-4e92-a361-e98b919ce458":{"role":"reader","value":{"id":"9abcc5b6-a1ba-4e92-a361-e98b919ce458","version":4,"type":"text","created_time":1676044978644,"last_edited_time":1676044978644,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"97aa50be-dee1-493a-9d87-33c18b3d2118":{"role":"reader","value":{"id":"97aa50be-dee1-493a-9d87-33c18b3d2118","version":4,"type":"sub_sub_header","properties":{"title":[["행동",[["b"]]]]},"created_time":1676044667525,"last_edited_time":1676044978644,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"8a38bbbf-b9c3-4d96-b65a-7fa0b5ce2d54":{"role":"reader","value":{"id":"8a38bbbf-b9c3-4d96-b65a-7fa0b5ce2d54","version":40,"type":"text","properties":{"title":[["에이전트가 상태"],["⁍",[["e"," S_t"]]],["에서 할 수 있는 가능한 행동의 집합은 "],["⁍",[["e","A"]]],["이다. 보통 에이전트가 할 수 있는 행동은 모든 상태에서 같다. 따라서 하나의 집합 "],["⁍",[["e","A"]]],["로 나타낼 수 있다. "],["시간 t에서의 행동 a",[["b"]]],["는 "],["⁍",[["e","A_t=a"]]],["와 같이 표현하며, t라는 시간에 에이전트가 어떤 행동을 할지 정해져 있는 것이 아니므로 "],["⁍",[["e","A_t"]]],["와 같이 대문자로 표현한다. 즉 확률변수이다 "]]},"created_time":1676044667525,"last_edited_time":1676045104899,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"1011d570-8eab-45bc-9460-85a4dad1827b":{"role":"reader","value":{"id":"1011d570-8eab-45bc-9460-85a4dad1827b","version":5,"type":"text","properties":{"title":[["그리드월드에서 에이전트가 할 수 있는 행동의 집합은 아래와 같다."]]},"created_time":1676045104896,"last_edited_time":1676045104899,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"752e8374-5b1d-4e59-9f8a-0c331a77931d":{"role":"reader","value":{"id":"752e8374-5b1d-4e59-9f8a-0c331a77931d","version":13,"type":"equation","properties":{"title":[["⁍",[["e","A={up,down,left,right}"]]]]},"created_time":1676044667525,"last_edited_time":1676045290531,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"8e5283d4-4bd1-4107-9de1-3b626c31259c":{"role":"reader","value":{"id":"8e5283d4-4bd1-4107-9de1-3b626c31259c","version":20,"type":"text","properties":{"title":[["만약 시간 t에서 St=(3,1)이고 At=right 라면 St+1=(4,1)이다. 그런데 바람과 같은 예상치 못한 요소가 있다면 에이전트는 (4,1)에 도달하지 못할 수도 있고, 이러한 요소를 포함하여 에이전트가 어디로 이동할지 결정하는 것을 "],["상태 변환 확률",[["c"]]],["이라고 한다. 이는 조금 뒤 자세히 다루도록 하겠다."]]},"created_time":1676044667526,"last_edited_time":1676045195554,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"ac6edb5e-40f9-4de2-b7bd-8f245e4dd7af":{"role":"reader","value":{"id":"ac6edb5e-40f9-4de2-b7bd-8f245e4dd7af","version":5,"type":"text","created_time":1676045273471,"last_edited_time":1676045273474,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"a9ff620c-dbee-48ee-9156-c057b85c0766":{"role":"reader","value":{"id":"a9ff620c-dbee-48ee-9156-c057b85c0766","version":2,"type":"sub_sub_header","properties":{"title":[["보상함수",[["b"]]]]},"created_time":1676044667526,"last_edited_time":1676044667537,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"30e15b59-cef8-46eb-a2b0-ac6461b322d4":{"role":"reader","value":{"id":"30e15b59-cef8-46eb-a2b0-ac6461b322d4","version":6,"type":"text","properties":{"title":[["보상(reward)",[["b"]]],[" 은 에이전트가 학습할 수 있는 유일한 정보로서 환경이 에이전트에게 주는 정보이다. 시간 t에서 상태가 St=s이고 행동이 At=a일 때 보상함수는 아래와 같이 정의된다."]]},"created_time":1676044667526,"last_edited_time":1676045201544,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"231735d2-1015-4cac-a64e-8171dc788fef":{"role":"reader","value":{"id":"231735d2-1015-4cac-a64e-8171dc788fef","version":12,"type":"equation","properties":{"title":[["⁍",[["e","r(s,a)=E[R_{t+1}|S_t=s,A_t=a]"]]]]},"created_time":1676044667526,"last_edited_time":1676045295997,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"c7acbc0b-0a56-4650-aa3d-364f988e2b08":{"role":"reader","value":{"id":"c7acbc0b-0a56-4650-aa3d-364f988e2b08","version":21,"type":"text","properties":{"title":[["보상함수는 시간 t일 때 상태가 "],["⁍",[["e","S_t=s"]]],["이고 그 상태에서 행동 "],["⁍",[["e","A_t=a"]]],["를 했을 경우에 받을 보상에 대한 기댓값 "],["⁍",[["e","E"]]],["이다. 여기서 "],["기댓값",[["b"]]],[" 은 무엇일까? (한번쯤 쉽게 풀어써보고 싶었는데 책의 예시가 아주 적절하다🤩)"]]},"created_time":1676044667526,"last_edited_time":1676045223211,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"8934455e-025b-4b46-aa3a-53478f4f2c7a":{"role":"reader","value":{"id":"8934455e-025b-4b46-aa3a-53478f4f2c7a","version":21,"type":"text","properties":{"title":[["기댓값",[["b"]]],[" 이란 일종의 평균이다. 주사위의 기댓값을 한번 생각해보면, 모든 면이 "],["⁍",[["e","1/6"],["c"]]],["의 동등한 확률로 나올테고, 주사위에서는 다음과 같은 값이 나올 것이라고 "],["기대",[["b"]]],[" 할 수 있다."]]},"created_time":1676044667526,"last_edited_time":1676094648770,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"387148f8-4747-46aa-848b-c15b0e7bc8b1":{"role":"reader","value":{"id":"387148f8-4747-46aa-848b-c15b0e7bc8b1","version":115,"type":"equation","properties":{"title":[["기댓값_{주사위}=1∗{1\\over6}+2∗{1\\over6}+3∗{1\\over6}+4∗{1\\over6}+5∗{1\\over6}+6∗{1\\over6}={21\\over6}"]]},"created_time":1676044667527,"last_edited_time":1676051996599,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"d21071fa-a52b-4119-92aa-3ae4a689f25e":{"role":"reader","value":{"id":"d21071fa-a52b-4119-92aa-3ae4a689f25e","version":2,"type":"text","properties":{"title":[["다시 본론으로 돌아와, "],["보상함수는 왜 기댓값으로 표현하는 것일까?",[["b"]]]]},"created_time":1676044667527,"last_edited_time":1676044667537,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"9e541795-d40a-412e-a218-16ce4af69367":{"role":"reader","value":{"id":"9e541795-d40a-412e-a218-16ce4af69367","version":2,"type":"quote","properties":{"title":[["보상을 에이전트에게 주는 것은 환경이고,환경에 따라서 같은 상태에서 같은 행동을 취하더라도 다른 보상을 줄 수 있기 때문이다."]]},"created_time":1676044667527,"last_edited_time":1676044667537,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"5f677d45-0a8f-442e-8c01-844353fd63a5":{"role":"reader","value":{"id":"5f677d45-0a8f-442e-8c01-844353fd63a5","version":2,"type":"text","properties":{"title":[["또한 보상함수에서 특이한 점은 에이전트가 어떤 상태에서 행동한 것은 시간 t에서인데 보상을 받는 것은 t+1이라는 것이다. 이는 보상을 에이전트가 알고 있는 것이 아니라 환경이 알려주기 때문이다. 때문에 에이전트가 받는 보상은 하나의 시간 단위가 지난 다음에 주어진다. 이 시간단위를 앞으로 "],["타임스텝(time step)",[["c"]]],["이라고 한다."]]},"created_time":1676044667528,"last_edited_time":1676044667537,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"bed4cfe4-ae4d-4404-be5b-9163d823d358":{"role":"reader","value":{"id":"bed4cfe4-ae4d-4404-be5b-9163d823d358","version":5,"type":"text","created_time":1676045458099,"last_edited_time":1676045458102,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"6a91998b-c72e-44b6-b417-0dca3ba704ee":{"role":"reader","value":{"id":"6a91998b-c72e-44b6-b417-0dca3ba704ee","version":2,"type":"sub_sub_header","properties":{"title":[["상태 변환 확률",[["b"]]]]},"created_time":1676044667528,"last_edited_time":1676044667537,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"061ee495-d579-4cb6-8bfe-d85e01b9bff1":{"role":"reader","value":{"id":"061ee495-d579-4cb6-8bfe-d85e01b9bff1","version":23,"type":"text","properties":{"title":[["에이전트가 어떤 상태에서 어떤 행동을 취한다면 에이전트의 상태는 변할 것이다. "],["⁍",[["e","s'"]]],["은 다음 스텝에 에이전트가 갈 수 있는 어떤 특정한 상태를 의미하는데, 꼭 다음 상태에 도달하리라는 보장은 없다. 옆에서 바람이 불 수도 있고 갑자기 넘어질 수 있는 것이다. 이처럼 상태의 변화에는 확률적인 요인이 들어가고, 이를 수치적으로 표현한 것이 "],["상태 변환 확률",[["c"]]],[" "],["⁍",[["e","P"]]],["이다."]]},"created_time":1676044667528,"last_edited_time":1676045558043,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"224fd49b-8e74-4c6b-a9b8-5486c558ee29":{"role":"reader","value":{"id":"224fd49b-8e74-4c6b-a9b8-5486c558ee29","version":9,"type":"equation","properties":{"title":[["⁍",[["e","P_{ss′}^a=P[S_{t+1}=s′|S_t=s,A_t=a]"]]]]},"created_time":1676044667528,"last_edited_time":1676052072164,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"4c8e837e-1516-4dd7-97ef-d80134593734":{"role":"reader","value":{"id":"4c8e837e-1516-4dd7-97ef-d80134593734","version":21,"type":"text","properties":{"title":[["상태 변환 확률",[["c"]]],["은 상태 "],["⁍",[["e","s"]]],["에서 행동 "],["⁍",[["e","a"]]],["를 취했을 때 다른 상태 "],["⁍",[["e","s′"],["b"]]],["으로 도달할 확률이다. 이 값은 보상과 마찬가지로 에이전트가 알지 못하는 값으로서 에이전트가 아닌 환경의 일부이다. 다른 말로 "],["환경의 모델(model)",[["b"]]],[" 이라 부르기도 한다."]]},"created_time":1676044667528,"last_edited_time":1676045575070,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"3fe44ec4-a4b0-4a60-aa3b-82aff4f04e13":{"role":"reader","value":{"id":"3fe44ec4-a4b0-4a60-aa3b-82aff4f04e13","version":2,"type":"text","properties":{"title":[["환경은 에이전트가 행동을 취하면 "],["상태 변환 확률",[["c"]]],["을 통해 다음에 에이전트가 갈 상태를 알려준다."]]},"created_time":1676044667528,"last_edited_time":1676044667537,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"a3facfd4-732e-476b-a647-b33c406fed43":{"role":"reader","value":{"id":"a3facfd4-732e-476b-a647-b33c406fed43","version":5,"type":"text","created_time":1676045578482,"last_edited_time":1676045578488,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"4c15dd48-6ab0-4ea4-9537-43247bd86ada":{"role":"reader","value":{"id":"4c15dd48-6ab0-4ea4-9537-43247bd86ada","version":2,"type":"sub_sub_header","properties":{"title":[["할인율",[["b"]]]]},"created_time":1676044667528,"last_edited_time":1676044667537,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"f1ad4ed7-b87d-4ebb-99a8-1a1481ccd8ed":{"role":"reader","value":{"id":"f1ad4ed7-b87d-4ebb-99a8-1a1481ccd8ed","version":2,"type":"text","properties":{"title":[["에이전트가 항상 현재에 판단을 내리기 때문에 현재에 가까운 보상일 수록 더 큰 가치를 지닌다. 가령 우리가 10억원 짜리 복권에 당첨되었다고 생각해보자. 지금 당장 받을 수도 10년 뒤에 받을 수도 있다고 할때, 우리는 당연히 당장 받는 것을 선호할 것이다. 시간이 지남에 따라 이자가 붙을 것을 가정하기 때문이다. 이는 다른 말로 "],["같은 보상이면 나중에 받을수록 가치가 줄어든다",[["b"]]],[" 고 말할 수 있다. 강화학습에도 이와 같은 가정이 적용되고 이를 수학적으로 표현하기 위해 "],["할인율(Discount Factor)",[["c"]]],["이라는 개념을 도입한다."]]},"created_time":1676044667528,"last_edited_time":1676044667537,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"c14e7f62-c3d6-4034-90dd-3c5c7468094d":{"role":"reader","value":{"id":"c14e7f62-c3d6-4034-90dd-3c5c7468094d","version":9,"type":"equation","properties":{"title":[["⁍",[["e","γ∈[0,1]"]]]]},"created_time":1676044667528,"last_edited_time":1676045632825,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"a5db69ca-e1ab-49f7-b0f6-5b0d5525de69":{"role":"reader","value":{"id":"a5db69ca-e1ab-49f7-b0f6-5b0d5525de69","version":13,"type":"text","properties":{"title":[["할인율",[["c"]]],["은 0과 1사이의 값이고, 보상에 곱해지면 보상이 감소한다. 이렇게 미래의 가치를 현재의 가치로 환산하는 것을 "],["할인한다",[["b"]]],[" 고 하고, 시간에 따라 할인하는 비율을 할인율이라고 한다. 만약 현재의 시간 t로부터 시간 k가 지난 후에 보상을 "],["⁍",[["e","R_{t+k}"]]],["만큼 받을 것이라고 하면 그 보상의 가치는 아래와 같다"]]},"created_time":1676044667528,"last_edited_time":1676045595943,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"43186572-acce-473c-aa7c-ecf622c08808":{"role":"reader","value":{"id":"43186572-acce-473c-aa7c-ecf622c08808","version":9,"type":"equation","properties":{"title":[["⁍",[["e","γ^{k−1}R_{t+k}"]]]]},"created_time":1676044667529,"last_edited_time":1676045627925,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"894f81af-c600-4e3e-ab31-5b5a7806c12e":{"role":"reader","value":{"id":"894f81af-c600-4e3e-ab31-5b5a7806c12e","version":2,"type":"quote","properties":{"title":[["더 먼 미래에 받는 보상일수록 현재의 에이전트는 더 작은 값으로 받아들인다."]]},"created_time":1676044667529,"last_edited_time":1676044667537,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"b554f847-bf2f-477c-8ab4-051220685d95":{"role":"reader","value":{"id":"b554f847-bf2f-477c-8ab4-051220685d95","version":5,"type":"text","created_time":1676045688517,"last_edited_time":1676045688520,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"d4ceba4a-1548-496b-a078-5c4a266a6821":{"role":"reader","value":{"id":"d4ceba4a-1548-496b-a078-5c4a266a6821","version":2,"type":"sub_sub_header","properties":{"title":[["정책",[["b"]]]]},"created_time":1676044667529,"last_edited_time":1676044667537,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"d267a9c1-d237-43b6-b88e-98b7abdcf0ab":{"role":"reader","value":{"id":"d267a9c1-d237-43b6-b88e-98b7abdcf0ab","version":12,"type":"text","properties":{"title":[["정책",[["c"]]],["은 모든 상태에서 에이전트가 할 행동이다. 상태가 입력으로 들어오면 행동을 출력으로 내보내는 "],["일종의 함수",[["b"]]],[" 라고 생각해도 좋다. 정책은 각 상태에서 단 하나의 행동만을 나타낼 수도 있고, 확률적으로 "],["⁍",[["e","a_1=10, a_2=90"]]],["와 같이 나타낼 수도 있다. 에이전트가 학습하고 있을 땐, 정책이 하나의 행동만을 선택하기 보다는 확률적으로 여러개의 행동을 선택할 수 있어야 하며 수식으로 나타내면 아래와 같다."]]},"created_time":1676044667529,"last_edited_time":1676045665951,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"9d3cdb38-cc12-4ef0-ba7e-b6c4daa9f29f":{"role":"reader","value":{"id":"9d3cdb38-cc12-4ef0-ba7e-b6c4daa9f29f","version":17,"type":"equation","properties":{"title":[["⁍",[["e","π(a|s)=P[A_t=a|S_t=s]"]]]]},"created_time":1676044667529,"last_edited_time":1676045686521,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"cbbde329-23fc-4883-b5d0-8ed6992bdb1d":{"role":"reader","value":{"id":"cbbde329-23fc-4883-b5d0-8ed6992bdb1d","version":2,"type":"text","properties":{"title":[["이러한 정책은 하나의 예시로서 각 상태마다 어떤 행동을 해야할지 아래의 그림과 같이 알려준다."]]},"created_time":1676044667529,"last_edited_time":1676044667537,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"fee7c100-7bb4-4663-ad01-dff0f9c2e169":{"role":"reader","value":{"id":"fee7c100-7bb4-4663-ad01-dff0f9c2e169","version":10,"type":"image","properties":{"source":[["https://longshiine.github.io/2021/01/20/Reinforcement-Learning-Basic-1/policy.png"]],"caption":[["그리드월드에서의 정책"]]},"format":{"block_width":384,"block_full_width":false,"block_page_width":false},"created_time":1676044667529,"last_edited_time":1676052094099,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"3283e945-b41d-4875-9471-6c863c386326":{"role":"reader","value":{"id":"3283e945-b41d-4875-9471-6c863c386326","version":2,"type":"text","properties":{"title":[["정책만 가지고 있으면 에이전트는 사실 모든 상태에서 자신이 해야 할 행동을 알 수 있다. 그러나 강화학습 문제를 통해 알고 싶은 것은 그냥 정책이 아니라 "],["최적의 정책",[["b"]]],[" 이다."]]},"created_time":1676044667529,"last_edited_time":1676044667537,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"5de300af-89c5-4411-af86-7eeb407c6b22":{"role":"reader","value":{"id":"5de300af-89c5-4411-af86-7eeb407c6b22","version":5,"type":"text","created_time":1676045701800,"last_edited_time":1676045701803,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"f9b6c2e9-db69-49ee-bf57-c1787e344962":{"role":"reader","value":{"id":"f9b6c2e9-db69-49ee-bf57-c1787e344962","version":2,"type":"quote","properties":{"title":[["최적 정책을 얻기 위해서 현재의 정책보다 더 좋은 정책을 학습해나가는 것이 강화학습이다."]]},"created_time":1676044667529,"last_edited_time":1676044667537,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"d55efc54-ce56-4ad4-a76f-daa39b4f7047":{"role":"reader","value":{"id":"d55efc54-ce56-4ad4-a76f-daa39b4f7047","version":5,"type":"text","created_time":1676045693578,"last_edited_time":1676045693581,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"5611a4c2-acdc-4216-a8e4-08b47f389637":{"role":"reader","value":{"id":"5611a4c2-acdc-4216-a8e4-08b47f389637","version":2,"type":"sub_sub_header","properties":{"title":[["정리",[["b"]]]]},"created_time":1676044667529,"last_edited_time":1676044667537,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"aad9bafa-fc9a-4da3-8d0b-a40b0aa045ef":{"role":"reader","value":{"id":"aad9bafa-fc9a-4da3-8d0b-a40b0aa045ef","version":2,"type":"text","properties":{"title":[["이처럼 "],["MDP",[["c"]]],["를 통해 순차적 행동 결정 문제를 정의했다. 에이전트가 현재 상태에서 앞으로 받을 보상들을 고려해서 행동을 결정한다. 그러면 환경은 에이전트에게 실제 보상과 다음 상태를 알려준다. 이러한 과정을 반복하면서 에이전트는 어떤 상태에서 앞으로 받을 것이라 예상했던 보상에 대해 틀렸다는 것을 알게 된다. 이때 앞으로 받을 것이라 예상하는 보상을 "],["가치함수(Value Function)",[["c"]]],["라고 하며, 다음 장에서 설명된다. 그러한 과정에서 에이전트는 실제로 받은 보상을 토대로 가치함수와 정책을 바꿔나간다. 이러한 학습 과정을 충분히 반복한다면 가장 많은 보상을 받게하는 정책을 학습할 수 있다."]]},"created_time":1676044667529,"last_edited_time":1676044667537,"parent_id":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"3d335e41-f00a-45df-8432-34a266c7566a":{"role":"reader","value":{"id":"3d335e41-f00a-45df-8432-34a266c7566a","version":480,"type":"page","properties":{"title":[["About"]]},"content":["e1067486-5fb5-4dc3-9415-54abc63fc3a5","ab253a7b-1dec-4620-8c4e-e99f94e1f6c9","460c887f-cc3d-4351-8671-7cfbc6d3bc1d","c62b8a9d-e8cb-4c5b-be5a-48412e956c95","abe79af7-1342-438a-9a59-2d0b1ce59007","88d551b8-6b87-4540-89b9-3619a8fe4059","ae31ce74-317a-45ae-9dd3-16c14e764361","78f83c2a-05c2-4033-bcd8-945d700b2952","fd280029-9a4c-47fe-8e3c-ce8d3f146671","6a0d0c32-f1bc-4277-a128-33da8613ede4","2bcccf09-dccf-4896-8819-255cf75aabaf","d84b4bd5-3b6b-45dd-a807-5fea62c5213b","8b7fb4d3-0195-4f61-a0eb-330792f84053","e9394173-e197-4fad-a1db-ac74fdba3738","cb29948c-7258-4b05-9bfa-25bf19ad0946","a4e4243c-25aa-480d-a9ee-0af2230c7d76","172e2e25-e319-4766-ad38-02fc168994cd","6c428956-d84e-4855-a998-7a79f1fc8ea5","90f1cecf-6258-4b54-b0ee-26ee4d950872","3eec82e9-0211-4271-8c30-1dbc86ab3be1","8ce5f430-8c4c-4e8f-b6a4-23396731b27d","aeb9d85e-fffa-4c34-b253-cb0499f047a2","3d37f4c7-8064-4792-a863-234d8b4d05a6","77daa708-adbc-4092-b7f7-9e3c8a988049","26e3aca8-3e09-4a37-8c92-3c95cf579bbb"],"format":{"page_icon":"https://s3-us-west-2.amazonaws.com/secure.notion-static.com/fc853118-df50-43ed-96d9-0711493d5e25/jang_inspiration_logo.png","page_cover":"/images/page-cover/nasa_reduced_gravity_walking_simulator.jpg","copied_from_pointer":{"id":"f1199d37-579b-41cb-abfc-0b5174f4256a","table":"block","spaceId":"fde5ac74-eea3-4527-8f00-4482710e1af3"},"page_cover_position":0.5},"created_time":1675998770866,"last_edited_time":1677390731360,"parent_id":"d83b5165-627c-41b3-82f0-f1cbf904e176","parent_table":"block","alive":true,"copied_from":"f1199d37-579b-41cb-abfc-0b5174f4256a","file_ids":["c8028bee-f4c7-4736-8c36-fffcab5d977e","9407e769-d877-4de9-bbaa-9e5626d971ed","07d5a5b4-de2a-4322-bfcc-c4ade3a63b86","fc853118-df50-43ed-96d9-0711493d5e25"],"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"d83b5165-627c-41b3-82f0-f1cbf904e176":{"role":"reader","value":{"id":"d83b5165-627c-41b3-82f0-f1cbf904e176","version":17,"type":"column","content":["e1fe2c9c-6eb8-412c-9874-a8ac2fce9ed8","3d335e41-f00a-45df-8432-34a266c7566a"],"format":{"column_ratio":0.25},"created_time":1676020529237,"last_edited_time":1676091451534,"parent_id":"1109cf3f-d8f9-4532-a13e-1af177ad4fdd","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"1109cf3f-d8f9-4532-a13e-1af177ad4fdd":{"role":"reader","value":{"id":"1109cf3f-d8f9-4532-a13e-1af177ad4fdd","version":13,"type":"column_list","content":["91ac285d-9a69-4ec4-96f6-9b046a15647c","028efbb4-417e-4742-8474-dd7a2ddeb8ee","d63c036c-b99a-4aa9-a068-87abc40d37a6","d83b5165-627c-41b3-82f0-f1cbf904e176"],"format":{"block_width":720,"block_full_width":false,"block_page_width":true},"created_time":1676020365844,"last_edited_time":1676091451534,"parent_id":"6246082f-4014-4d06-98ab-59e9840b298a","parent_table":"block","alive":true,"created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"e1067486-5fb5-4dc3-9415-54abc63fc3a5":{"role":"reader","value":{"id":"e1067486-5fb5-4dc3-9415-54abc63fc3a5","version":86,"type":"text","properties":{"title":[["Instagram",[["h","blue_background"],["a","https://www.instagram.com/jang.inspiration/"]]],[" • "],["GitHub",[["h","teal_background"],["a","https://github.com/longshiine"]]],[" • "],["LinkedIn",[["h","pink_background"],["a","https://www.linkedin.com/in/jangyeong-kim-b7924422a/"]]]]},"format":{"copied_from_pointer":{"id":"0ab4ae84-463a-4b31-9430-f4ed513c5fc2","table":"block","spaceId":"fde5ac74-eea3-4527-8f00-4482710e1af3"}},"created_time":1675998770872,"last_edited_time":1676960330564,"parent_id":"3d335e41-f00a-45df-8432-34a266c7566a","parent_table":"block","alive":true,"copied_from":"0ab4ae84-463a-4b31-9430-f4ed513c5fc2","created_by_table":"notion_user","created_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","last_edited_by_table":"notion_user","last_edited_by_id":"3cb8c772-be03-4bd1-aa7d-b0df8495a3bb","space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}}},"collection":{"ba8460cf-4781-486e-8976-01358ef4659d":{"role":"reader","value":{"id":"ba8460cf-4781-486e-8976-01358ef4659d","version":117,"schema":{";KhU":{"name":"Last Updated","type":"last_edited_time"},"==~K":{"name":"Public","type":"checkbox"},"=bhc":{"name":"Curated","type":"checkbox"},"AfoN":{"name":"Category","type":"select","options":[{"id":"20579d4b-5ad0-469e-8946-2560e458bb81","color":"orange","value":"강화학습"},{"id":"b78b3694-0089-4efb-802c-c288e6190037","color":"green","value":"알고리즘"},{"id":"34c6aef1-b3b3-490e-9369-b5a385f7da4e","color":"blue","value":"딥러닝"},{"id":"67465999-8b72-4fe4-92ce-db5a812b880a","color":"brown","value":"공학수학"},{"id":"8b385c1c-8e95-4a02-b009-6215a718ebef","color":"pink","value":"글쓰기"},{"id":"435e1850-8e40-4b73-8496-0a3f694b6aeb","color":"purple","value":"스타트업"},{"id":"2b34c718-9490-4d2c-8c1d-949ea1a5010c","color":"gray","value":"독서"}]},"BN]P":{"name":"Tags","type":"multi_select","options":[{"id":"210cfb45-7eae-44e5-83dc-dba99aa3a853","color":"green","value":"Node.js"},{"id":"02b16a55-92ee-455d-9449-5e5e0a67cd04","color":"brown","value":"Computer Science"},{"id":"7993ec69-9767-4b84-adb9-5f1c907a6c77","color":"blue","value":"React.js"},{"id":"264c4a74-71d6-4015-bc91-5742970abd89","color":"yellow","value":"OSS"},{"id":"da38c5e1-f969-4abe-b28f-389b37aa22e5","color":"pink","value":"Startups"},{"id":"ceb7f269-10b7-49a9-bd4e-ba99533dee7b","color":"red","value":"Career"},{"id":"46b1d846-537f-43b5-a18c-298386278e63","color":"default","value":"Video"},{"id":"e485cc4c-e0de-4363-b442-22a0f6f588a9","color":"orange","value":"Saasify"},{"id":"5b6ee85e-ba0a-4c12-8e27-00e5b868939e","color":"gray","value":"SaaS"},{"id":"01c8627e-ae07-4f7b-b248-f877f88c8c4f","color":"purple","value":"Web Dev"},{"id":"136170f8-fe43-466b-b790-087508e8bc27","color":"blue","value":"Software Development"},{"id":"77f16ef0-4622-4f47-8f39-f5913a96421d","color":"pink","value":"Projects"},{"id":"aba9d5d2-c4f2-40ad-9f37-db6211536f92","color":"blue","value":"App Dev"},{"id":"84f76e57-8d2d-4078-bb7d-cf85520ba75c","color":"orange","value":"Lifestyle"},{"id":"38b5979d-877b-4cf4-920b-8b5da5ce9ba1","color":"yellow","value":"Thought Experiments"},{"id":"452e3943-dbcc-4d48-95fc-20d5d6339ddd","color":"orange","value":"Research"},{"id":"534e3ab2-fe89-487b-9784-71d76d5396a1","color":"purple","value":"Passion Economy"},{"id":"aa48c5a8-cfe3-4195-bfaa-25a72353299a","color":"yellow","value":"Tech"},{"id":"ff6898c4-f3f1-4024-b6a9-1cb871133744","color":"blue","value":"Creator Economy"},{"id":"ec59ba49-f2d4-4317-9a0e-3c7fb0bc60b8","color":"green","value":"Crypto"},{"id":"3fc961e4-4408-4d65-b750-7c136977ce61","color":"gray","value":"Deep Learning"},{"id":"a1419788-3341-407e-8df0-cd3d1f4ff636","color":"brown","value":"Gradient Vanishing"},{"id":"50ebe91f-e808-4056-b8ec-3b0bae45d464","color":"yellow","value":"Convolution Layer"},{"id":"ae8b3ff9-00e0-4d25-8d8d-e7389fc7fc8c","color":"orange","value":"Dot Product"},{"id":"8f6c2d82-17e0-4fb1-82eb-a6047ec75d02","color":"orange","value":"Vector"},{"id":"96ed8c32-a637-4b4f-ba99-3b397d04435a","color":"red","value":"Auction Theory"},{"id":"d4090af3-0dfb-480d-9503-0d0328774c16","color":"pink","value":"Reinforcement Learning"},{"id":"6fbb9d5c-c0f4-4791-8397-6e89c63e0c82","color":"yellow","value":"MDP"},{"id":"92885d4f-2302-43ac-87b0-e9e1c7a8cd8b","color":"yellow","value":"Introduction"},{"id":"f5ee4f74-bef9-4701-9d7d-1b9e7068d702","color":"blue","value":"Value Function"},{"id":"a575fa70-0999-44a3-b96c-262d57984b63","color":"green","value":"bellman equation"},{"id":"807f640e-1abc-476a-8300-1fe0ec0a15a3","color":"gray","value":"Grid World"},{"id":"e959edcb-aeb9-4a66-91e9-7677c68a85a8","color":"purple","value":"Dynamic Programming"},{"id":"3c5ba2f8-7ec5-4d9c-89b8-2ae975f4740a","color":"default","value":"Policy Iteration"},{"id":"b79852f3-5c3c-4de7-8d6d-731673caae40","color":"green","value":"Value Iteration"},{"id":"c2bd765b-351d-4a6d-a75d-94e8c476a180","color":"red","value":"Policy Evaluation"},{"id":"4a911c23-ca00-4f29-ad2f-463eb166f8f2","color":"brown","value":"SARSA"},{"id":"563d1a3e-095d-4ad8-bb7e-cc8c8195a9c1","color":"green","value":"Q-Learning"},{"id":"9e1bb3b2-a545-4e9d-b1eb-c47d2db8f4a8","color":"gray","value":"Writing"},{"id":"fcaf8148-917c-44bd-8ca1-fce87ba28f55","color":"blue","value":"Nepal"},{"id":"d93da022-471d-4d13-8dbc-051aa3e4639f","color":"orange","value":"Travel"},{"id":"f20c5e6f-58fe-4208-baa3-20704f665d21","color":"purple","value":"Algorithm"},{"id":"447d9963-8cfc-4c4c-9adb-88f0dc85f249","color":"red","value":"Python"},{"id":"633d7256-ffbf-4e5b-9912-906130d85250","color":"yellow","value":"Big-O"},{"id":"1a1af600-6b9b-4b8e-9483-c038712cda7c","color":"default","value":"String"},{"id":"b24d85c4-b7af-411c-905f-c9ae68eab612","color":"pink","value":"Array"},{"id":"87516a34-7662-4a77-a219-149777b960dc","color":"red","value":"LinkedList"},{"id":"d63330f6-ba17-4b9c-b370-579f4c99358a","color":"blue","value":"Stack"},{"id":"04eff27e-18a0-4883-8915-6ba26059106d","color":"yellow","value":"Queue"},{"id":"328cf039-a0b4-4b5c-90f7-be587808a1dc","color":"orange","value":"Deque"},{"id":"3364095f-3a21-47c6-8460-16ddc7f02c13","color":"brown","value":"HashTable"},{"id":"64841489-db5b-45ba-972a-7af2f53f41c7","color":"yellow","value":"Graph"},{"id":"b6dea43b-7633-4290-91f0-9761023838b0","color":"purple","value":"Tree"},{"id":"9dc575d0-d0d5-4282-8430-aae294857404","color":"gray","value":"Heap"},{"id":"edc331ab-bf6d-4985-9ecc-6884de83e8fc","color":"brown","value":"Trie"},{"id":"f2fdcca6-c69d-4b53-ab23-665ca7a223b9","color":"yellow","value":"Sort"},{"id":"5e02ca54-b743-4501-9776-62e030442114","color":"default","value":"BinarySearch"},{"id":"9a317b05-c2df-41ff-bf3f-32c39b5c451b","color":"red","value":"Greedy"},{"id":"536f3cec-7932-4444-b8c7-154672ad5fe6","color":"pink","value":"DivideAndConquer"},{"id":"1fe05376-d59a-4e4a-8596-fcd70f194621","color":"orange","value":"Basic"},{"id":"6fdde3f1-6554-4e54-ba5b-af1fc95b0ce4","color":"green","value":"Linear"},{"id":"28fc1aec-d1dd-44bb-a3ab-78ebc6f74837","color":"yellow","value":"NonLinear"},{"id":"e74e9521-68b1-4c11-ab42-a034a44dfd24","color":"blue","value":"알고리즘"},{"id":"42f167a6-7e2e-4d8e-854d-426689132f08","color":"brown","value":"Amazon"},{"id":"8aefb99d-7b6b-453b-941f-e114e32c6438","color":"blue","value":"book"},{"id":"d5083966-d1d3-4116-a615-5107104890c2","color":"pink","value":"Business"},{"id":"df607bdb-4379-4273-a45e-e52d436aef0d","color":"red","value":"Diffusion Model"},{"id":"da21253b-a1e7-4fe8-9ed3-6073a347c457","color":"orange","value":"Paper Review"}]},"NVm^":{"name":"Slug","type":"text"},"QSi`":{"name":"Series","type":"checkbox"},"a\u003cql":{"name":"Published","type":"date"},"jhf;":{"name":"Tweet","type":"text"},"nAX{":{"name":"Created","type":"created_time"},"}nqi":{"name":"Author","type":"text"},"~]S\u003c":{"name":"Description","type":"text"},"title":{"name":"Name","type":"title"}},"format":{"copied_from_pointer":{"id":"e5fdcb8e-6e29-4bc9-828d-263749307808","table":"collection","spaceId":"fde5ac74-eea3-4527-8f00-4482710e1af3"},"property_visibility":[{"property":"AfoN","visibility":"hide_if_empty"},{"property":"BN]P","visibility":"hide_if_empty"},{"property":"a\u003cql","visibility":"hide_if_empty"},{"property":"}nqi","visibility":"hide_if_empty"},{"property":"~]S\u003c","visibility":"hide"},{"property":"==~K","visibility":"hide"},{"property":"jhf;","visibility":"hide"},{"property":"=bhc","visibility":"hide"},{"property":"NVm^","visibility":"hide"},{"property":"nAX{","visibility":"hide"},{"property":";KhU","visibility":"hide"},{"property":"QSi`","visibility":"hide"}],"collection_page_properties":[{"visible":false,"property":"AfoN"},{"visible":true,"property":"BN]P"},{"visible":true,"property":"a\u003cql"},{"visible":false,"property":"}nqi"},{"visible":true,"property":"~]S\u003c"},{"visible":true,"property":"==~K"},{"visible":true,"property":"jhf;"},{"visible":false,"property":"=bhc"},{"visible":false,"property":"NVm^"},{"visible":true,"property":"nAX{"},{"visible":false,"property":";KhU"}]},"parent_id":"4f20ede8-ccf7-4ae1-82e5-819e100dd032","parent_table":"block","alive":true,"copied_from":"e5fdcb8e-6e29-4bc9-828d-263749307808","template_pages":["cacb6437-50cd-4a3d-9d43-58962f75e40b"],"migrated":true,"space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6","deleted_schema":{":nQy":{"name":"Curating","type":"text"}}}}},"collection_view":{"d91647c5-6a81-48b1-a1ff-a04529d0ddba":{"role":"reader","value":{"id":"d91647c5-6a81-48b1-a1ff-a04529d0ddba","version":33,"type":"gallery","name":"Gallery view","format":{"gallery_cover":{"type":"page_cover"},"gallery_cover_size":"medium","gallery_properties":[{"visible":false,"property":"AfoN"},{"visible":false,"property":"jhf;"},{"visible":false,"property":"NVm^"},{"visible":false,"property":"==~K"},{"visible":false,"property":";KhU"},{"visible":false,"property":"=bhc"},{"visible":false,"property":"}nqi"},{"visible":false,"property":"nAX{"},{"visible":false,"property":"BN]P"},{"visible":true,"property":"title"},{"visible":true,"property":"~]S\u003c"},{"visible":true,"property":"a\u003cql"}],"gallery_cover_aspect":"cover","hide_linked_collection_name":true,"inline_collection_first_load_limit":{"type":"load_limit","limit":10}},"parent_id":"4f20ede8-ccf7-4ae1-82e5-819e100dd032","parent_table":"block","alive":true,"page_sort":["e3df5b79-eb34-4bec-a82f-628699f43852","7a8e9c32-39af-4bca-a26e-3f8f1c9a1762","7864e0d8-d646-4df5-a4ae-057371b7559d","36a57bd6-d63a-40f0-a0a2-6ba247885fab","c2994f3d-696a-4015-b7d1-2b71883562ad","58fc3d75-92a2-44e9-ab0d-84cb61086232","30ac6075-b07c-41fa-b684-15c16f1134ba","4c804cac-3119-4cc3-a621-a82040d9a0db","6ce6c0ba-0d6f-40b3-b7d4-afe8ca715c73","64158711-b8f3-4c3d-ba2f-3999059c5581","0c679f04-0865-4142-a65b-28668aa9daea","54fa8c94-ead7-4b14-b068-174936adb37f","ab2e2675-7daf-464f-a2c4-563136630232","ef52bf7e-8726-4804-a0d5-e9b5f7919284","460f888e-b262-4f14-a39f-93ae0090bb9d","eff2e07b-ee44-45e0-b46a-622deb611d0b","c94f3f08-d65f-4f7d-a409-72c353f9786d","daac7673-f388-475e-8f43-a345dc350bef","6ff46924-9cc3-4445-badc-28e7daa5fa5a","c114dc9f-b09a-4e30-9c0d-5ace92e6a8e3","51b78f8b-69ff-4a46-9ec6-74f0d5400d3e","06adcb1d-3acf-4266-a8d9-6cb0d3c439ce","22c49483-380f-4b94-be90-b437b7cbab52","45a3d677-b3df-4cc1-ac40-7e0ab214aeef","7a177274-38ce-4f03-b3f6-06e20b0ba4f8","15aa9135-7cca-40c7-b656-ba4457524924","599373d0-99e2-4c28-89de-d273b43aca5c","48756e2f-e604-4421-9b34-be2c90e20589","b94b3e79-f161-45a8-bf8e-5315f85dca99","3e71fe2e-c827-4fc6-b610-7d71147ae4a7","298f8e1a-a9f2-4274-b5f1-7279f7bc4e5b","c7f629b2-6f4e-43fe-8720-4a3a10d4e651","b1fded7b-ceea-46d0-89e2-32010807d35c","bd99ffd5-ce6d-4418-be0e-6db2cd8ae070","5e852682-71e3-45b9-8056-d40e972563fd","1db23f4a-8d7e-4445-b241-a8e4be5bd02a","70df362d-5c3a-4d2c-a3da-ef27e1f207e3","dae737f3-5216-4c50-b708-2bf7c2662020","a13c9d7e-5e2f-44c3-bd8c-17fd50ca4892","f3236b1d-73c7-45d5-920e-f7cac8c19573","c181e327-c3bf-42f8-b8aa-21a367ce64f2","f5613c52-6b68-4073-b593-03d26f51e710"],"query2":{"sort":[{"property":"=bhc","direction":"descending"},{"property":"a\u003cql","direction":"descending"}],"filter":{"filters":[{"filters":[{"filter":{"value":{"type":"exact","value":true},"operator":"checkbox_is"},"property":"==~K"},{"filter":{"value":{"type":"exact","value":true},"operator":"checkbox_is"},"property":"=bhc"},{"filter":{"value":{"type":"exact","value":true},"operator":"checkbox_is"},"property":"QSi`"}],"operator":"and"}],"operator":"and"},"aggregations":[{"aggregator":"count"}]},"space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}},"27bc73a3-5779-44e0-b617-2f0d26f5aa2f":{"role":"reader","value":{"id":"27bc73a3-5779-44e0-b617-2f0d26f5aa2f","version":1,"type":"table","name":"","format":{"table_properties":[{"width":293,"visible":true,"property":"title"},{"width":398,"visible":true,"property":"~]S\u003c"},{"width":81,"visible":true,"property":"==~K"},{"width":105,"visible":true,"property":"=bhc"},{"width":146,"visible":true,"property":"a\u003cql"},{"width":200,"visible":true,"property":"BN]P"},{"width":122,"visible":true,"property":"}nqi"},{"width":156,"visible":true,"property":"jhf;"},{"width":146,"visible":true,"property":"NVm^"},{"width":200,"visible":true,"property":";KhU"},{"width":200,"visible":false,"property":"nAX{"}]},"parent_id":"4f20ede8-ccf7-4ae1-82e5-819e100dd032","parent_table":"block","alive":true,"page_sort":["f3d44d4c-975d-4b11-8396-c68b35bfdb26","70df362d-5c3a-4d2c-a3da-ef27e1f207e3","f3236b1d-73c7-45d5-920e-f7cac8c19573","4117e62e-18ec-4503-a32a-6c31806a5e2a","6b9736df-63cb-4e7c-ba8e-d6e54f26d6c9","c2f261a2-c616-4198-b1a9-2caf6be162a0","a082287f-d4c9-4cfb-b3b2-72b8bbf043c6","4801bc76-a763-46a8-981d-79dc38c5d85a","e3df5b79-eb34-4bec-a82f-628699f43852","36a57bd6-d63a-40f0-a0a2-6ba247885fab","c2994f3d-696a-4015-b7d1-2b71883562ad","58fc3d75-92a2-44e9-ab0d-84cb61086232","30ac6075-b07c-41fa-b684-15c16f1134ba","4c804cac-3119-4cc3-a621-a82040d9a0db","6ce6c0ba-0d6f-40b3-b7d4-afe8ca715c73","64158711-b8f3-4c3d-ba2f-3999059c5581","0c679f04-0865-4142-a65b-28668aa9daea","54fa8c94-ead7-4b14-b068-174936adb37f","ab2e2675-7daf-464f-a2c4-563136630232","ef52bf7e-8726-4804-a0d5-e9b5f7919284","460f888e-b262-4f14-a39f-93ae0090bb9d","eff2e07b-ee44-45e0-b46a-622deb611d0b","c94f3f08-d65f-4f7d-a409-72c353f9786d","daac7673-f388-475e-8f43-a345dc350bef","6ff46924-9cc3-4445-badc-28e7daa5fa5a","c114dc9f-b09a-4e30-9c0d-5ace92e6a8e3","51b78f8b-69ff-4a46-9ec6-74f0d5400d3e","06adcb1d-3acf-4266-a8d9-6cb0d3c439ce","22c49483-380f-4b94-be90-b437b7cbab52","1db23f4a-8d7e-4445-b241-a8e4be5bd02a","dae737f3-5216-4c50-b708-2bf7c2662020","a13c9d7e-5e2f-44c3-bd8c-17fd50ca4892","7864e0d8-d646-4df5-a4ae-057371b7559d","7a8e9c32-39af-4bca-a26e-3f8f1c9a1762","0f58fdf7-da3e-4793-93f8-b2503443020d","ca43f24a-f6c2-4611-a92a-8051ec80fb0f","36b681a4-e13e-4a78-a2d6-f56b2e3657a1","8cfc0d5a-59ba-4ea0-9609-4720753d5fba","88891af2-1639-4eff-a2c9-f3ba6668b2ea","92db0a94-6e1e-44a6-8df5-882e60ff5b3f","0d958b63-df7a-44a4-b80c-5408c78f59e4","7a177274-38ce-4f03-b3f6-06e20b0ba4f8","ae0b3ee0-531c-4f81-8d4b-cc6dd040fea1","bf9953ee-2589-4969-948c-0d31106a9deb","fdb7d90f-a355-4e7d-8f9a-3d07a58a457e","561e9779-b56c-4310-8913-d54675e02c74","6951e99b-10cc-4833-a27f-f0e08f8941d0","76a4bb7d-be00-45f3-bbcd-cba28c38588b","18c92268-31e5-4509-8644-9c6ad9e44c10","6463af62-efa9-47dc-91ce-99df728f66e0"],"query2":{"sort":[{"property":"=bhc","direction":"descending"},{"property":"a\u003cql","direction":"descending"}],"aggregations":[{"property":"title","aggregator":"count"}]},"space_id":"4af10338-3e65-4b50-af9f-798d59d5c8f6"}}},"notion_user":{},"collection_query":{},"signed_urls":{},"preview_images":{"images.unsplash.com/photo-1563209259-b2fa97148ce1":{"originalWidth":4509,"originalHeight":3433,"dataURIBase64":"data:image/webp;base64,UklGRkQAAABXRUJQVlA4IDgAAAAQAgCdASoQAAwABUB8JZQCdAEN4Z4/0jgAAP7nI6g5q7g7NuXgHKw945x+RhNJbTW2vaRJ2E2wAA=="},"notion.so/image/notion.so%2Fimages%2Fpage-cover%2Fnasa_reduced_gravity_walking_simulator.jpg":{"originalWidth":2000,"originalHeight":1597,"dataURIBase64":"data:image/webp;base64,UklGRjwAAABXRUJQVlA4IDAAAADQAQCdASoQAA0ABUB8JaQAAuUwaXLIHAD+3O9cAAKxBQKBBYBng9tx6GTwHd24mAA="},"notion.so/image/s3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fb29e9b03-c79c-4e52-a45e-7228163ba524%2Fcompass-circular-tool_(3).png":{"originalWidth":2000,"originalHeight":2000,"dataURIBase64":"data:image/webp;base64,UklGRv4AAABXRUJQVlA4WAoAAAAQAAAADwAADwAAQUxQSFUAAAARL6CmkRQ4eF5eaio0EBGB5+2AUWxbbR5t/yWkKiDsUwVQnn8xoViI6P8EAOK9AiCF/BQQSXKHsKrOmo21PddC4xsKsbafNSc1jfQLgBSSGsD0HBoAAFZQOCCCAAAAsAIAnQEqEAAQAAVAfCWkAA+DSIAfdDvk/UudpKAAAPYcuQdG3VZft4V447Ryb0FWml/BLapzhboSZ6SLFp2WFSojeqJNq2VAANIrwpK47u1M/geoV8ECX9+TEWBDeQC4c2g3HcSldGi6FylcOf6PWeqJaegO6jdPFlsvX04FC2gAAA=="},"notion.so/image/longshiine.github.io%2F2021%2F01%2F20%2FReinforcement-Learning-Basic-1%2FMDP.png":{"originalWidth":2000,"originalHeight":600,"dataURIBase64":"data:image/webp;base64,UklGRkIAAABXRUJQVlA4IDYAAACwAQCdASoQAAUABUB8JZQC7ADx9MAAAP6yIRD7JlC+hmv02zQnELzepk5gtoAwEAITIASQAAA="},"notion.so/image/longshiine.github.io%2F2021%2F01%2F20%2FReinforcement-Learning-Basic-1%2Fgreedworld.png":{"originalWidth":2000,"originalHeight":1469,"dataURIBase64":"data:image/webp;base64,UklGRjwAAABXRUJQVlA4IDAAAAAwAQCdASoQAAwABUB8JZwAA3AA/vAR1PXtQWFU4Gxe1p0CcP6WEUad3UHYCuZeAAA="},"notion.so/image/longshiine.github.io%2F2021%2F01%2F20%2FReinforcement-Learning-Basic-1%2Fpolicy.png":{"originalWidth":2000,"originalHeight":2000,"dataURIBase64":"data:image/webp;base64,UklGRtAAAABXRUJQVlA4WAoAAAAQAAAADwAADwAAQUxQSCgAAAARL6AQQADE37nRiIj4JyiGJInZCwKnkPwl53kAEf2fgJvGFGdxmvA/VlA4IIIAAABwAgCdASoQABAABUB8JaQAH0AgDN1mZSci2r8AAP7iJ/n8v1+Xhzrvoramiai4mPCF5lS1Wv+BU/lB6obS9/x7BsFec4bRQK5UjOCdby1+nrtV4sWcj6lanSrI97znvumfQr2KwWdXhXfEzSA7wqvxsLy1GfYmUxCbx5uAo8dDMAAA"},"notion.so/image/s3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Ffc853118-df50-43ed-96d9-0711493d5e25%2Fjang_inspiration_logo.png":{"originalWidth":2000,"originalHeight":2000,"dataURIBase64":"data:image/webp;base64,UklGRv4AAABXRUJQVlA4WAoAAAAQAAAADwAADwAAQUxQSFUAAAARL6CmkRQ4eF5eaio0EBGB5+2AUWxbbR5t/yWkKiDsUwVQnn8xoViI6P8EAOK9AiCF/BQQSXKHsKrOmo21PddC4xsKsbafNSc1jfQLgBSSGsD0HBoAAFZQOCCCAAAAsAIAnQEqEAAQAAVAfCWkAA+DSIAfdDvk/UudpKAAAPYcuQdG3VZft4V447Ryb0FWml/BLapzhboSZ6SLFp2WFSojeqJNq2VAANIrwpK47u1M/geoV8ECX9+TEWBDeQC4c2g3HcSldGi6FylcOf6PWeqJaegO6jdPFlsvX04FC2gAAA=="}}},"pageId":"d1e3fc6c-4e58-42f7-9779-b9e78243e18e","rawPageId":"markov-decision-process"},"__N_SSG":true},"page":"/[pageId]","query":{"pageId":"markov-decision-process"},"buildId":"a6SgCHTQj8ioz2ipgYdLM","isFallback":false,"dynamicIds":[635,7274],"gsp":true,"scriptLoader":[]}</script></body></html>